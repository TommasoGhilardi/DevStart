{
  "hash": "33c2dc03c8afe37888feb9f0c4e8ec96",
  "result": {
    "markdown": "---\ntitle: \"From fixations to data\"\nexecute:\n  eval: false\n\npagetitle: From fixations to data\nauthor-meta: Tommaso Ghilardi\ndescription-meta: Learn how to extract different eye-tracking measures from the collected data\nkeywords: Python, saccadic-latency, looking time, eye-tracking, tobii, experimental psychology, tutorial, experiment, DevStart, developmental science\n---\n\nIn the previous two tutorial we [collected some eye-trackign data](CreateAnEyetrackingExperiment.qmd) and then we have [used I2MC to extract the fixations](I2MC_tutorial.qmd) from that data. Let's load the data we recorded and pre-processed in the previous tutorial. We will import some libraries and read the raw data and the output from I2MC.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n{python}\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n#%% Settings\n\n# Settign the working directory\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n# Screen resolution\nres = [1920, 1080]\n\n\n#%% Read and prepare data\n\n# The fixation data extracted from I2MC\nFixations = pd.read_csv('Data\\\\i2mc_output\\\\Test1\\Test1.csv')\n\n# The original RAW data\nRaw_data = pd.read_csv('Data\\\\RAW\\\\Test1.csv')\n\n# Start Recording from 0 and in Seconds\nRaw_data['time'] = Raw_data['time'] - Raw_data.loc[0,'time']\n```\n:::\n\n\nThat is a good first step but what can we do with the raw data and teh fixations? Not much I am afraid. But we can use these fixation to extract some more meaningful indexes.\n\nIn this tutorial we will look at how to extract two features from our paradigm.\n\n-   **Saccadic latency:** how quickly our participant looked at the correct location. This includes checking whether the participant was able to anticipate the appearance of the stimulus. In our paradigm we will look at how fast our participant looked at the target location (left: NoReward, right: Reward).\n\n-   **Looking time:** how long our participant looked at certain locations on the screen. In our case we will look at how long our participant looked at the two target locations (left: NoReward, right: Reward).\n\nSo what do these two measures have in common? EXACTLY!!! They are both clearly related to the position of our stimuli. It is indeed important to define our Areas Of Interest (AOI), where we will check whether our participant was looking into.\n\n# AOIs\n\n## Define AOIs\n\nAs AOIs are important, let's define them. We will define two squares around the target positions. To do this we can simply pass two coordinates for each AOI: the lower left corner and the upper right corner.\n\nAn important point to understand is that tobii and psychopy use two different coordinate systems:\n\n-   Psychopy has it's origin (0,0) in the centre of the window/screen by default.\n\n-   Tobii reports data with it's origin (0,0) in the lower left corner.\n\nThis inconsistency is not a problem per se, but we obviously need to take it into account when defining the AOIs. Let's try to define the AOIs:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Define the variable realted to the screen and to the target position\nscreensize = (1920, 1080)\ndimension_of_AOI = 600/2\nTarget_position = 500\n\n# Create areas of interest\nAOI1 =[[screensize[0]/2 - Target_position - dimension_of_AOI, screensize[1]/2-dimension_of_AOI], [screensize[0]/2 - Target_position + dimension_of_AOI, screensize[1]/2 + dimension_of_AOI]]\n\nAOI2 =[[screensize[0]/2 + Target_position - dimension_of_AOI, screensize[1]/2-dimension_of_AOI], [screensize[0]/2 + Target_position + dimension_of_AOI, screensize[1]/2 + dimension_of_AOI]]\n\nAOIs = [AOI1, AOI2]\n```\n:::\n\n\nNice!! This step is essential. We have created two AOIs. We will use them to define whether the gaze of our participant was on these two AOIs. Let's get a better idea by just plotting these two AOIs and two random points `(600, 500)` and `(1400,1000)`.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create a figure\nfig, ax = plt.subplots(1)\n\n# Set the limits of the plot\nax.set_xlim(0, 1920)\nax.set_ylim(0, 1080)\n\n# Define the colors for the rectangles\ncolors = ['#46AEB9', '#C7D629']\n\n# Create a rectangle for each area of interest and add it to the plot\nfor i, (bottom_left, top_right) in enumerate(AOIs):\n    width = top_right[0] - bottom_left[0]\n    height = top_right[1] - bottom_left[1]\n    rectangle = patches.Rectangle(bottom_left, width, height, linewidth=2, edgecolor='k', facecolor=colors[i])\n    ax.add_patch(rectangle)\n\nax.plot(600,500,marker='o', markersize=8, color='green')    \nax.plot(1400,1000,marker='o', markersize=8, color='red')    \n\n# Show the plot\nplt.show()\n```\n:::\n\n\n## Points in AOIs\n\nAs you can see, we are plotting the two AOIs and two points. One falls into one of them and the other doesn't. But how can we get Python to tell us if a point falls within one of our AOIs?\n\nWe can simply check that the (x, y) coordinates of the point are within the x and y coordinates of the left bottom and top right. So imagine we have a point: `point` and an area: `area`, we can check if the point falls inside the area by:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Extract bottom left and top right points\nbottom_left, top_right = area\n\n# Extract the x and y of each point\nbottom_X, bottom_y = bottom_left\ntop_x, top_y = top_right\n\n# Extract the x and y of our point of interest\nx, y = point\n\n# Check if the point is in the area\nbottom_x <= x <= top_x and bottom_y <= y <= top_y\n```\n:::\n\n\nPerfectly, this would return True if the point falls inside the area and False if it falls outside. Now we want to make things a bit fancier. We will create a function that checks if a point falls within a list of areas, and tells us which area it falls in.\n\nWe will run the code above in a loop using `enumerate`. This extracts two elements to our loop: the index of the element and the element itself. In our case the index of the area and the area itself. This is very useful as we can then use both these information. We will use the actual area to check if our points falls into it and if it does we will return the index of that area. In case the point doesn't falls in any area the function will return -1.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# We define a function that simply takes the a point and a list of areas.\n# This function checks in which area this point is and return the index\n# of the area. If the point is in no area it returns -1\ndef find_area_for_point(point, areas):\n\n    for i, area in enumerate(areas):\n        # Extract bottom left and top right points\n        bottom_left, top_right = area\n        \n        # Extract the x and y of each point\n        bottom_X, bottom_y = bottom_left\n        top_x, top_y = top_right\n        \n        # Extract the x and y of our point of interest\n        x, y = point\n        \n        # Check if the point is in the area\n        bottom_x <= x <= top_x and bottom_y <= y <= top_y :\n            return(i)\n    return(-1)\n```\n:::\n\n\nPerfect now we have a cool function to check whether a point falls into a list of areas. We can use this function to filter the fixations that are into the AOIs, these are the one we care about.\n\n## Add AOIs to database\n\nNow that we have defined our areas of interest and have this cool function to find out if a point falls within them, we can add these AOIs to our database.\n\n### Load the data\n\nFirst of all, let's load the data we recorded and pre-processed in the previous tutorial. We will import some libraries and read the raw data and the output from I2MC. The raw data will be used to find where the events occurred and the fixations from I2MC will tell us where the participant was looking.\n\n```{}\n```\n\n### New Columns\n\nHere we add a new column to our Fixation database containing the AOIs we defined together before. Thus each row of this column will inform us which AOIs we should check.\n\n::: callout-important\nOur study paradigm is extremely simple as we have only 2 stable AOIs. If you have multiple or even changing AOIs you can add them to each row based on the event or on the trials\n:::\n\n# TOIs\n\nNow that we\\'ve figured out how to select fixations that fall within our areas of interest, it\\'s time to consider another important factor: Time. Our experiment involves presenting a variety of stimuli, including fixation crosses, cues, and rewards. However, similar to real-life studies, we\\'re not interested in analyzing the entire experiment. Instead, we focus on specific events.\n\nIn this case, our attention is on the target window. We\\'re going to establish a time window that begins 750ms before the target appears and continues for 2000ms after its presentation. This will allow us to extract both **Saccadic latency** and **Lookign time.**\n\nLet's start by finding the moment in which the target appeared:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Let's find the time we presented a reward or no reward\nTargets = Raw_data.loc[(Raw_data['Event'] == 'Reward') | (Raw_data['Event'] == 'NoReward'), ['time', 'Event']].values\n```\n:::\n\n\nHaving identified the moments when the targets were presented, we can now establish a time window around each of these instances. To accomplish this, we will iterate over the identified times and select all the fixations that occur within the defined window. To make things even clearer, we\\'re going to add two new columns to our fixation database: `Event` and `Event_trial`. These will help us know which event each fixation is linked to and which specific trial it belongs to. Plus, we\\'re going to add another column called `Onset` to the fixations database. This will let us easily store the onset times of specific events, making our analysis down the line a whole lot simpler.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Find the fixations that we care about\npre  = -750\npost = 400\n\nfor i,c in enumerate(Targets):\n    \n    # Find which row meet our conditions\n    mask = (Fixations['startT'] >= c[0]+pre) & (Fixations['startT'] < c[0]+post)\n    \n    # Fill the rows with have found with more info\n    Fixations.loc[mask, 'Event'] = c[1]\n    Fixations.loc[mask, 'Event_trial'] = i\n    Fixations.loc[mask, 'Onset'] =  c[0]\n```\n:::\n\n\nOur Fixations database is now chock-full of event-related info. But, there\\'s a catch. The fixations that took place at times other than during the target presentation are still hanging around in the database. And they\\'re filled with NANs in the new columns we just created. We can use this at our advantage adn filter all the NANs out:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# We can drop the NANs to have only the fixations that interest us!!!!\nTarget_fixations = Fixations[Fixations['Event'].notna()].reset_index(drop = True)\n```\n:::\n\n\n# Add the AOIs\n\nNow we have selected our fixations based on the events. But we also need to filter the fixations based on the AOIs. If only we had some functions to do so.... Oh wait we actually just created them!! Let's make use of them!!\n\nAs first step we will add a new column to our Target_fixations database containing the AOIs we defined together before. Thus each row of this column will inform us which AOIs we should check.\n\n::: callout-important\nJust a heads up, our study design is pretty straightforward with only two stable Areas of Interest (AOIs). But if you\\'re dealing with multiple or moving AOIs, you\\'ve got options. You can add them to each row of the database, depending on the event or trial. This way, you get more control over which area to inspect at any given moment of the study. It\\'s all about flexibility and precision!\n:::\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nLatency_fixations['AOIs'] = [AOIs]* len(Latency_fixations)\nLatency_fixations['Looked_AOI'] = np.NAN\n```\n:::\n\n\n``` python\nLatency_fixations['AOIs'] = [AOIs]* len(Latency_fixations)\nLatency_fixations['Looked_AOI'] = np.NAN\n```\n\n",
    "supporting": [
      "FromFixationsToData_files"
    ],
    "filters": [],
    "includes": {}
  }
}