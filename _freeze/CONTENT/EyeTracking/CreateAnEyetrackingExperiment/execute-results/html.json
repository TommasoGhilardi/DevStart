{
  "hash": "076ff587ef25a24b355bc1f6bce682fb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Create an eye-tracking experiment\"\nexecute:\n  eval: false\njupyter:\n  kernel: \"python3\"\nauthor-meta: \"Tommaso Ghilardi, Francesco Poli\"\ndescription-meta: \"Learn how to record eyetracking data in your psychopy experiment using Tobii SDK, tobii_research\"\nkeywords: \"PsychoPy, Python, eye-tracking, tobii, tobii_research, experimental psychology, tutorial, experiment, DevStart, developmental science\"\ncategories:\n  - Eye-tracking\n  - Python\n---\n\n\n\n\nThis page will show you how to collect eye-tracking data in a simple Psychopy paradigm. We will use the same paradigm that we built together in the [Getting started with Psychopy](/CONTENT/GettingStarted/GettingStartedWithPsychopy.qmd) tutorial. If you have not done that tutorial yet, please go through it first.\n\n::: callout-caution\n**Tobii eye-tracker**\n\nNote that this tutorial is specific for using **Tobii eye-trackers**. The general steps and idea are obviously applicable to other eye-trackers, but the specific code and packages may vary.\n:::\n\n# Tobii_sdk\n\nTo start, we will look into how to connect and talk to our Tobii eyetracker with the **SDK** that Tobii provides. An **SDK** is a collection of tools and programs for developing applications for a specific platform or device. We will use the **Python Tobii SDK** that lets us easily find and get data from our Tobii eye tracker.\n\n## Install\n\nTo install the Python Tobii SDK, we can simply run this command in our conda terminal:\n\n``` bash\npip install tobii_research\n```\n\nGreat! We have installed the Tobii SDK.\n\n## Connect to the eye-tracker\n\nSo how does this library work, how do we connect to the eye-tracker and collect our data? Very good questions!\n\nThe `tobii_research` documentation is quite extensive and describes in detail a lot of functions and data classes that are very useful. However, we don't need much to start our experiment.\n\nFirst we need to identify all the eye trackers connected to our computer. Yes, plural, `tobii_research` will return a list of all the eye trackers connected to our computer. 99.99999999% of the time you will only have 1 eye tracker connected, so we can just select the first (and usually only) eye tracker found.\n\n::: {#817827c6 .cell execution_count=1}\n``` {.python .cell-code}\n# Import tobii_research library\nimport tobii_research as tr\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n```\n:::\n\n\nPerfect!! We have identified our eye-trackers, and we have selected the first one (and only).\n\nWe are now ready to use our eye-tracker to collect some data... but how?\n\n## Collect data\n\nTobii_research has a cool way of telling us what data we are collecting at each time point. It uses a callback function. What is a callback function, you ask? It is a function that tobii runs each time it has a new data point. Let's say we have an eye tracker that collects data at 300Hz (300 samples per second): the function will be called every time the tobii has one of those 300 samples.\n\nThis callback function will give us a `gaze_data` object. This object contains multiple information of that collected sample and we can simply select the information we care about. In our case we want:\n\n-   the `system_time_stamp`, our time variable\n\n-   the `left_eye.gaze_point.position_on_display_area`, it contains the coordinates on the screen of the left eye (both x and y)\n\n-   the `right_eye.gaze_point.position_on_display_area`, it contains the coordinates on the screen of the right eye (both x and y)\n\n-   the `left_eye.pupil.diameter`, is the pupil diameter of the left eye\n\n-   the `right_eye.pupil.diameter`, is the pupil diameter of the right eye\n\n-   the `left_eye.gaze_point.validity`, this is a value that tells us whether we think the recognition is ok or not\n\nHere is our callback function:\n\n::: {#08e18451 .cell execution_count=2}\n``` {.python .cell-code}\ndef gaze_data_callback(gaze_data):\n\n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0]\n    ly = gaze_data.left_eye.gaze_point.position_on_display_area[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0]\n    ry = gaze_data.right_eye.gaze_point.position_on_display_area[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n```\n:::\n\n\nHow we said, this function will be called every time tobii has a new data-point. COOL!! Now we need to tell tobii to run this function we have created. This is very simple, we can just do the following:\n\n::: {#1de6f2c6 .cell execution_count=3}\n``` {.python .cell-code}\n# Start the callback function\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n```\n:::\n\n\nWe are telling tobii that we are interested in the `EYETRACKER_GAZE_DATA` and that we want it to pass it to our function `gaze_data_callback`.\n\n## Triggers/Events\n\nAs we have seen, our callback function can access the tobii data and tell us what it is for each sample. Just one little piece missing... We want to know what we presented and when. In most studies, we present stimuli that can be pictures, sounds or even videos. For the following analysis, it is important to know at what exact point in time we presented these stimuli.\n\nLuckily there is a simple way we can achieve this. We can set a text string that our callback function can access and include in our data. To make sure that our callback function can access this variable we will use the `global` keyword. This makes sure that we can read/modify a variable that exists outside of the function.\n\nIn this way, each time the callback function will be run, it will also have access to the trigger variable. We save the trigger to the `ev` variable and we set it back to an empty string `\"\"` .\n\nThis means that we can set the trigger to whatever string we want and when we set it it will be picked up by the callback function.\n\n::: {#05e5137c .cell execution_count=4}\n``` {.python .cell-code}\ndef gaze_data_callback(gaze_data):\n    global trigger\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n    \n    \ntrigger = ''\n\n# Time passes\n# when you present a stimulus you can set trigger to a string that will be captured by the callabck function\n\ntrigger = 'Presented Stimulus'\n```\n:::\n\n\nPerfect! Now we have 1. a way to access the data from the eye-tracker and 2. know exactly what stimuli we are presenting the participant and when.\n\n## Correct the data\n\nTobii presents gaze data in a variety of formats. The one we're most interested in is the Active Display Coordinate System (ADCS). This system maps all gaze data onto a 2D coordinate system that aligns with the Active Display Area. When an eye tracker is used with a monitor, the Active Display Area refers to the display area that doesn't include the monitor frame. In the ADCS system, the point (0, 0) represents the upper left corner of the screen, and (1, 1) represents the lower right corner.\n\nWhile this coordinate system is perfectly acceptable, it might cause some confusion when we come to analyze and plot the data. This is because in most systems, the data's origin is located in the lower left corner, not the upper left. It might seem a bit complicated, but the image below will make everything clear.\n\n![](/images/CreateAnEyetrackingExperiment/Origins.png){fig-align=\"center\" width=\"419\"}\n\nFor this reason, we typically adjust the data to position the origin in the bottom left corner. This can be achieved by subtracting the gaze coordinates from the maximum window height size.\n\nIn addition to the origin point issue, the gaze coordinates are reported between 0 and 1. It's often more convenient to handle data in pixels, so we can transform our data into pixels. This is done by multiplying the obtained coordinates by the pixel dimensions of our screen.\n\nLastly, we also modify the time column. Tobii provides data in microseconds, but such precision isn't necessary for our purposes, so we convert it to milliseconds by dividing by 1000.\n\n::: {#e98e6f45 .cell execution_count=5}\n``` {.python .cell-code}\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global winsize\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n    \n    \ntrigger = ''\nwinsize = (1920, 1080)\n```\n:::\n\n\n## Save the data\n\nYou might have noticed that we get the data in our callback function but don't actually save it anywhere. So how to save them? There are two main approaches we can use:\n\n1.  We could have a saving function inside our callback that could append the new data to a .csv each time the callback is called.\n\n2.  We could append the data to a list. Once the experiment is finished we could save our data.\n\nThese two approaches have however some weaknesses. The first could slow down the callback function if our PC is not performing or if we are sampling at a very high sampling rate. The second is potentially faster, but if anything happens during the study which makes python crash (and trust me, it can happen.....) you would lose all your data.\n\nWhat is the solution you ask? A mixed approach!!!!!!!\\\nWe can store our data in a list and save it during the less important parts of the study, for example the Inter Stimulus Interval (the time between a trial and another). So let's write a function to do exactly that.\n\nLets first create an empty list that we will fill with out data from the callback function. As before, we make sure that our callback function can access this list and append the new data we will use the `global` keyword.\n\n::: {#cb210c96 .cell execution_count=6}\n``` {.python .cell-code}\n# Create an empty list we will append our data to\ngaze_data_buffer = []\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global gaze_data_buffer\n    global trigger\n    global winsize\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n        \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,ev))\n```\n:::\n\n\nNow the gaze_data_buffer will be filled with the data we extract. Let's save this list then.\n\nWe will first make a copy of the list, and then empty the original list. In this way, we have our data stored, while the original list is empty and can be filled with new data.\n\nAfter creating a copy, we use `pandas` to transform the list into a data frame and save it to a csv file. Using `mode = 'a'` we tell pandas to append the new data to the existing .csv. If this is the first time we are trying to save the data the .csv does not yet exist, so pandas will create a new csv instead.\n\n::: {#ec83d6e7 .cell execution_count=7}\n``` {.python .cell-code}\ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names for the dataframe\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = not os.path.isfile(output_path)\n    \n    # Write the DataFrame to an HDF5 file\n    out.to_csv(output_path, mode='a', index =False, header = file_exists)\n```\n:::\n\n\n# Create the actual experiment\n\nNow we have two function, one to access and append the data to a list, and the second to save the data to a csv. Let's see now how to include these functions in our study.\n\n## Short recap of the paradigm\n\nAs we already mentioned, we will use the experimental design that we created in [Getting started with Psychopy](/CONTENT/GettingStarted/GettingStartedWithPsychopy.qmd) as a base and we will add things to it to make it an eye-tracking study. If you don't remember the paradigm please give it a rapid look as we will not go into much detail about each specific part of it.\n\nHere a very short summary of what the design was: After a fixation cross, two shapes can be presented: a circle or a square. The circle indicates that a reward will appear on the right of the screen while the square predicts the appearance of an empty cloud on the left.\n\n## Combine things\n\nLet's try to build together the experiment then.\n\n### Import and functions\n\nTo start, let's import the libraries and define the two functions that we create before\n\n::: {#c56c5a90 .cell execution_count=8}\n``` {.python .cell-code}\nimport os\nimport glob\nimport pandas as pd\n\n# Import some libraries from PsychoPy\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound\n\nimport tobii_research as tr\n\n\n#%% Functions\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global gaze_data_buffer\n    global winsize\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,ev))\n    \n        \ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = not os.path.isfile(output_path)\n    \n    # Write the DataFrame to an HDF5 file\n    out.to_csv(output_path, mode='a', index =False, header = file_exists)\n```\n:::\n\n\n### Load the stimuli\n\nNow we are going to set a few settings, such as the screen size, create a Psychopy window, load the stimuli and then prepare the trial definition. This is exactly the same as we did in the previous Psychopy tutorial.\n\n::: {#b9c1c010 .cell execution_count=9}\n``` {.python .cell-code}\n#%% Load and prepare stimuli\n\n# Setting the directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n\n# Winsize\nwinsize = (1920, 1080)\n\n# create a window\nwin = visual.Window(size = winsize,fullscr=True, units=\"pix\", pos =(0,30), screen=1)\n\n# Load images\nfixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\ncircle   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\circle.png', size = (200, 200))\nsquare   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\square.png', size = (200, 200))\nwinning   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\winning.png', size = (200, 200), pos=(560,0))\nloosing  = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\loosing.png', size = (200, 200), pos=(-560,0))\n\n# Load sound\nwinning_sound = sound.Sound('EXP\\\\Stimuli\\\\winning.wav')\nlosing_sound = sound.Sound('EXP\\\\Stimuli\\\\loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, loosing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n```\n:::\n\n\n### Start recording\n\nNow we are ready to look for the eye-trackers connected to the computer and select the first one that we find. Once we have selected it, we will launch our callback function to start collecting data.\n\n::: {#f067386a .cell execution_count=10}\n``` {.python .cell-code}\n#%% Record the data\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\n#Start recording\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n```\n:::\n\n\n### Present our stimuli\n\nThe eye-tracking is running! Let's show our participant something!\n\nAs you can see below, after each time we flip our window (remember: flipping means we actually show what we drew), we set the trigger variable to a string that identifies the specific stimulus we are presenting. This will be picked up our callback function.\n\n::: {#40c67f03 .cell execution_count=11}\n``` {.python .cell-code}\n#%% Trials\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n        \n    fixation.draw()\n    win.flip()\n    trigger = 'Fixation'\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    if trial ==0:\n        trigger = 'Circle'\n    else:\n        trigger = 'Square'\n\n    core.wait(3)  # wait for 3 seconds\n    win.flip()\n\n    ### Wait for saccadic latencty\n    core.wait(0.75)\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n    if trial ==0:\n        trigger = 'Reward'\n    else:\n        trigger = 'NoReward'\n    core.wait(2)  # wait for 1 second\n    win.flip()    # we re-flip at the end to clean the window\n    \n    ### ISI\n    clock = core.Clock()\n    while clock.getTime() < 1:\n        pass\n    \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        core.quit()  # stop study\n```\n:::\n\n\nAs we said before in [Save the data], it is best to save the data during our study to avoid any potential data loss. And it is better to do this when there are things of minor interest, such as an ISI. If you remember, in the previous tutorial: [Getting started with Psychopy](/CONTENT/GettingStarted/GettingStartedWithPsychopy.qmd), we created the ISI in a different way than just a `clock.wait()` and we said that this different method would come in handy later on. This is the moment!\n\nOur **ISI** starts the clock and checks when 1 second has passed after starting this clock. Why is this important? Because we can save the data after starting the clock. Since the time that it will take will be variable, we will be simply check how much time has passed after saving the data and wait (using the `while clock.getTime() < 1:  pass` code) until 1 second has fully passed. This will ensure that we will wait for 1 second in total considering the saving of the data.\n\n::: {#8c042e67 .cell execution_count=12}\n``` {.python .cell-code}\n### ISI\nclock = core.Clock()\nwrite_buffer_to_file(gaze_data_buffer, 'DATA\\\\RAW\\\\'+ Sub +'.csv')\nwhile clock.getTime() < 1:\n    pass\n```\n:::\n\n\n::: callout-warning\nCarefull!!!\\\nIf saving the data takes more than 1 second, your ISI will also be longer. However, this should not be the case with typical studies where trials are not too long. Nonetheless, it's always a good idea to keep an eye out.\n:::\n\n### Stop recording\n\nWe're almost there! We have imported our functions, started collecting data, sent the triggers, and saved the data. The last step will be stop data collection (or python will keep getting an endless amount of data from the eye tracker!). Do do that, We simply unsubscribe from the eye tracker to which we had subscribed to start of data collection:\n\n::: {#cbc650fc .cell execution_count=13}\n``` {.python .cell-code}\nwin.close()\nEyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n```\n:::\n\n\nNote that we also closed the Psychopy window, so that the stimulus presentation is also officially over. Well done!!! Now go and get your data!!! We'll see you back when it's time to analyze it.\n\n# END!!\n\nGreat job getting to here!! it want easy but you did it. Here is all the code we made together.\n\n``` python\nimport os\nimport glob\nimport pandas as pd\n\n# Import some libraries from PsychoPy\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound\n\nimport tobii_research as tr\n\n\n#%% Functions\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global gaze_data_buffer\n    global winsize\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,ev))\n    \n        \ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = not os.path.isfile(output_path)\n    \n    # Write the DataFrame to an HDF5 file\n    out.to_csv(output_path, mode='a', index =False, header = file_exists)\n    \n    \n    \n#%% Load and prepare stimuli\n\n# Winsize\nwinsize = (1920, 1080)\n\n# create a window\nwin = visual.Window(size = winsize,fullscr=True, units=\"pix\", pos =(0,30), screen=1)\n\n# Load images\nfixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\ncircle   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\circle.png', size = (200, 200))\nsquare   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\square.png', size = (200, 200))\nwinning   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\winning.png', size = (200, 200), pos=(560,0))\nloosing  = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\loosing.png', size = (200, 200), pos=(-560,0))\n\n# Load sound\nwinning_sound = sound.Sound('EXP\\\\Stimuli\\\\winning.wav')\nlosing_sound = sound.Sound('EXP\\\\Stimuli\\\\loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, loosing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\n\n#%% Record the data\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\n#Start recording\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\n\n# Create an empty list we will append our data to\ngaze_data_buffer = []\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n        \n    fixation.draw()\n    win.flip()\n    trigger = 'Fixation'\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    if trial ==0:\n        trigger = 'Circle'\n    else:\n        trigger = 'Square'\n    core.wait(3)  # wait for 3 seconds\n\n    ### Wait for saccadic latencty\n    win.flip()\n    core.wait(0.75)\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n\n    if trial ==0:\n        trigger = 'Reward'\n    else:\n        trigger = 'NoReward'\n    sounds[trial].play()\n    core.wait(2)  # wait for 2 second\n\n    ### ISI\n    win.flip()    # we re-flip at the end to clean the window\n    clock = core.Clock()\n    write_buffer_to_file(gaze_data_buffer, 'DATA\\\\RAW\\\\'+ Sub +'.csv')\n    while clock.getTime() < 1:\n        pass\n    \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        Eyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\n        core.quit()  # stop study\n      \nwin.close() # close window\nEyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\ncore.quit() # stop study\n```\n\n",
    "supporting": [
      "CreateAnEyetrackingExperiment_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}