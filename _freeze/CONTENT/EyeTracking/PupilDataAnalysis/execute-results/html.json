{
  "hash": "42dfc833c81a12a7e176e4b4e9e7fdc6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Pupil Data Analysis\"\ndate: \"01/02/2025\"\n\n\nauthor-meta: Francesco Poli\ndescription-meta: \"Learn how to analyse pupil data, from a simple linear model to generalised additive models\"\nkeywords-meta: \"R, pupillometry, gam, additive models, statistics, analysis, psychology, tutorial, experiment, DevStart, developmental science\"\ncategories:\n  - Stats\n  - R\n  - Modelling\n  - Additive models\nlightbox: true\n---\n\n\n\n\nIf you have collected and pre-processed your pupil data, the long awaited moment arrived: It's finally time to analyse your data and get your results!!!\n\nIn this tutorial we will do two types of analysis. The first one is more simple, and the second is more advanced. For some research questions, simple analyses are enough: they are intuitive and easy to understand. However, pupil data is actually very rich and complex, and more sophisticated analyses can sometimes help to really get the most out of your data and let them shine!\n\n::: callout-note\nWe’re currently working on a tutorial about linear mixed-effect models, but it’s not ready yet! If you’re new to statistical analyses, we highly recommend this tutorial: [Introduction to linear mixed models](https://ourcodingclub.github.io/tutorials/mixed-models/). While it doesn’t focus on pupil data specifically, it covers all the essential basics that won’t be addressed here and can be incredibly helpful.\n:::\n\nBefore starting any type of analysis, let's import the data and take a quick look at it.\n\n## Import data\n\nThe data used in this tutorial comes from the pre-processing tutorial of pupil dilation. If you haven’t run that tutorial yet, it’s a good idea to check it out first to ensure your data is prepared and ready for analysis: [Pre-processing pupil data](PupilPreprocessing.qmd). In case you did not save the result of the pre-processing you can download them from here :\n\n\n\n\n{{< downloadthis ../../resources/Pupillometry/Processed/Processed_PupilData.csv label=\"Preprocessed_PupilData.csv\" type=\"secondary\" >}}\n\n\n\n\n\n\nNow that you have the data, let's import it along with the necessary libraries. We'll also ensure that the `Event` and `Subjects` columns are properly set as factors (categorical for easier analysis. Here's how:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)  # Data manipulation and visualization\nlibrary(easystats)  # Easy statistical modeling\n\ndata = read.csv(\"..\\\\..\\\\resources\\\\Pupillometry\\\\Processed\\\\Processed_PupilData.csv\")\n\n# Make sure Event and Subject are factors\ndata$Event = as.factor(data$Event)\ndata$Subject = as.factor(data$Subject)\n\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  X   Subject  Event TrialN mean_pupil time\n1 1 Subject_1 Square      2 0.04125765    0\n2 2 Subject_1 Square      2 0.10143081   50\n3 3 Subject_1 Square      2 0.12862906  100\n4 4 Subject_1 Square      2 0.16747935  150\n5 5 Subject_1 Square      2 0.21956792  200\n6 6 Subject_1 Square      2 0.27146304  250\n```\n\n\n:::\n:::\n\n\n\n\nFor a detailed description of the data, you can have a look at the tutorial on [preprocessing pupil data](PupilPreprocessing.qmd). The key variables to focus on here are the following:\n\n-   `mean_pupil` indicates what the pupil size was at every moment in time (every 50 milliseconds, 20Hz). This is our dependent variable.\n\n-   `time` indicates the specific moment in time within each trial\n\n-   `TrialN` indicates the trial number\n\n-   `Event` indicates whether the trial contained a circle (followed by a reward) or a square (not followed by a reward). This variable is not numerical, but categorical. We thus set it to factor with `as.factor()`.\n\n-   `Subject` contains a different ID number for each subject. This is also a categorical variable.\n\n## Comparing means\n\nIn many paradigms, you have two or more conditions and want to test whether your dependent variable (pupil size in this case!) is significantly different across conditions. In [our example paradigm](..\\GettingStarted\\CreateYourFirstParadigm.qmd), we may want to test whether, on average, pupil size while looking at the rewarding cue (the circle) is greater than pupil size while looking at the non-rewarding cue (the square). This would mean that even before the reward is presented, infants have learned that a reward will be coming and dilate their pupils in response to it! Pretty cool, uh?\n\nIf we want to test multiple groups, we can use a t-test, an ANOVA or... A linear model! Here, we'll be using a special type of linear model, a mixed-effect model - which is infinitely better for many many reasons \\[add link\\].\n\n### Adapt the data\n\nWe want to compare the means across conditions but... We don't have means yet! We have a much richer dataset, that contains hundreds of datapoints with milliseconds precision. For this first simple analysis, we just want one average measure of pupil dilation for each trial instead. We can compute this using the `tidyverse` library (that is container of multiple packages) a powerful collection of packages for wrangling and visualuzating dataframes in R.\n\nHere, we group the data by Subject, Event, and TrialN, then summarize it within these groups by calculating the mean values.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naveraged_data = data %>%\n  group_by(Subject, Event, TrialN) %>%\n  summarise(mean_pupil = mean(mean_pupil, na.rm = TRUE))\n\nhead(averaged_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 4\n# Groups:   Subject, Event [2]\n  Subject   Event  TrialN mean_pupil\n  <fct>     <fct>   <int>      <dbl>\n1 Subject_1 Circle      3     0.0397\n2 Subject_1 Circle      4     0.281 \n3 Subject_1 Circle      6     0.0559\n4 Subject_1 Circle      9    -0.0393\n5 Subject_1 Square      2    -0.139 \n6 Subject_1 Square      5     0.0420\n```\n\n\n:::\n:::\n\n\n\n\n::: callout-important\nIn this step, we used the **average** to calculate an index for each trial, meaning we averaged the pupil dilation over the trial duration. However, this is not the only option. Other approaches include extracting the **peak** value (`max(mean_pupil, na.rm = TRUE)`) or calculating the **sum** of the signal (`sum(mean_pupil, na.rm = TRUE)`), which can also represent the **area under the curve (AUC)** for the trial.\n:::\n\n### Linear mixed-effect model\n\nWith a single value for each participant, condition, and trial (averaged across time points), we are now ready to proceed with our analysis. Even if the word \"Linear mixed-effect model\" might sound scary, the model is actually very simple. We take our experimental conditions (Event) and check whether they affect pupil size (mean_pupil). To account for individual differences in pupil response intensity, we include participants as a random intercept.\n\nLet's give it a go!!!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmerTest)   # Mixed-effect models library\n\n# The actual model\nmodel_avg = lmer(mean_pupil ~ Event + (1|Subject), data = averaged_data)\n\nsummary(model_avg) # summary of the model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ Event + (1 | Subject)\n   Data: averaged_data\n\nREML criterion at convergence: 17.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.84767 -0.66407  0.02253  0.65595  2.02017 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n Subject  (Intercept) 0.0008353 0.0289  \n Residual             0.0709333 0.2663  \nNumber of obs: 55, groups:  Subject, 6\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(>|t|)   \n(Intercept)  0.10770    0.05263 17.80244   2.046   0.0558 . \nEventSquare -0.20781    0.07186 48.97131  -2.892   0.0057 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nEventSquare -0.695\n```\n\n\n:::\n:::\n\n\n\n\nWe won’t dive into the detailed interpretation of the results here—this isn’t the right place for that. However, if you’re curious about how to interpret mixed-effects model outputs, you can check out this excellent guide: [Linear mixed-effects models](https://mspeekenbrink.github.io/sdam-r-companion/linear-mixed-effects-models.html). It provides a great foundation. **And keep an eye out for our upcoming page dedicated to mixed-effects modeling, where we’ll break everything down step by step!**\n\nThe key takeaway here is that there’s a significatn difference between the **Event**. Specifically, the **Square** cue appears to result in smaller pupil dilation compared to the **Circle** event (which serves as the reference level for the intercept). **COOL!**\n\nLet’s visualize the effect!!\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create a data grid for Event and time\ndatagrid = get_datagrid(model_avg, by = c('Event'))\n\n# Compute model-based expected values for each level of Event\npred = estimate_expectation(datagrid)\n\n# 'pred' now contains predicted values and confidence intervals for each event condition.\n# We can visualize these predictions and overlay them on the observed data.\n\nggplot() +\n  # Observed data (jittered points to show distribution)\n  geom_jitter(data = averaged_data, aes(x=Event, y=mean_pupil, color=Event), width=0.1, alpha=0.5, size = 5) +\n  \n  # Model-based predictions: points for Predicted values\n  geom_point(data=pred, aes(x=Event, y=Predicted, fill=Event), \n             shape=21, size=10) +\n  \n  # Error bars for the confidence intervals\n  geom_errorbar(data=pred, aes(x=Event, ymin=Predicted - SE, ymax=Predicted + SE, color=Event), \n                width=0.2, lwd=1.5) +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'none') +\n  labs(title=\"Predicted Means vs. Observed Data\",\n       x=\"Condition\",\n       y=\"Baseline-Corrected Pupil Size\")\n```\n\n::: {.cell-output-display}\n![](PupilDataAnalysis_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=2880}\n:::\n:::\n\n\n\n\n## Analysing the time course of pupil size\n\nAlthough we have seen how to compare mean values of pupil size, our original data was much richer. By taking averages, we made it simpler but we also lost precious information. Usually, it is better to keep the data as rich as possible, even if that might require more complex analyses. Here we'll show you one example of a more complex analysis: generalised additive models. Fear not though!!! As usual, we will try to break it down in small digestible bites, and you might realise it's not actually that complicated after all.\n\nThe key aspect here is that we will stop taking averages, and analyse the time course of pupil dilation instead. We will analyse how it changes over time with precision in the order of milliseconds!! This is exciting!!!\n\nThis is something that we cannot do with linear models. For example, in this case linear models would assume that, over the course of a trial, pupil size will only increase *linearly* over time. The model would be something like this:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model = lmer(mean_pupil ~ Event * time + (1|Subject), data = data) \n```\n:::\n\n\n\n\nNote that, compared to the previous model, we have made two changes: First, we have changed the data. While before we were using averages, now we use the richer data set; Second, we added time as a predictor. We are saying that mean_pupil might be changing *linearly* across time... But this is very silly!!! To understand how silly it is, let's have a look at the data over time.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Let's first compute average pupil size at each time point by condition\ndata_avg_time = data %>%\n  group_by(Event, time) %>%\n  summarise(mean_pupil = mean(mean_pupil, na.rm=TRUE))\n\n# Now let's plot these averages over time\nggplot(data_avg_time, aes(x=time, y=mean_pupil, color=Event)) +\n  geom_line(lwd=1.5) +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'bottom',\n        legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) +\n\n  labs(x = \"time (ms)\",\n       y = \"Baseline-Corrected Pupil Size\") \n```\n\n::: {.cell-output-display}\n![](PupilDataAnalysis_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=2880}\n:::\n:::\n\n\n\n\nHere’s the data averaged by condition at each time point. As you can clearly see, pupil dilation doesn’t follow a simple linear increase or decrease; the pattern is much more complex. Let’s see how poorly a simple linear model fits this intricate pattern!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create a data grid for Event and time\ndatagrid = get_datagrid(linear_model, by = c('Event','time'))\n\n# Estimate expectation and uncertainty (Predicted and SE)\nEst = estimate_expectation(linear_model, datagrid)\n\n# Plot predictions with confidence intervals and the observed data\nggplot() +\n  # Real data line\n  geom_line(data = data_avg_time, aes(x=time, y=mean_pupil, color=Event), lwd=1.5) +\n  \n  # Predicted ribbons\n  geom_ribbon(data = Est, aes(x=time, ymin = Predicted - SE, ymax = Predicted + SE,\n                              fill = Event), alpha = 0.2) +\n  \n  # Predicted lines\n  geom_line(data = Est, aes(x=time, y=Predicted, color=Event), lwd=1.8,linetype = \"dashed\") +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'bottom',\n        legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) +\n  labs(title = \"Linear Model Predictions vs. Data\",\n       x = \"time (ms)\",\n       y = \"Baseline-corrected Pupil Size\")\n```\n\n::: {.cell-output-display}\n![](PupilDataAnalysis_files/figure-html/unnamed-chunk-7-1.png){width=2880}\n:::\n:::\n\n\n\n\nThe estimates from our model don’t really resemble the actual data! To capture all those non-linear, smooth changes over time, we need a more sophisticated approach. Enter **Generalized Additive Models (GAMs)**—the perfect tool to save the day!\n\n### Generalized additive model\n\nHere, we will not get into all the details of generalized additive models (from now on, GAMs). We will just show one example of how they can be used to model pupil size. To do this, we have to abandon linear models and download a new package instead, mgcv (`install.packages(\"mgcv\")`). This package is similar to the one we used before for linear models but offers greater flexibility, particularly for modeling time-series data and capturing non-linear relationships.\n\n#### What are GAMs\n\nOk, cool! GAMs sound awesome... but you might still be wondering what they actually do. Let me show you an example with some figures—that always helps make things clearer!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(patchwork)\n\n# Parameters\namp <- 1; freq <- 1; phase <- 0; rate <- 100; dur <- 2\ntime <- seq(0, dur, by = 1 / rate)\n\n# Sinusoidal wave with noise\nwave <- amp * sin(2 * pi * freq * time + phase) + rnorm(length(time), mean = 0, sd = 0.2)\n\n\n# Plot\none = ggplot()+\n  geom_point(aes(y=wave, x= time),size=3)+\n  theme_bw(base_size = 45)+\n  labs(y='Data')\n\n\ntwo = ggplot()+\n  geom_point(aes(y=wave, x= time),size=3)+\n  geom_smooth(aes(y=wave, x= time), method = 'lm', color='black', lwd=1.5)+\n  theme_bw(base_size = 45)+\n   theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\ntree= ggplot()+\n  geom_point(aes(y=wave, x= time),size=3)+\n  geom_smooth(aes(y=wave, x= time), method = 'gam', color='black', lwd=1.5)+\n  theme_bw(base_size = 45)+\n   theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\none + two + tree\n```\n\n::: {.cell-output-display}\n![](PupilDataAnalysis_files/figure-html/unnamed-chunk-8-1.png){width=2880}\n:::\n:::\n\n\n\n\nWhen modeling data with many fluctuations, a simple linear model often falls short. In the left plot, we see the raw data with its complex, non-linear pattern. The middle plot illustrates a linear model’s attempt to capture these fluctuations, but it oversimplifies the relationships and fails to reflect the true data structure. Finally, the right plot showcases an additive model, which adapts to the data’s variability by following its fluctuations and accurately capturing the underlying pattern. This demonstrates the strength of additive models in modeling non-linear, smooth changes.\n\n**Well..... This sounds like the same problem we have in our pupil data!!! Let's go figure**\n\n::: callout-note\nLinear models can be extended to capture fluctuations using polynomial terms, but this approach has limitations. Higher-order polynomials can overfit the data, capturing noise instead of meaningful patterns. Additive models, however, use smooth functions like splines to flexibly adapt to data fluctuations without the instability of polynomials, making them a more robust and interpretable choice.\n:::\n\n### Run our GAM\n\nTo run a GAM, the syntax is relatively similar to what we used in the linear model section.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"mgcv\")\n\n# Additive model\nadditive_model = bam(mean_pupil ~ Event\n                     + s(time, by=Event, k=20)\n                     + s(time, Subject, bs='fs', m=1),\n                     data=data)\n```\n:::\n\n\n\n\nLet's break the formula down:\n\n-   `mean_pupil ~ Event`: Here, I treat Condition as a main effect, just like we did before.\n\n-   `s(time, by=Event, k=20)`: This is where the magic happens. By wrapping `time` in `s()`, we are telling the model: “*Don’t assume that changes in pupil size over time are linear. Instead, estimate a smooth, wiggly function.*” The argument `by=Event` means: “*Do this separately for each condition, so that each condition gets its own smooth curve over time.*” Finally, `k=20` controls how wiggly the curve can be (technically, how many ‘knots’ or flexibility points the smoothing function is allowed to have). In practice, we are allowing the model to capture complex, non-linear patterns of pupil size changes over time for each condition.\n\n-   `s(time, Subject, bs='fs', m=1)`: Here, we go one step further and acknowledge that each participant might have their own unique shape of the time course. By using `bs='fs'`, I am specifying a ‘factor smooth’, which means: “*For each subject, estimate their own smooth function over time.*” Setting `m=1` is a specific parameter choice that defines how we penalize wiggliness. Essentially, this term is allowing us to capture individual differences in how pupil size changes over time, over and above the general pattern captured by the main smooth. It's something like the random effect we have seen before in the linear mixed-effect model.\n\nNow that we have run our first GAM, we can see how well it predicts the data!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Data grid\ndatagrid = get_datagrid(additive_model, length = 100, include_random = T)\n\n# Estimate expectation and uncertainty (Predicted and SE)\nEst = estimate_expectation(additive_model, datagrid, exclude=c(\"s(time,Subject)\"))\n\n\n# Plot predictions with confidence intervals and the observed data\nggplot() +\n  # Real data line\n  geom_line(data = data_avg_time, aes(x=time, y=mean_pupil, color=Event), size=1.5) +\n  \n  # Predicted ribbons\n  geom_ribbon(data = Est, aes(x=time, ymin = CI_low, ymax = CI_high,\n                              fill = Event), alpha = 0.2) +\n  \n  # Predicted lines\n  geom_line(data = Est, aes(x=time, y=Predicted, color=Event), size=1.8, linetype = \"dashed\") +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'bottom',\n        legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) +\n  labs(title = \"Additive model Predictions vs. Data\",\n       x = \"time (ms)\",\n       y = \"Baseline-corrected Pupil Size\")\n```\n\n::: {.cell-output-display}\n![](PupilDataAnalysis_files/figure-html/unnamed-chunk-10-1.png){width=2880}\n:::\n:::\n\n\n\n\nThis looks so much better!!! The line fit so much better to the data!! We can also have a look at whether the effect of our experimental condition is significant:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(additive_model) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nmean_pupil ~ Event + s(time, by = Event, k = 20) + s(time, Subject, \n    bs = \"fs\", m = 1)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.10594    0.03649   2.903  0.00373 ** \nEventSquare -0.20353    0.01333 -15.267  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                       edf Ref.df     F  p-value    \ns(time):EventCircle  3.959  4.883 6.306 1.31e-05 ***\ns(time):EventSquare  1.512  1.748 5.269   0.0197 *  \ns(time,Subject)     20.334 53.000 4.408  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.215   Deviance explained = 22.4%\nfREML = 594.43  Scale est. = 0.097128  n = 2200\n```\n\n\n:::\n:::\n\n\n\n\nThe fixed effects (*Parametric coefficients*) show a strong negative effect for `Eventquare`, indicating that pupil size for Square is significantly lower than for Circle. This suggests that pupil size is greater when expecting a rewarding stimulus compared to a non-rewarding one.\n\nThe smooth terms indicate whether the non-linear relationships modeled by s() explain significant variance in the data. A significant smooth term confirms that the function captures meaningful, non-linear patterns beyond random noise or simpler terms. While fixed effects are typically more important for hypothesis testing, it’s crucial to ensure the model specification captures the data's fluctuations accurately.\n\nYou did it!!! You started from a simpler model and little by little you built a very complex Generalized Additive Model!! Amazing work!!!\n\n::: callout-warning\n**This is just a very basic tutorial!**\n\nThere are additional checks and considerations to keep in mind when using additive models to model pupil dilation data. We plan to extend this tutorial over time to include more details.\n\nLuckily, there are researchers who have already explored and explained these steps thoroughly. [This paper](https://journals.sagepub.com/doi/10.1177/2331216519832483), in particular, has greatly informed our approach. It dives deeper into the use of GAMs (still with the **mgcv** package), reviewing techniques for fitting models, addressing auto-correlations, and ensuring the accuracy and robustness of your GAMs. We highly recommend reading this paper to deepen your understanding!\n:::",
    "supporting": [
      "PupilDataAnalysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}