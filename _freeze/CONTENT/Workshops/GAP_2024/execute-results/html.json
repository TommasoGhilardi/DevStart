{
  "hash": "781c23f5a637381e2b3469f3ee03d057",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Bridging the Technological Gap Workshop (GAP)\"\n\nexecute:\n  eval: false\ntitle-meta: \"GAP 2024\"\nauthor-meta: Tommaso Ghilardi\ndescription-meta: \"Data and resource for the workshop at GAP 2024\"\nkeywords-meta: \"PsychoPy, Python, eye-tracking, tobii, tobii_research, experimental psychology, tutorial, experiment, DevStart, developmental science, workshop\"\ncategories: Workhsop\n---\n\nHello hello!!! This page has been created to provide support and resources for the tutorial that will take place during the [Bridging the Technological Gap Workshop](https://www.eva.mpg.de/de/comparative-cultural-psychology/events/2024-btg2/).\n\n# What will you learn?\n\nThis workshop will focus on using Python to run and analyze an eye-tracking study.\n\nWe will focus on:\n\n-   How to implement eye-tracking designs in Python\n\n-   How to interact with an eye-tracker via Python\n\n-   How to extract and visualize meaningful eye-tracking measures from the raw data\n\n# What will you need?\n\n## Python\n\nIn this tutorial, our primary tool will be Python!! There are lots of ways to install python. [We recommend installing it via Miniconda](/CONTENT/GettingStarted/GettingStartedWithPython.qmd). However, for this workshop, the suggested way to install Python is using [Anaconda](https://www.anaconda.com/download/success).\n\nYou might ask....Then which installation should I follow? Well, it doesn't really matter! Miniconda is a minimal installation of Anaconda. It lacks the GUI, but has all the main features. So follow whichever one you like more!\n\nOnce you have it installed, we need a few more things. For the ***Gaze Tracking & Pupillometry Workshop*** (the part we will be hosting) we will need some specific libraries and files. We have tried our best to make everything as simple as possible:\n\n### Libraries\n\nWe will be working with a conda environment (a self-contained directory that contains a specific collection of Python packages and dependencies, allowing you to manage different project requirements separately). To create this environment and install all the necessary libraries, all you need is this file:\n\n\n{{< downloadthis ../../resources/Workshops/GAP2024/psychopy_GAP.yml label=\"Psychopy.yml\" dname= \"psychopy_GAP\" type=\"secondary\" >}}\n\n\n\nOnce you have downloaded the file, simply open the anaconda/miniconda terminal and type `conda env create -f`, then simply drag and drop the downloaded file onto the terminal. This will copy the filename with its absolute path. In my case it looked something like this:\n\n![](/resources/Workshops/GAP2024/exampleTerminal_gap2024.png){fig-align=\"center\" width=\"588\"}\n\nNow you will be asked to confirm a few things (by pressing `Y`) and after a while of downloading and installing you will have your new workshop environment called **Psychopy**!\n\nNow you should see a shortcut in your start menu called **Spyder(psychopy)**, just click on it to open spyder in our newly created environment. If you don't see it, just reopen the anaconda/miniconda terminal, activate your new environment by typing `conda activate psychopy` and then just type `spyder`.\n\n### Files\n\nWe also need some files if you want to run the examples with us. Here you can download the zip files with everything you need:\n\n\n{{< downloadthis ../../resources/Workshops/GAP2024/GAP_2024.zip label=\"Files\" dname= \"Files_Gap\" icon=\"database-fill-down\" type=\"secondary\" >}}\n\n\n\nOnce downloaded, simply extract the file by unzipping it. For our workshop we will work together in a folder that should look like this:\n\n![](/resources/Workshops/GAP2024/FinalFolder.png){fig-align=\"center\" width=\"611\"}\n\nIf you have a similar folder... you are ready to go!!!!\n\n# Q&A\n\nWe received many interesting questions during the workshop! We'll try to add new tutorials and pages to address your queries. However, since this website is a side project, it may take some time.\n\nIn the meantime, we'll share our answers here. They may be less precise and exhaustive than what we'll have in the future, but they should still provide a good idea of how to approach things.\n\n## Videos\n\nWe received several questions about working with videos and PsychoPy while doing eye-tracking. It can be quite tricky, but here are some tips:\n\n-   Make sure you're using the right codec.\n\n-   If you need to change the codec of the video, you can re-encode it using a tool like\n\n-   [Handbrake](https://handbrake.fr) (remember to set the constant framerate in the video option)\n\nBelow, you'll find a code example that adapts our [Create an eye-tracking experiment](/CONTENT/EyeTracking/CreateAnEyetrackingExperiment.qmd) tutorial to work with a video file. The main differences are:\n\n-   We're showing a video after the fixation.\n\n-   We're saving triggers to our eye-tracking data and also saving the frame index at each sample (as a continuous number column).\n\n::: {#09fc96b9 .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\n\n# Import some libraries from PsychoPy\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound\n\nimport tobii_research as tr\n\n\n#%% Functions\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global gaze_data_buffer\n    global winsize\n    global frame_indx\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,trigger, frame_indx))\n    trigger = ''\n    \ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event', 'FrameIndex']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = not os.path.isfile(output_path)\n    \n    # Write the DataFrame to an HDF5 file\n    out.to_csv(output_path, mode='a', index =False, header = file_exists)\n    \n    \n    \n#%% Load and prepare stimuli\n\nos.chdir(r'C:\\Users\\tomma\\Desktop\\EyeTracking\\Files')\n\n# Winsize\nwinsize = (960, 540)\n\n# create a window\nwin = visual.Window(size = winsize,fullscr=False, units=\"pix\", screen=0)\n\n\n# Load images and video\nfixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\nVideo = visual.MovieStim(win, filename='EXP\\\\Stimuli\\\\Video60.mp4',  loop=False, size=[600,380],volume =0.4, autoStart=True)  \n\n\n# Define the trigger and frame index variable to pass to the gaze_data_callback\ntrigger = ''\nframe_indx = np.nan\n\n\n\n#%% Record the data\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\n#Start recording\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\n# Crate empty list to append data\ngaze_data_buffer = []\n\nTrials_number = 10\nfor trial in range(Trials_number):\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n    \n    fixation.draw()\n    win.flip()\n    trigger = 'Fixation'\n    core.wait(1)  # wait for 1 second\n\n    Video.play()\n    trigger = 'Video'\n    while not Video.isFinished:\n\n        # Draw the video frame\n        Video.draw()\n\n        # Flip the window and add index to teh frame_indx\n        win.flip()\n        \n        # add which frame was just shown to the eyetracking data\n        frame_indx = Video.frameIndex\n        \n    Video.stop()\n    win.flip()\n\n\n    ### ISI\n    win.flip()    # we re-flip at the end to clean the window\n    clock = core.Clock()\n    write_buffer_to_file(gaze_data_buffer, 'DATA\\\\RAW\\\\Test.csv')\n    while clock.getTime() < 1:\n        pass\n    \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        Eyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\n        core.quit()  # stop study\n      \nwin.close() # close window\nEyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\ncore.quit() # stop study\n```\n:::\n\n\n## Calibration\n\nWe received a question about the calibration. How to change the focus time that the eye-tracking uses to record samples for each calibration point. Luckily, the function from the [Psychopy_tobii_infant](https://github.com/yh-luo/psychopy_tobii_infant) repository allows for an additional argument that specifies how long we want the focus time (default = 0.5s). Thus, you can simply change it by running it with a different value.\n\nHere below we changed the example of [Calibrating eye-tracking](CONTENT/EyeTracking/EyetrackingCalibration.qmd) by increasing the focus_time to 2s. You can increase or decrease it based on your needs!!\n\n::: {#bba3fb4c .cell execution_count=2}\n``` {.python .cell-code}\nimport os\nfrom psychopy import visual, sound\n\n# import Psychopy tobii infant\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\nfrom psychopy_tobii_infant import TobiiInfantController\n\n\n#%% window and stimuli\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\n\n#%% Center face - screen\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n#%% Calibration\n\n# define calibration points\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound, focus_time=2)\nwin.flip()\n```\n:::\n\n\n",
    "supporting": [
      "GAP_2024_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}