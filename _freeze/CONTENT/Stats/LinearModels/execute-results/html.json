{
  "hash": "9c30601a3aefae5c44f4350f4751696a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Models\"\ndate: \"5/03/2025\"\nauthor-meta: \"Tommaso Ghilardi\"\ndescription-meta: \"Learn what a linear model is, how to run it and how to check its assumptions\"\nkeywords-meta: \"R, lm, Linear models, statistics, analysis, psychology, tutorial, experiment, DevStart, developmental science\"\n\ncategories:\n  - Stats\n  - R\n  - linear models\n---\n\n\n\nWelcome to the first tutorial on data analysis!!! Today we are going to talk about one of the most flexible statistical methods: Linear models.\n\nLet‚Äôs be clear, WE ARE NOT STATISTICIANS!!!!\n\nWe‚Äôll be discussing linear models in a very accessible and practical manner. Our explanations might not align with the rigorous definitions statisticians are accustomed to, and for that, we apologize in advance! However, our aim is to provide a stepping stone for you to grasp the concept of linear models and similar analyses. Let‚Äôs get started!\n\n::: callout-note\nand only R [![](https://www.r-project.org/Rlogo.png){style=\"float:left ; margin-right:10px;margin-bottom:10px;\" width=\"31\" height=\"21\"}](https://www.r-project.org/)\n\nIn the tutorials regarding stats, we will use R (and R-Studio as the IDE). The concepts and procedures are the same in other programming languages (e.g., Python, Julia). However, we choose R for a simple reason. R is just better at most statistical things!!\n\nDeal with it!\n:::\n\n# What is a linear model?\n\nA linear model is a simple statistical test that tries to find the best line that represent the relations between two variables ( or more).\n\n[![image found on towardsdatascience.com](https://miro.medium.com/v2/resize:fit:640/format:webp/1*MtuQBTW0-XbjA2RrP2z3Kw.gif){fig-align=\"center\"}](https://towardsdatascience.com/linear-regression-5100fe32993a)\n\nWhat's truly fascinating about linear models is their versatility. They start off incredibly simple, but their complexity can grow exponentially! This makes them a remarkably flexible tool in the world of data analysis.\n\nThink of linear models as the foundation of a house. ![](/images/Stats/Lm/House.png){fig-align=\"right\" style=\"float:left; margin-right:10px;margin-bottom:10px;\" width=\"150\"}\n\nYou can start with a basic structure, but with some clever modifications (like mixed effect models, generalized linear models, or additive models), you can build anything from a cozy cottage to a multi-story mansion.\n\nIn essence, linear models offer a perfect balance: they're accessible enough for beginners to grasp, yet powerful enough to satisfy the needs of advanced researchers. As we dive deeper into this topic, you'll see just how these seemingly simple tools can unlock complex insights in your data.\n\n# Hands-on\n\nOK, enough chitchat - let's start with a practical example. We'll be working with a data set we created specifically for this tutorial. As mentioned, we're going to begin with a very basic model, and in the upcoming tutorials, we'll gradually increase both the complexity and accuracy of our approach.\n\nSo, if you notice something that doesn't seem quite perfect at this stage, don't worry! It's all part of the plan. Our goal is to guide you step-by-step towards building the best model. Just remember, this process takes time!\n\n## Import data\n\nYou can download the data that we will use in this tutorial from here:\n\n\n\n{{< downloadthis ../../resources/Stats/Dataset.csv label=\"Dataset.csv\" type=\"secondary\" >}}\n\n\n\n\nOnce downloaded we need to import it in our R session. Here we read our csv and we print a small preview of it.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = read.csv(\"..\\\\..\\\\resources\\\\Stats\\\\Dataset.csv\")\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Id    Event Event_trial ReactionTime LookingTime\n1  1 NoReward           1     344.1243   1139.8102\n2  1 NoReward           2     361.9389   1156.4140\n3  1 NoReward           3     406.3499   1073.5468\n4  1 NoReward           4     395.2774   1106.5766\n5  1 NoReward           5     382.1649    990.5572\n6  1 NoReward           6     367.3000   1053.6605\n```\n\n\n:::\n:::\n\n\n\nYou can see that the data is really simple! We have 4 columns:\n\n-   `Id` column that tell us form which participant the data was collected\n\n-   `Event` represent which condition we were in\n\n-   `Event_trial` the passing number of trials\n\n-   `ReactionTime` how quickly participants reacted to the stimuli. We will ignore this variable for now\n\n-   `LookingTime`the variable of interest for today, the one that we want to model\n\n::: callout-important\n## Long format\n\nOne important information that we need to keep in mind is that to run `lm()` (and most models!!) in R we need the data in a long format and not a wide format.\n\nIn long format, each row represents a single observation. Variables are organized in columns, with one column for the variable names and another for the values. This means that the column you want to model (in the example `LookingTime`) has 1 row for observation but the other columns usually have repeated entries ( e.g. `Id` , `Event_trial`, `Event`)\n\nWide format, on the other hand, has each row representing a subject or group, with multiple columns for different variables or time-points. While this can be visually appealing for humans, it's not optimal for our linear modeling needs.\n\nIf your data is currently in wide format, don't worry! R provides tools like the [tidyr](https://tidyr.tidyverse.org/index.html) package with functions such as [pivot_longer()](https://tidyr.tidyverse.org/reference/pivot_longer.html) to easily convert your data from wide to long format.\n:::\n\n## Formula\n\nTo run models in R we usually use formulas! Sounds complex doesn't it?!? Well it is not, let me guide you through it.\n\nIn R, model formulas follow a specific structure. On the left side of the formula, we place our dependent variable - the one we're interested in studying. In this case, it's the `LookingTime` column. Next, we use the tilde symbol `~`. This tilde tells R that we want to model the variable on the left using the variables on the right side of the formula. On the right side, we list the independent variables we believe may influence our dependent variable. To test whether `Event_trial` predicts LookingTime, we can use the formula:\n\n-   `LookingTime ~ Event_trial`. This basic structure allows us to examine a single predictor.\n\nWe can extend this model by adding another variable, such as `Event`, to see if it also predicts LookingTime:\n\n-   `LookingTime ~ Event_trial + Event`. This formulation tells the model to assess whether either `Event_trial` and `Event` predicts `LookingTime`, treating them as independent predictors.\n\nTo examine the interaction between these variables, we use a slightly different syntax:\n\n-   `LookingTime ~ Event_trial : Event`. This instructs the model to evaluate whether the interaction between the two variables predicts `LookingTime`.\n\nIt's important to note that using `:` only tests the interaction, not the individual effects of each variable. To include both main effects and their interaction, we can use the formula:\n\n-   `LookingTime ~ Event_trial + Event + Event_trial:Event`.\n\nR offers a shorthand for this complete model using the `*` operator. The formula:\n\n-   `LookingTime ~ Event_trial * Event` is equivalent to the longer version above, testing both main effects and the interaction in a more concise format.\n\nThese formulas are for simple linear models. Different types of models add small and different pieces to this basic structure. We will see in the next tutorial how to handle these \"add-ons\". Now that we have seen how to make a proper formula let's use it in our model!!\n\n## Run the model\n\nOK, now we have our amazing data! Let's run this Linear model.\n\nIt's extremely simple. We will use the function `lm()` and we will pass our data `df` and the formula we just made together!!\n\nAfter fitting the model we extract the summary of it. This is how we will get all the information we need.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_lm = lm(LookingTime ~ Event_trial*Event, data = df)\nsummary(mod_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = LookingTime ~ Event_trial * Event, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-772.0 -159.7   -6.1  190.5  607.1 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             1390.1655    29.1769  47.646   <2e-16 ***\nEvent_trial               -7.5998     2.9897  -2.542   0.0113 *  \nEventReward               99.8279    39.3360   2.538   0.0114 *  \nEvent_trial:EventReward    0.1332     3.7240   0.036   0.9715    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 253.2 on 656 degrees of freedom\n  (140 observations deleted due to missingness)\nMultiple R-squared:  0.05147,\tAdjusted R-squared:  0.04713 \nF-statistic: 11.87 on 3 and 656 DF,  p-value: 1.417e-07\n```\n\n\n:::\n:::\n\n\n\nWoohoo! This was super simple!! üéâ We can use the output of the model to understand whether the variables are predicting LookingTime. The magic number we're looking for is the p-value, hiding in the last column of the **Coefficients section**. If the p-value is below 0.05, we've got ourselves an effect! If it's above, sadly we don't. [**AND YES, EVEN IF IT'S 0.051!!!**]{.underline} Rules are rules in the p-value game!\n\nWhat goodies can we spot here? First, our **Intercept** is significant (don't worry, we'll decode this mysterious value later!). Even more exciting, we've got a significant effect of **Event_trial** ‚Äì our continuous variable ‚Äì (p = 0.0113) and a significant effect of **Event** (p = 0.0114). Double win! However, the interaction between Event_trial and Event is playing hard to get with that p = 0.9715. That's nowhere near significant!\n\nThis is already pretty cool, right?!? But here's the deal - when looking at model outputs, people often get hypnotized by p-values. **However, there's MUCH more to unpack in a model summary!** The full story of our data is waiting to be discovered in the complete model output. Let's dive in together and crack this statistical puzzle!\n\n# Interpret our Model\n\nHere what the summary is telling us!!\n\n-   **Call**: This section just reports the function call that we passed to `lm()`. Nothing fancy, just reminding us what we asked for!\n\n<!-- -->\n\n-   **Residuals**: This section shows the leftovers of our model - the differences between what we observed and what our model predicted. Think of residuals as the \"Hmm, that's interesting\" parts that our model couldn't explain. We want these to be as small as possible! üîç\n\n<!-- -->\n\n-   **Coefficients**: This is where the real magic happens! This section displays:\n\n    -   **Estimates** - The actual numbers that tell us how much our outcome changes when we tweak each predictor\n\n        ::: callout-note\n        Estimates are often called **beta coefficients** and are represented using the Greek letter Œ≤ (we will use this in some plots as it is easier to show)\n        :::\n\n    -   **Standard Error** - How confident we are about those estimates (smaller = more confident!)\n\n    -   **T-value** - A measure of how many standard errors away from zero our estimate is\n\n    -   **p-value** - The statistical measure that helps us determine if an effect is statistically significant\n\n<!-- -->\n\n-   **Model Fit**: The final act of our statistical show! Here we find:\n\n    -   **Residual standard error:** The average miss by our prediction line ( lower scores are better).\n\n    -   **Multiple R-Squared:** The proportion of variance explained by our predictors. Ranges from 0 (no explanation) to 1 (perfect explanation).\n\n    -   **Adjusted R-squared:** A modified version of R-squared that accounts for the number of predictors in the model. Useful for comparing models with different numbers of variables.\n\n    -   **F-statistics and p-value:** Tests whether the model as a whole is statistically significant. If p \\< 0.05, the model performs significantly better than using just the intercept.\n\n## Estimates\n\nThe Estimate section is probably one of the most important parts of our model summary. While the other columns (t-values and p-values) are just numbers that tell us whether the predictor fits, the estimates tell us HOW they fit!!\n\nLet's go together through the most challenging information:\n\n### (Intercept)\n\nThe intercept often confuses people who approach linear models for the first time. What exactly is it? ü§î\n\nThe (Intercept) represents the reference levels where all our predictors (`Event_trial` and `Event`) are **0**. While `Event_trial` is easy to understand when it's 0 (`Event_trial` 0 in our case would be the first trial), you may be scratching your head thinking...how can `Event` be 0? It's a categorical variable, it can't be 0!!!! `Event_trial == 0`...sure....but `Event`??\n\nYou are absolutely right! When a model encounters a categorical variable, it cleverly selects the first level of such variable as the reference level. If you take another look at our model summary, you can see that there's information for the **Reward** level of the `Event` variable but nothing about the **NoReward** level. This is because the **NoReward** level has been selected by the model as the reference level and is thus represented in the intercept value! üí°\n\nSo our intercept (1390.1) actually represents the predicted **LookingTime** when:\n\n1.  We're on the first trial (`Event_trial` = 0)\n\n2.  We're in the **NoReward** condition\n\nThe Standard Error of the estimate (29.1) tells us the precision of the estimate.\n\n::: callout-tip\nSince the intercept has a significant p-value, it means that the estimate for the **NoReward** condition at trial 0 is actually significantly different from 0. In other words, our participants are definitely looking at something during the **NoReward** condition in the first trial ‚Äì their **LookingTime** isn't zero! This might seem obvious (of course they're looking!), but statistically confirming this baseline is actually meaningful.\n:::\n\nOk, all this explanation is great...but it's much easier to visualize these Estimates!\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](LinearModels_files/figure-html/unnamed-chunk-3-1.png){width=3000}\n:::\n:::\n\n\n\nAs you can see, the intercept is pretty straightforward‚Äîit gives us the estimate when everything is set to 0, both for continuous and categorical variables. The intercept is the foundation of your model, where all the predictors are at their baseline value (in this case, **NoReward** was cleverly selected as the reference or 0 level for the categorical variable).\n\n### Event\n\nAwesome! Now that we've got the intercept down, let's take a look at the rest of the model output. We'll skip over the **Event_trial** variable for now and focus on what's happening with the **Event**.\n\nAt first, the results for `Event [Reward]` might look like they're giving us the value of looking time for the **Reward**. Super easy, right?!?\n\nWell... not exactly! ü§î\n\n**In linear models, each coefficient shows the difference in relation to the intercept (the 0 or the reference level), not the exact value of the Reward condition.**\n\nIt sounds a bit confusing, but let's break it down. If we want to understand what is the estimate for a **Rewarding** event we need to take the **Intercept** (1390.1) ‚Äìas we mentioned that is actually the event **NoReward** ‚Äì and then just simply add to it the Estimate for the `Event [Reward]` (99.8). So the model is telling us that a Rewarding event should be 1390.1 + 99.8 = 1489.9.\n\nSee? Not too bad! Let's visualize it and make it even clearer!\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](LinearModels_files/figure-html/unnamed-chunk-4-1.png){width=3000}\n:::\n:::\n\n\n\n### Event_trial\n\nSo, interpreting the coefficients for categorical variables wasn't too tricky, right? But what about continuous variables like `Event_trial`?\n\nNo worries, it's actually pretty straightforward! The coefficient for a continuous variable represents the slope, or the incline, of the line for that variable.\n\nIn simpler terms, it shows how much the outcome (in this case, `LookingTime`) changes for each unit increase in the continuous variable (`Event_trial`). So, in our case the coefficient for `Event_trial` is -7.5, this means that for each unit increase in `Event_trial`, the LookingTime is expected to decrease by 7.5 units (assuming all other variables stay the same).\n\n::: callout-important\nRemember!! This coefficient represents the effect of `Event_trial` specifically when `Event` is at its reference level (**NoReward**). In other words, this -7.5 decrease in LookingTime per trial applies specifically to the **NoReward** condition!\n:::\n\nEven easier..let's plot again!\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](LinearModels_files/figure-html/unnamed-chunk-5-1.png){width=3000}\n:::\n:::\n\n\n\n## Interaction\n\nAlright, now we're getting to the final step! Let's talk about the interaction between `Event_trial` and `Event`! Now, we're not just dealing with a single factor or continuous variable, but looking at how they interact with each other. Don't worry‚Äîif you understood the previous steps, this will be a breeze!\n\nWe'll take it step by step and look at the interaction in our model parameters. The interaction term between `Event_trial` and `Event [Reward]` tells us how the relationship between `Event_trial` and `LookingTime` changes when we switch from the reference Event (**NoReward**) to the **Reward** condition.\n\nTo put it simply:\n\n-   For **NoReward** events, **LookingTime** decreases by 7.5 units per trial (that's our main `Event_trial` coefficient)\n\n-   The interaction coefficient (let's say it's 0.13) tells us how this slope changes for **Reward** events\n\nSo for **Reward** events, the slope would be: -7.5 + 0.13 = -7.37 units per trial.\n\nWhat does this mean in real terms? If the interaction coefficient is positive (like in our example), it means participants' looking time decreases more slowly during **Reward** trials compared to **NoReward** trials ‚Äì the estimate is slightly less negative.\n\n::: callout-important\nIn this model the interaction effect is extremely small and not significant. This means that there is actually no difference in how the Event_trial predicted Looking time in either NoReward or Reward! Looking time decreases at basically the same rate regardless of which event type we're looking at!!!\n:::\n\nWhile there is no significant difference, let's plot the estimated effects for **Event_trial** for both Reward and **NoReward** conditions to visualize this relationship!\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] time                        est_performance_EventTrial \n[3] est_performance_Interaction\n<0 rows> (or 0-length row.names)\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](LinearModels_files/figure-html/unnamed-chunk-6-1.png){width=3000}\n:::\n:::\n\n\n\n::: callout-important\nDo you notice any difference in how the slopes between the two conditions? Probably not! If you examine the model summary, you'll see the interaction effect is very small and not statistically significant.\n\nThis tells us that Event_trial predicts Looking time similarly in both **NoReward** and Reward conditions. In other words, looking time decreases at essentially the same rate regardless of which event type participants observed.\n:::\n\nI hope that by now you got a decent understanding on how to interpret what a linear model is telling you!! Different kinds of models will add small pieces of information here and there, but the main information will still be there. Thus, if you got here, you are a step closer to becoming a stats genius!!! üß†üìä\n\n**But wait, we're not done yet!** Now it's time to check whether our model actually meets the assumptions of linear regression. Remember those plots we generated earlier? Let's make sure our statistical foundation is solid before we draw any final conclusions!\n\n# Model checks\n\nSo now we have run our model and seen the summary... That's great but how can we know that our model actually is ok?? Linear models, like most statistical techniques require some data assumption to be run. These assumption need to be met otherwise even if our model could be showing amazing results it won't be valid.\n\n::: callout-warning\nAs we mentioned at the beginning of our tutorial the model we have run in this tutorial is a very simple model that is not actully...correct!!\n:::\n\nWhat are these assumptions?? Well they depend a lot on the model you are running. We won't go into much details as there are very good website that explain them<sup>[1](https://bookdown.org/pingapang9/linear_models_bookdown/assumptions.html)</sup> ,<sup>[2](https://www.statology.org/linear-regression-assumptions/)</sup>, in this simple linear mode they are:\n\n1.  **Linear relationship:** There exists a linear relationship between the independent variable, x, and the dependent variable, y.\n\n2.  **Independence:** The residuals are independent. In particular, there is no correlation between consecutive residuals in time-series data.\n\n3.  **Homoscedasticity/Homogeneity of variance:** The residuals have constant variance at every level of x.\n\n4.  **Normality:** The residuals of the model are normally distributed.\n\nAgain this is not a theoretical tutorial. So we won't go into details as which are the assumptions (please read some of the link provided tho!!) but we will show you how to actually check these assumptions.\n\nThere is a super easy and convenient way we usually check these assumptions. Using the [easystats](https://easystats.github.io/easystats/) library.\n\n::: callout-note\n[**Easystats**](https://easystats.github.io/easystats/)\n\nEasystats is a collection of R packages that includes tools dedicated to the post-processing of statistical models. It is made of all these packages: [**report**](https://easystats.github.io/report/index.html), [**correlation**](https://easystats.github.io/correlation/index.html), [**modelbased**](https://easystats.github.io/modelbased/index.html), [**bayestestR**](https://easystats.github.io/bayestestR/index.html), [**effectsize**](https://easystats.github.io/effectsize/index.html), [**see**](https://easystats.github.io/see/index.html), [**parameters**](https://easystats.github.io/parameters/index.html), [**performance**](https://easystats.github.io/performance/), [**insight**](https://easystats.github.io/insight/index.html), [**datawizard**](https://easystats.github.io/datawizard/index.html). We will extensively use all these package in our tutorials. The cool thing is that you can import all of them by just simply importing the collection **Easystats** with `library(easystats)`.\n\nIn this tutorial here we will use the function from the package [**performance**](https://easystats.github.io/performance/)**.** This is a package to check model performance metrices. However instead of importing **performance** we will import **Easystats** that will import all of the packages mentioned above.\n:::\n\nSo now we import `easystats` and we use the function [`check_model()`](https://easystats.github.io/performance/reference/check_model.html) to indeed check the model assumptions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(easystats)\ncheck_model(mod_lm)\n```\n\n::: {.cell-output-display}\n![](LinearModels_files/figure-html/PlotCheck-1.png){width=4200}\n:::\n:::\n\n\n\nPerfect all done!! We have a plot of the model assumptions and we can check if they are met!! But what do these plot represent? Here below we created a table that mirrors each plot with it explanation in it. These are brief and simple explanations. If you want to understand more about the [`check_model()`](https://easystats.github.io/performance/reference/check_model.html) function we suggest you to read the documentation about it and also the [very nice vignette](https://easystats.github.io/performance/articles/check_model.html) that the package provides.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div id=\"vxqtbierbt\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>@import url(\"https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\n@import url(\"https://fonts.googleapis.com/css2?family=Libre+Franklin:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\n@import url(\"https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap\");\n#vxqtbierbt table {\n  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n}\n\n#vxqtbierbt thead, #vxqtbierbt tbody, #vxqtbierbt tfoot, #vxqtbierbt tr, #vxqtbierbt td, #vxqtbierbt th {\n  border-style: none;\n}\n\n#vxqtbierbt p {\n  margin: 0;\n  padding: 0;\n}\n\n#vxqtbierbt .gt_table {\n  display: table;\n  border-collapse: collapse;\n  line-height: normal;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #000000;\n  border-right-style: solid;\n  border-right-width: 1px;\n  border-right-color: #000000;\n  border-bottom-style: solid;\n  border-bottom-width: 1px;\n  border-bottom-color: #000000;\n  border-left-style: solid;\n  border-left-width: 1px;\n  border-left-color: #000000;\n}\n\n#vxqtbierbt .gt_caption {\n  padding-top: 4px;\n  padding-bottom: 4px;\n}\n\n#vxqtbierbt .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#vxqtbierbt .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 3px;\n  padding-bottom: 5px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#vxqtbierbt .gt_heading {\n  background-color: #FFFFFF;\n  text-align: left;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#vxqtbierbt .gt_bottom_border {\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#vxqtbierbt .gt_col_headings {\n  border-top-style: none;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: none;\n  border-bottom-width: 1px;\n  border-bottom-color: #334422;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#vxqtbierbt .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 12px;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#vxqtbierbt .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 12px;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#vxqtbierbt .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#vxqtbierbt .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#vxqtbierbt .gt_column_spanner {\n  border-bottom-style: none;\n  border-bottom-width: 1px;\n  border-bottom-color: #334422;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 5px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#vxqtbierbt .gt_spanner_row {\n  border-bottom-style: hidden;\n}\n\n#vxqtbierbt .gt_group_heading {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  text-align: left;\n}\n\n#vxqtbierbt .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#vxqtbierbt .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#vxqtbierbt .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#vxqtbierbt .gt_row {\n  padding-top: 7px;\n  padding-bottom: 7px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#vxqtbierbt .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vxqtbierbt .gt_stub_row_group {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 5px;\n  padding-right: 5px;\n  vertical-align: top;\n}\n\n#vxqtbierbt .gt_row_group_first td {\n  border-top-width: 2px;\n}\n\n#vxqtbierbt .gt_row_group_first th {\n  border-top-width: 2px;\n}\n\n#vxqtbierbt .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vxqtbierbt .gt_first_summary_row {\n  border-top-style: solid;\n  border-top-color: #D3D3D3;\n}\n\n#vxqtbierbt .gt_first_summary_row.thick {\n  border-top-width: 2px;\n}\n\n#vxqtbierbt .gt_last_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#vxqtbierbt .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vxqtbierbt .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#vxqtbierbt .gt_last_grand_summary_row_top {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-bottom-style: double;\n  border-bottom-width: 6px;\n  border-bottom-color: #D3D3D3;\n}\n\n#vxqtbierbt .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#vxqtbierbt .gt_table_body {\n  border-top-style: none;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #FFFFFF;\n}\n\n#vxqtbierbt .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#vxqtbierbt .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vxqtbierbt .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#vxqtbierbt .gt_sourcenote {\n  font-size: 90%;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vxqtbierbt .gt_left {\n  text-align: left;\n}\n\n#vxqtbierbt .gt_center {\n  text-align: center;\n}\n\n#vxqtbierbt .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#vxqtbierbt .gt_font_normal {\n  font-weight: normal;\n}\n\n#vxqtbierbt .gt_font_bold {\n  font-weight: bold;\n}\n\n#vxqtbierbt .gt_font_italic {\n  font-style: italic;\n}\n\n#vxqtbierbt .gt_super {\n  font-size: 65%;\n}\n\n#vxqtbierbt .gt_footnote_marks {\n  font-size: 75%;\n  vertical-align: 0.4em;\n  position: initial;\n}\n\n#vxqtbierbt .gt_asterisk {\n  font-size: 100%;\n  vertical-align: 0;\n}\n\n#vxqtbierbt .gt_indent_1 {\n  text-indent: 5px;\n}\n\n#vxqtbierbt .gt_indent_2 {\n  text-indent: 10px;\n}\n\n#vxqtbierbt .gt_indent_3 {\n  text-indent: 15px;\n}\n\n#vxqtbierbt .gt_indent_4 {\n  text-indent: 20px;\n}\n\n#vxqtbierbt .gt_indent_5 {\n  text-indent: 25px;\n}\n\n#vxqtbierbt .katex-display {\n  display: inline-flex !important;\n  margin-bottom: 0.75em !important;\n}\n\n#vxqtbierbt div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {\n  height: 0px !important;\n}\n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n  <thead>\n    <tr class=\"gt_heading\">\n      <td colspan=\"2\" class=\"gt_heading gt_title gt_font_normal gt_bottom_border\" style=\"font-family: 'Libre Franklin'; font-weight: 800; text-align: center;\"><span data-qmd-base64=\"PHN0cm9uZz5Nb2RlbCBEaWFnbm9zdGljIENoZWNrczwvc3Ryb25nPg==\"><span class='gt_from_md'><strong>Model Diagnostic Checks</strong></span></span></td>\n    </tr>\n    \n  </thead>\n  <tbody class=\"gt_table_body\">\n    <tr><td headers=\"Column1\" class=\"gt_row gt_left\" style=\"font-family: 'Source Sans Pro'; font-weight: 400; border-left-width: 1px; border-left-style: solid; border-left-color: black; border-right-width: 1px; border-right-style: solid; border-right-color: black; border-top-width: 1px; border-top-style: solid; border-top-color: black; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: black;\"><span data-qmd-base64=\"PHN0cm9uZz5Qb3N0ZXJpb3IgcHJlZGljdGl2ZSBjaGVja3M8L3N0cm9uZz48YnI+Q29tcGFyZSBzaW11bGF0ZWQgZGF0YSBmcm9tIHRoZSBmaXR0ZWQgbW9kZWwgdG8gYWN0dWFsIGRhdGEuIFRoaXMgcmV2ZWFscyBzeXN0ZW1hdGljIGRpc2NyZXBhbmNpZXMsIGhlbHBpbmcgYXNzZXNzIGlmIHRoZSBtb2RlbCBjYXB0dXJlcyBrZXkgZGF0YSBmZWF0dXJlcyBhbmQgaWYgdGhlIGNob3NlbiBkaXN0cmlidXRpb24gZmFtaWx5IGlzIGFwcHJvcHJpYXRlLg==\"><span class='gt_from_md'><strong>Posterior predictive checks</strong><br>Compare simulated data from the fitted model to actual data. This reveals systematic discrepancies, helping assess if the model captures key data features and if the chosen distribution family is appropriate.</span></span></td>\n<td headers=\"Column2\" class=\"gt_row gt_left\" style=\"font-family: 'Source Sans Pro'; font-weight: 400; border-left-width: 1px; border-left-style: solid; border-left-color: black; border-right-width: 1px; border-right-style: solid; border-right-color: black; border-top-width: 1px; border-top-style: solid; border-top-color: black; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: black;\"><span data-qmd-base64=\"PHN0cm9uZz5MaW5lYXJpdHk8L3N0cm9uZz48YnI+Q2hlY2tzIGlmIHByZWRpY3RvcnMgaGF2ZSBhIGxpbmVhciByZWxhdGlvbnNoaXAgd2l0aCB0aGUgb3V0Y29tZS4gQSBzdHJhaWdodCwgaG9yaXpvbnRhbCBsaW5lIHN1Z2dlc3RzIHRoZSBtb2RlbCBzcGVjaWZpY2F0aW9uIGlzIGFwcHJvcHJpYXRlLiBDdXJ2ZWQgb3Igc2xvcGVkIGxpbmVzIGluZGljYXRlIHBvdGVudGlhbCBub24tbGluZWFyIHJlbGF0aW9uc2hpcHMsIHNpZ25hbGluZyB0aGUgbmVlZCBmb3IgbW9kZWwgYWRqdXN0bWVudHMu\"><span class='gt_from_md'><strong>Linearity</strong><br>Checks if predictors have a linear relationship with the outcome. A straight, horizontal line suggests the model specification is appropriate. Curved or sloped lines indicate potential non-linear relationships, signaling the need for model adjustments.</span></span></td></tr>\n    <tr><td headers=\"Column1\" class=\"gt_row gt_left\" style=\"font-family: 'Source Sans Pro'; font-weight: 400; border-left-width: 1px; border-left-style: solid; border-left-color: black; border-right-width: 1px; border-right-style: solid; border-right-color: black; border-top-width: 1px; border-top-style: solid; border-top-color: black; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: black;\"><span data-qmd-base64=\"PHN0cm9uZz5Ib21vc2NlZGFzdGljaXR5L0hvbW9nZW5laXR5IG9mIHZhcmlhbmNlPC9zdHJvbmc+PGJyPkNoZWNrcyBmb3IgaG9tb3NjZWRhc3RpY2l0eSAoY29uc3RhbnQgdmFyaWFuY2UpLiBSZXNpZHVhbHMgc2hvdWxkIHNwcmVhZCBldmVubHkgYXJvdW5kIGEgaG9yaXpvbnRhbCBsaW5lIGFjcm9zcyBhbGwgcHJlZGljdG9yIHZhbHVlcy4gVW5ldmVuIHNwcmVhZCBzdWdnZXN0cyB2YXJpYW5jZSBpbmNvbnNpc3RlbmNpZXMsIHBvdGVudGlhbGx5IHJlcXVpcmluZyBtb2RlbCBhZGp1c3RtZW50cy4=\"><span class='gt_from_md'><strong>Homoscedasticity/Homogeneity of variance</strong><br>Checks for homoscedasticity (constant variance). Residuals should spread evenly around a horizontal line across all predictor values. Uneven spread suggests variance inconsistencies, potentially requiring model adjustments.</span></span></td>\n<td headers=\"Column2\" class=\"gt_row gt_left\" style=\"font-family: 'Source Sans Pro'; font-weight: 400; border-left-width: 1px; border-left-style: solid; border-left-color: black; border-right-width: 1px; border-right-style: solid; border-right-color: black; border-top-width: 1px; border-top-style: solid; border-top-color: black; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: black;\"><span data-qmd-base64=\"PHN0cm9uZz5PdXRsaWVyczwvc3Ryb25nPjxicj5JZGVudGlmaWVzIGluZmx1ZW50aWFsIG9ic2VydmF0aW9ucyB1c2luZyBDb29r4oCZcyBkaXN0YW5jZS4gUG9pbnRzIGJleW9uZCB0aGUgZGFzaGVkIGxpbmVzIGFyZSBvdXRsaWVycyB0aGF0IG1heSBleGNlc3NpdmVseSBpbXBhY3QgbW9kZWwgZXN0aW1hdGVzLCB3YXJyYW50aW5nIGZ1cnRoZXIgaW52ZXN0aWdhdGlvbiBvciBwb3RlbnRpYWwgcmVtb3ZhbC4=\"><span class='gt_from_md'><strong>Outliers</strong><br>Identifies influential observations using Cook‚Äôs distance. Points beyond the dashed lines are outliers that may excessively impact model estimates, warranting further investigation or potential removal.</span></span></td></tr>\n    <tr><td headers=\"Column1\" class=\"gt_row gt_left\" style=\"font-family: 'Source Sans Pro'; font-weight: 400; border-left-width: 1px; border-left-style: solid; border-left-color: black; border-right-width: 1px; border-right-style: solid; border-right-color: black; border-top-width: 1px; border-top-style: solid; border-top-color: black; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: black;\"><span data-qmd-base64=\"PHN0cm9uZz5NdWx0aS1Db2xsaW5lYXJpdHk8L3N0cm9uZz48YnI+RXZhbHVhdGVzIHByZWRpY3RvciBpbmRlcGVuZGVuY2UuIEhpZ2ggY29sbGluZWFyaXR5IGluZGljYXRlcyByZWR1bmRhbnQgcHJlZGljdG9ycyBvciBwb3NzaWJsZSB1bm9ic2VydmVkIHZhcmlhYmxlcyBpbmZsdWVuY2luZyBtdWx0aXBsZSBwcmVkaWN0b3JzLCB3aGljaCBjYW4gYWZmZWN0IGludGVycHJldGF0aW9uIGFuZCBtb2RlbCBzdGFiaWxpdHku\"><span class='gt_from_md'><strong>Multi-Collinearity</strong><br>Evaluates predictor independence. High collinearity indicates redundant predictors or possible unobserved variables influencing multiple predictors, which can affect interpretation and model stability.</span></span></td>\n<td headers=\"Column2\" class=\"gt_row gt_left\" style=\"font-family: 'Source Sans Pro'; font-weight: 400; border-left-width: 1px; border-left-style: solid; border-left-color: black; border-right-width: 1px; border-right-style: solid; border-right-color: black; border-top-width: 1px; border-top-style: solid; border-top-color: black; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: black;\"><span data-qmd-base64=\"PHN0cm9uZz5Ob3JtYWxpdHkgb2YgcmVzaWR1YWxzPC9zdHJvbmc+PGJyPlVzZXMgUS1RIHBsb3RzIHRvIGFzc2VzcyBpZiByZXNpZHVhbHMgZm9sbG93IGEgbm9ybWFsIGRpc3RyaWJ1dGlvbi4gUG9pbnRzIHNob3VsZCBhbGlnbiB3aXRoIHRoZSByZWZlcmVuY2UgbGluZTsgZGV2aWF0aW9ucyBzdWdnZXN0IHRoZSBtb2RlbCBwb29ybHkgcHJlZGljdHMgY2VydGFpbiBvdXRjb21lIHJhbmdlcywgcG90ZW50aWFsbHkgdmlvbGF0aW5nIG5vcm1hbGl0eSBhc3N1bXB0aW9ucy4=\"><span class='gt_from_md'><strong>Normality of residuals</strong><br>Uses Q-Q plots to assess if residuals follow a normal distribution. Points should align with the reference line; deviations suggest the model poorly predicts certain outcome ranges, potentially violating normality assumptions.</span></span></td></tr>\n  </tbody>\n  \n  \n</table>\n</div>\n```\n\n:::\n:::\n\n\n\nGiven these information about these plots.... don't you think there is something that doesn't add up???????? ü§î\n\nWell some assumptions are not really met aren't they!!! Look at those residuals!!! Not great...not really normal...not really straight!! üìâ\n\nDo not worry!! We have fit an extremely simple model in this tutorial... as you will see in the next tutorial this model is actually WRONG!!! Just do not worry...this model has served its purpose.......for now.\n\n::: callout-tip\nOne of the awesome features of `easystats` is its broad support for various model types. What's the big deal? Well, it means that the `check_model()` function adapts its checks based on the specific model you're using! This flexibility makes it an incredibly powerful and user-friendly tool. Most of the linear and non-linear models (or at least most of them) can be fed into the `check_model()` function, allowing you to easily verify if it meets the necessary assumptions.\n\n**Keep in mind: Always be aware of which assumptions your model should satisfy. We're not suggesting you use this function blindly! Instead, we're showing you how to efficiently plot all relevant assumptions in one go. It's simpler and quicker!!**\n:::\n\n## Statistical tests\n\nYou've probably noticed that we've been relying on visual checks so far. In our view, this is often the best approach, as statistical tests for model assumptions can sometime be overly stringent. However, there may be situations where you need to provide statistical evidence to support your model assumptions. This often happens when a reviewer (let's call them Reviewer 2, shall we?) insists on seeing numerical proof. Fortunately, `easystats` has got your back.\n\nHere are some examples of what you can use:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_normality(mod_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWarning: Non-normality of residuals detected (p < .001).\n```\n\n\n:::\n:::\n\n\n\nTo check the normality of our residuals and:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_homogeneity(mod_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.986).\n```\n\n\n:::\n:::\n\n\n\nto check homoscedasticity/homogeneity of variance. Again you can find all the function in the **performance** package (part of the Easystats collection)\n\n# END\n\nWell done reaching the end!!\n\nThis was a tough one!! But again, remember these are solid foundations that will come into use so many times in the future!!\n\nUnderstanding linear models is like learning to ride a bike - it might feel wobbly at first, but once you get it, you'll never forget! You now have the skills to interpret intercepts, coefficients, p-values, and interactions, plus you know how to check if your model is actually doing what it's supposed to do.\n",
    "supporting": [
      "LinearModels_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}