{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Bridging the Technological Gap Workshop (GAP)\"\n",
        "\n",
        "execute:\n",
        "  eval: false\n",
        "title-meta: \"GAP 2024\"\n",
        "author-meta: Tommaso Ghilardi\n",
        "description-meta: \"Data and resource for the workshop at GAP 2024\"\n",
        "keywords-meta: \"PsychoPy, Python, eye-tracking, tobii, tobii_research, experimental psychology, tutorial, experiment, DevStart, developmental science, workshop\"\n",
        "categories: Workhsop\n",
        "---\n",
        "\n",
        "\n",
        "Hello hello!!! This page has been created to provide support and resources for the tutorial that will take place during the [Bridging the Technological Gap Workshop](https://www.eva.mpg.de/de/comparative-cultural-psychology/events/2024-btg2/).\n",
        "\n",
        "# What will you learn?\n",
        "\n",
        "This workshop will focus on using Python to run and analyze an eye-tracking study.\n",
        "\n",
        "We will focus on:\n",
        "\n",
        "-   How to implement eye-tracking designs in Python\n",
        "\n",
        "-   How to interact with an eye-tracker via Python\n",
        "\n",
        "-   How to extract and visualize meaningful eye-tracking measures from the raw data\n",
        "\n",
        "# What will you need?\n",
        "\n",
        "## Python\n",
        "\n",
        "In this tutorial, our primary tool will be Python!! There are lots of ways to install python. [We recommend installing it via Miniconda](/CONTENT/GettingStarted/GettingStartedWithPython.qmd). However, for this workshop, the suggested way to install Python is using [Anaconda](https://www.anaconda.com/download/success).\n",
        "\n",
        "You might ask....Then which installation should I follow? Well, it doesn't really matter! Miniconda is a minimal installation of Anaconda. It lacks the GUI, but has all the main features. So follow whichever one you like more!\n",
        "\n",
        "Once you have it installed, we need a few more things. For the ***Gaze Tracking & Pupillometry Workshop*** (the part we will be hosting) we will need some specific libraries and files. We have tried our best to make everything as simple as possible:\n",
        "\n",
        "### Libraries\n",
        "\n",
        "We will be working with a conda environment (a self-contained directory that contains a specific collection of Python packages and dependencies, allowing you to manage different project requirements separately). To create this environment and install all the necessary libraries, all you need is this file:\n",
        "\n",
        "\n",
        "{{< downloadthis ../../resources/Workshops/GAP2024/psychopy_GAP.yml label=\"Psychopy.yml\" dname= \"psychopy_GAP\" type=\"secondary\" >}}\n",
        "\n",
        "\n",
        "Once you have downloaded the file, simply open the anaconda/miniconda terminal and type `conda env create -f`, then simply drag and drop the downloaded file onto the terminal. This will copy the filename with its absolute path. In my case it looked something like this:\n",
        "\n",
        "![](/resources/Workshops/GAP2024/exampleTerminal_gap2024.png){fig-align=\"center\" width=\"588\"}\n",
        "\n",
        "Now you will be asked to confirm a few things (by pressing `Y`) and after a while of downloading and installing you will have your new workshop environment called **Psychopy**!\n",
        "\n",
        "Now you should see a shortcut in your start menu called **Spyder(psychopy)**, just click on it to open spyder in our newly created environment. If you don't see it, just reopen the anaconda/miniconda terminal, activate your new environment by typing `conda activate psychopy` and then just type `spyder`.\n",
        "\n",
        "### Files\n",
        "\n",
        "We also need some files if you want to run the examples with us. Here you can download the zip files with everything you need:\n",
        "\n",
        "\n",
        "{{< downloadthis ../../resources/Workshops/GAP2024/GAP_2024.zip label=\"Files\" dname= \"Files_Gap\" icon=\"database-fill-down\" type=\"secondary\" >}}\n",
        "\n",
        "\n",
        "Once downloaded, simply extract the file by unzipping it. For our workshop we will work together in a folder that should look like this:\n",
        "\n",
        "![](/resources/Workshops/GAP2024/FinalFolder.png){fig-align=\"center\" width=\"611\"}\n",
        "\n",
        "If you have a similar folder... you are ready to go!!!!\n",
        "\n",
        "# Q&A\n",
        "\n",
        "We received many interesting questions during the workshop! We'll try to add new tutorials and pages to address your queries. However, since this website is a side project, it may take some time.\n",
        "\n",
        "In the meantime, we'll share our answers here. They may be less precise and exhaustive than what we'll have in the future, but they should still provide a good idea of how to approach things.\n",
        "\n",
        "## Videos\n",
        "\n",
        "We received several questions about working with videos and PsychoPy while doing eye-tracking. It can be quite tricky, but here are some tips:\n",
        "\n",
        "-   Make sure you're using the right codec.\n",
        "\n",
        "-   If you need to change the codec of the video, you can re-encode it using a tool like\n",
        "\n",
        "-   [Handbrake](https://handbrake.fr) (remember to set the constant framerate in the video option)\n",
        "\n",
        "Below, you'll find a code example that adapts our [Create an eye-tracking experiment](/CONTENT/EyeTracking/CreateAnEyetrackingExperiment.qmd) tutorial to work with a video file. The main differences are:\n",
        "\n",
        "-   We're showing a video after the fixation.\n",
        "\n",
        "-   We're saving triggers to our eye-tracking data and also saving the frame index at each sample (as a continuous number column).\n"
      ],
      "id": "2e471708"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import some libraries from PsychoPy\n",
        "from psychopy import core, event, visual, prefs\n",
        "prefs.hardware['audioLib'] = ['PTB']\n",
        "from psychopy import sound\n",
        "\n",
        "import tobii_research as tr\n",
        "\n",
        "\n",
        "#%% Functions\n",
        "\n",
        "# This will be called every time there is new gaze data\n",
        "def gaze_data_callback(gaze_data):\n",
        "    global trigger\n",
        "    global gaze_data_buffer\n",
        "    global winsize\n",
        "    global frame_indx\n",
        "    \n",
        "    # Extract the data we are interested in\n",
        "    t  = gaze_data.system_time_stamp / 1000.0\n",
        "    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n",
        "    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n",
        "    lp = gaze_data.left_eye.pupil.diameter\n",
        "    lv = gaze_data.left_eye.gaze_point.validity\n",
        "    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n",
        "    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n",
        "    rp = gaze_data.right_eye.pupil.diameter\n",
        "    rv = gaze_data.right_eye.gaze_point.validity\n",
        "        \n",
        "    # Add gaze data to the buffer \n",
        "    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,trigger, frame_indx))\n",
        "    trigger = ''\n",
        "    \n",
        "def write_buffer_to_file(buffer, output_path):\n",
        "\n",
        "    # Make a copy of the buffer and clear it\n",
        "    buffer_copy = buffer[:]\n",
        "    buffer.clear()\n",
        "    \n",
        "    # Define column names\n",
        "    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n",
        "               'R_X', 'R_Y', 'R_P', 'R_V', 'Event', 'FrameIndex']\n",
        "\n",
        "    # Convert buffer to DataFrame\n",
        "    out = pd.DataFrame(buffer_copy, columns=columns)\n",
        "    \n",
        "    # Check if the file exists\n",
        "    file_exists = not os.path.isfile(output_path)\n",
        "    \n",
        "    # Write the DataFrame to an HDF5 file\n",
        "    out.to_csv(output_path, mode='a', index =False, header = file_exists)\n",
        "    \n",
        "    \n",
        "    \n",
        "#%% Load and prepare stimuli\n",
        "\n",
        "os.chdir(r'C:\\Users\\tomma\\Desktop\\EyeTracking\\Files')\n",
        "\n",
        "# Winsize\n",
        "winsize = (960, 540)\n",
        "\n",
        "# create a window\n",
        "win = visual.Window(size = winsize,fullscr=False, units=\"pix\", screen=0)\n",
        "\n",
        "\n",
        "# Load images and video\n",
        "fixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\n",
        "Video = visual.MovieStim(win, filename='EXP\\\\Stimuli\\\\Video60.mp4',  loop=False, size=[600,380],volume =0.4, autoStart=True)  \n",
        "\n",
        "\n",
        "# Define the trigger and frame index variable to pass to the gaze_data_callback\n",
        "trigger = ''\n",
        "frame_indx = np.nan\n",
        "\n",
        "\n",
        "\n",
        "#%% Record the data\n",
        "\n",
        "# Find all connected eye trackers\n",
        "found_eyetrackers = tr.find_all_eyetrackers()\n",
        "\n",
        "# We will just use the first one\n",
        "Eyetracker = found_eyetrackers[0]\n",
        "\n",
        "#Start recording\n",
        "Eyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n",
        "\n",
        "# Crate empty list to append data\n",
        "gaze_data_buffer = []\n",
        "\n",
        "Trials_number = 10\n",
        "for trial in range(Trials_number):\n",
        "\n",
        "    ### Present the fixation\n",
        "    win.flip() # we flip to clean the window\n",
        "\n",
        "    \n",
        "    fixation.draw()\n",
        "    win.flip()\n",
        "    trigger = 'Fixation'\n",
        "    core.wait(1)  # wait for 1 second\n",
        "\n",
        "    Video.play()\n",
        "    trigger = 'Video'\n",
        "    while not Video.isFinished:\n",
        "\n",
        "        # Draw the video frame\n",
        "        Video.draw()\n",
        "\n",
        "        # Flip the window and add index to teh frame_indx\n",
        "        win.flip()\n",
        "        \n",
        "        # add which frame was just shown to the eyetracking data\n",
        "        frame_indx = Video.frameIndex\n",
        "        \n",
        "    Video.stop()\n",
        "    win.flip()\n",
        "\n",
        "\n",
        "    ### ISI\n",
        "    win.flip()    # we re-flip at the end to clean the window\n",
        "    clock = core.Clock()\n",
        "    write_buffer_to_file(gaze_data_buffer, 'DATA\\\\RAW\\\\Test.csv')\n",
        "    while clock.getTime() < 1:\n",
        "        pass\n",
        "    \n",
        "    ### Check for closing experiment\n",
        "    keys = event.getKeys() # collect list of pressed keys\n",
        "    if 'escape' in keys:\n",
        "        win.close()  # close window\n",
        "        Eyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\n",
        "        core.quit()  # stop study\n",
        "      \n",
        "win.close() # close window\n",
        "Eyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\n",
        "core.quit() # stop study"
      ],
      "id": "00a08533",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calibration\n",
        "\n",
        "We received a question about the calibration. How to change the focus time that the eye-tracking uses to record samples for each calibration point. Luckily, the function from the [Psychopy_tobii_infant](https://github.com/yh-luo/psychopy_tobii_infant) repository allows for an additional argument that specifies how long we want the focus time (default = 0.5s). Thus, you can simply change it by running it with a different value.\n",
        "\n",
        "Here below we changed the example of [Calibrating eye-tracking](CONTENT/EyeTracking/EyetrackingCalibration.qmd) by increasing the focus_time to 2s. You can increase or decrease it based on your needs!!\n"
      ],
      "id": "0e1c5eed"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from psychopy import visual, sound\n",
        "\n",
        "# import Psychopy tobii infant\n",
        "os.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\n",
        "from psychopy_tobii_infant import TobiiInfantController\n",
        "\n",
        "\n",
        "#%% window and stimuli\n",
        "winsize = [1920, 1080]\n",
        "win = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n",
        "\n",
        "# visual stimuli\n",
        "CALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n",
        "\n",
        "# video\n",
        "VideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n",
        "\n",
        "# sound\n",
        "Sound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n",
        "\n",
        "\n",
        "#%% Center face - screen\n",
        "\n",
        "# set video playing\n",
        "VideoGrabber.setAutoDraw(True)\n",
        "VideoGrabber.play()\n",
        "\n",
        "# show the relative position of the subject to the eyetracker\n",
        "EyeTracker.show_status()\n",
        "\n",
        "# stop the attention grabber\n",
        "VideoGrabber.setAutoDraw(False)\n",
        "VideoGrabber.stop()\n",
        "\n",
        "\n",
        "#%% Calibration\n",
        "\n",
        "# define calibration points\n",
        "CALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\n",
        "CALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n",
        "\n",
        "success = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound, focus_time=2)\n",
        "win.flip()"
      ],
      "id": "f0aefd9e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}