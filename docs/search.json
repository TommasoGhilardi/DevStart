[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "DevStart is an online hands-on manual for anyone who is approaching developmental psychology and developmental cognitive neuroscience for the first time, from master’s students and PhDs to postdocs. \nAssuming no a priori knowledge, this website will guide you through your first steps as a developmental researcher. You’ll find many examples and guidelines on how to grasp the basic principles of developmental psychology research, design and set up a study with different research methods, analyse data, and start programming.\n\n\nThere are many resources on the web to learn how to program, build experiments, and analyse data. However, they often assume basic skills in programming, and they don’t tackle the challenges that we encounter when testing babies and young children. This knowledge is often handed down from Professors and Postdocs to PhD students, or it is acquired through hours and hours of despair. \nWe tried to summarise all the struggles that we encountered during our PhDs and the solutions we came up with. With this website, we aim to offer you a solution to overcome these problems, and we invite anyone to help us and contribute to this open science framework! \n\n\n\nIf you’re an expert on a specific topic and you want to change something or add something, if you don’t agree with our pipeline, or you spot an error in our code, please get in touch! \nYou can find us at:\n\n t.ghilardi@bbk.ac.uk\n\n francesco.poli@donders.ru.nl\n\n g.serino@bbk.ac.uk\n\n\n\n\n\nThis is a story of bullying and love started in 2018 at the Donders Institute in the Netherlands. Tommaso and Francesco were living their best PhD life when Giulia bumped into them, looking for data to write her master’s thesis.\nUnited by a strong passion for research and a creepy attraction to complex and noisy datasets, they immediately got along well. We haven’t stopped collaborating since then.\n\n\n\n\n\n\nTommaso Ghilardi is the programmer behind the scenes. He excels in Python and is known for his impeccably clean code. If you explore our website, you're likely to come across his contributions. Tommaso's skills aren't limited to programming—he's capable of setting up a lab in just an hour. He's equally comfortable with research methods and a variety of programming languages. However, be cautious about imperfect code; Tommaso has a short fuse for anything less than perfect. He plays a significant role in shaping this website, making it what it is today. Thank you, Tommaso!\n\n\n\nPostdoc at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://tommasoghilardi.github.io\n t.ghilardi@bbk.ac.uk\n @TommasoGhi\n\n\n\n\n\n\n\n\nFrancesco Poli is the deep thinker among us. He's not just any ordinary thinker; he can build computational models to predict behavior and decipher your thoughts in the blink of an eye. Francesco's abilities extend to creating intricate mathematical theories to explain behavior patterns. He's adept at juggling multiple projects, though he might occasionally forget to reply to emails amidst his busy schedule. With Francesco, expect the extraordinary.\n\n\n\nPostDoc at the Donders Centre for Cognition.\n https://www.ru.nl/en/people/poli-f\n francesco.poli@donders.ru.nl\n @FrancescPoli\n\n\n\n\n\n\n\n\nGiulia Serino brings imagination to the table. Introduced to programming by Francesco and Tommaso, she has surpassed them to become a skilled programmer herself. Her experiments are a blend of art and science, often resembling graphic masterpieces. However, Giulia tends to get lost in the minutiae of her work. She has a reputation for disappearing, yet her curiosity leads her to explore every corner of Google and various repositories, piecing together her unique theories on attention development.\n\n\n\nPhD student in Developmental Cognitive Neuroscience at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://cbcd.bbk.ac.uk/people/students/giulia-serino\n g.serino@bbk.ac.uk\n @GiSerino\n\n\n\n\nWe tried our best to offer the best and most accurate solutions. However, do always check the code and if the outputs make sense. Please get in touch if you spot any errors.\nWe apologize in advance for our poor coding skills. Our scripts are not perfect, and they don’t mean to be. But, as Francesco always says, they work! And we hope they will support you during your PhD."
  },
  {
    "objectID": "index.html#why-did-we-create-it",
    "href": "index.html#why-did-we-create-it",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "There are many resources on the web to learn how to program, build experiments, and analyse data. However, they often assume basic skills in programming, and they don’t tackle the challenges that we encounter when testing babies and young children. This knowledge is often handed down from Professors and Postdocs to PhD students, or it is acquired through hours and hours of despair. \nWe tried to summarise all the struggles that we encountered during our PhDs and the solutions we came up with. With this website, we aim to offer you a solution to overcome these problems, and we invite anyone to help us and contribute to this open science framework!"
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "If you’re an expert on a specific topic and you want to change something or add something, if you don’t agree with our pipeline, or you spot an error in our code, please get in touch! \nYou can find us at:\n\n t.ghilardi@bbk.ac.uk\n\n francesco.poli@donders.ru.nl\n\n g.serino@bbk.ac.uk"
  },
  {
    "objectID": "index.html#who-are-we",
    "href": "index.html#who-are-we",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "This is a story of bullying and love started in 2018 at the Donders Institute in the Netherlands. Tommaso and Francesco were living their best PhD life when Giulia bumped into them, looking for data to write her master’s thesis.\nUnited by a strong passion for research and a creepy attraction to complex and noisy datasets, they immediately got along well. We haven’t stopped collaborating since then.\n\n\n\n\n\n\nTommaso Ghilardi is the programmer behind the scenes. He excels in Python and is known for his impeccably clean code. If you explore our website, you're likely to come across his contributions. Tommaso's skills aren't limited to programming—he's capable of setting up a lab in just an hour. He's equally comfortable with research methods and a variety of programming languages. However, be cautious about imperfect code; Tommaso has a short fuse for anything less than perfect. He plays a significant role in shaping this website, making it what it is today. Thank you, Tommaso!\n\n\n\nPostdoc at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://tommasoghilardi.github.io\n t.ghilardi@bbk.ac.uk\n @TommasoGhi\n\n\n\n\n\n\n\n\nFrancesco Poli is the deep thinker among us. He's not just any ordinary thinker; he can build computational models to predict behavior and decipher your thoughts in the blink of an eye. Francesco's abilities extend to creating intricate mathematical theories to explain behavior patterns. He's adept at juggling multiple projects, though he might occasionally forget to reply to emails amidst his busy schedule. With Francesco, expect the extraordinary.\n\n\n\nPostDoc at the Donders Centre for Cognition.\n https://www.ru.nl/en/people/poli-f\n francesco.poli@donders.ru.nl\n @FrancescPoli\n\n\n\n\n\n\n\n\nGiulia Serino brings imagination to the table. Introduced to programming by Francesco and Tommaso, she has surpassed them to become a skilled programmer herself. Her experiments are a blend of art and science, often resembling graphic masterpieces. However, Giulia tends to get lost in the minutiae of her work. She has a reputation for disappearing, yet her curiosity leads her to explore every corner of Google and various repositories, piecing together her unique theories on attention development.\n\n\n\nPhD student in Developmental Cognitive Neuroscience at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://cbcd.bbk.ac.uk/people/students/giulia-serino\n g.serino@bbk.ac.uk\n @GiSerino"
  },
  {
    "objectID": "index.html#warnings",
    "href": "index.html#warnings",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "We tried our best to offer the best and most accurate solutions. However, do always check the code and if the outputs make sense. Please get in touch if you spot any errors.\nWe apologize in advance for our poor coding skills. Our scripts are not perfect, and they don’t mean to be. But, as Francesco always says, they work! And we hope they will support you during your PhD."
  },
  {
    "objectID": "CONTENT/Workshops/BCCCD2024.html",
    "href": "CONTENT/Workshops/BCCCD2024.html",
    "title": "BCCCD2024",
    "section": "",
    "text": "Hello hello!!! This page has been created to provide support and resources for the tutorial and we presented at BCCC 2024."
  },
  {
    "objectID": "CONTENT/Workshops/BCCCD2024.html#software",
    "href": "CONTENT/Workshops/BCCCD2024.html#software",
    "title": "BCCCD2024",
    "section": "Software",
    "text": "Software\nIn this tutorial, our primary tool will be Python!! We recommend installing it via Miniconda. Our interaction with Python will primarily be through the Spyder IDE. You’re free to use any IDE of your choice, but if you’d like to follow along more smoothly, we suggest checking out our guide on how to install both Miniconda and Spyder: Getting started with Python.\nBesides Python, we’ll also be utilizing Psychopy! Psychopy is an awesome set of packages and functions designed for conducting psychological experiments. It’s available as a standalone software or can be installed as a Python package.\nBased on our experience, we find it more advantageous to use Psychopy as a Python package due to its increased flexibility. We provide this anaconda environment to easily create an virtual environment with both python, psychopy and all the libraries that we will need."
  },
  {
    "objectID": "CONTENT/Workshops/BCCCD2024.html#stimuli",
    "href": "CONTENT/Workshops/BCCCD2024.html#stimuli",
    "title": "BCCCD2024",
    "section": "Stimuli",
    "text": "Stimuli\nIn our workshop we will also make use of some stimuli to create a cool experiment. If you want to follow along you can dowload the stimuli here:"
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html",
    "title": "Starting with PsychoPy",
    "section": "",
    "text": "PsychoPy is an open source software package written in the Python programming language primarily for use in neuroscience and experimental psychology research. It’s one of our favorite ways to create experiments and we will use it through our tutorials.\nSo, let’s start and install PsychoPy!!!"
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#preparation",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#preparation",
    "title": "Starting with PsychoPy",
    "section": "Preparation:",
    "text": "Preparation:\nFirst thing first let’s import the relevant libraries and define the path where our stimuli are in. PsychoPy has a lot of different modules that allow us to interface with different type of stimuli and systems. For this tutorial\n\n# Import some libraries from PsychoPy and others\nimport os\nfrom psychopy import core\nfrom psychopy import visual  \nfrom psychopy import sound"
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#stimuli-1",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#stimuli-1",
    "title": "Starting with PsychoPy",
    "section": "Stimuli:",
    "text": "Stimuli:\nThe next step is to create the window The window is what we will show the stimuli in; it is the canvas in which to draw objects. For now we will create a small window of 960*540 pixels. In this way we will able to see the stimuli and still interact with the rest of our pc interface. In a real experiment we would probably set the window dimension to the entirety of the the display (Fullscreen).\n\n# Winsize\nwinsize = (1920, 1080)\n\n# create a window\nwin = visual.Window(size = winsize,fullscr=True, units=\"pix\", pos =(0,30), screen=1)\n\n#create a window\nwin = visual.Window([1920,1080], units=\"pix\")\n\nNow let’s import the stimuli that we will present in this tutorial. We have 5 stimuli:\n\na fixation cross that we will use to catch the attention of our participants\na circle that will be our cue that signal a rewarding trial\na square that will be our cue that signal a non-rewarding trial\na cartoon of a medal that will be our reward\na cartoon of an empty cloud that will be our non-reward\n\nOn top of these visual stimuli we will also import two sounds that will help us signal the type of trials. So:\n\na tada! winning sound\na papapaaa! losing sound\n\n\n\n\n\n\n\nTip\n\n\n\nWhen importing a visual stimulus we need to pass to the importing function in which window it will be displayed. In our case we will pass all of them the “win” window that we just created.\n\n\n\n#%% Load and prepare stimuli\n\n# Setting directory of our experiment\nos.chdir('C:\\\\Users\\\\CBCD\\\\Desktop\\\\BCCCD')\n\n\n# Load images\nfixation = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\fixation.png', size = (200, 200))\ncircle   = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\circle.png', size = (200, 200))\nsquare   = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\square.png', size = (200, 200))\nwinning   = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\winning.png', size = (200, 200), pos=(560,0))\nloosing  = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\loosing.png', size = (200, 200), pos=(-560,0))\n\n# Load sound\nwinning_sound = sound.Sound('EXP\\\\getting_started_psychopy\\\\winning.wav')\nlosing_sound = sound.Sound('EXP\\\\getting_started_psychopy\\\\loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, loosing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\n# Load images\nfixation = visual.ImageStim(win, image=Path + 'fixation.png', size = (100, 100))\ncircle   = visual.ImageStim(win, image=Path + 'circle.png', size = (100, 100))\nsquare   = visual.ImageStim(win, image=Path + 'square.png', size = (100, 100))\nwinning   = visual.ImageStim(win, image=Path + 'winning.png', size = (100, 100), pos=(250,0))\nloosing  = visual.ImageStim(win, image=Path + 'loosing.png', size = (100, 100), pos=(-250,0))\n\n# Load sound\nwinning_sound = sound.Sound(Path + 'winning.wav')\nlosing_sound = sound.Sound(Path + 'loosing.wav')\n\nNote that in this simple experiment we will present the reward always on the right and the non-rewards always on the left that’s why when we import the two rewards we set their pos to (250,0) and (-250,0). The first value indicates the number of pixels on the x-axis and the second the number of pixels on the y-axis."
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#show-a-visual-stimulus",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#show-a-visual-stimulus",
    "title": "Starting with PsychoPy",
    "section": "Show a visual stimulus:",
    "text": "Show a visual stimulus:\nNo we want to show a stimuli in the center of our window. To do so we will have to use the function “draw”. As the name suggests this function draws the stimulus that we want on the window.\nLet’s start with displaying the fixation cross in the center.\n\n# Draw the fixation\nfixation.draw()\n\nDo you see the fixation cross?????? Probably not!! This is because we have drawn the fixation cross but we have not refreshed the window. Psychopy allows you to draw as many stimuli as you want on a window but the changes are only shown when you “refresh” the window. To do so we need to use the “flip” function.\n\nwin.flip()\n\nPerfect!!!! The fixation cross is there. Before each flip we need to draw our objects. Otherwise we will only see the basic window with nothing in it. Let’s try!!! flip the window now.\n\n# Flipping the window (refreshing)\nwin.flip()\n\nThe fixation is gone again! Exactly as predicted. Flipping the window allows us to draw and show something new each frame. This means that the speed limit of our presentation is the actual frame rate of our display. If we have a 60Hz display we can present an image 60 times in a second.\nSo if we want to present our fixation for an entire second we would have to draw and flip it 60 times (our display has a refresh rate of 60Hz)! Let’s try:\n\nfor _ in range(60):\n    fixation.draw()\n    win.flip()\nwin.flip() # we re-flip at the end to clean the window\n\nNow we have shown the fixation for 1 second and then it disappeared. Nice!! However you probably have already figured out that what we have done was unnecessary. If we want to present a static stimulus for 1s we could have just drawn it, flip the window and then wait for 1s. But now you have an idea on how to show animated stimuli or even videos!!! AMAZING!!!.\nNow let’s try to show the fixation for 1s by just waiting.\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window"
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#play-a-sound",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#play-a-sound",
    "title": "Starting with PsychoPy",
    "section": "Play a sound:",
    "text": "Play a sound:\nWe have seen how to show a stimulus let’s now play the sounds that we have imported. This is extremely simple, we can just play() them:\n\nwinning_sound.play()\ncore.wait(2)\nlosing_sound.play()\n\nGreat now we have played our two sounds!!\n\n\n\n\n\n\nWarning\n\n\n\nWhen playing a sound the script will continue and will not wait for the sound to have finished playing. So if you play two sounds one after without waiting the two sounds will play overlapping. That’s why we have used core.wait(2), this tells PsychoPy to wait 2 seconds after starting to play the sound."
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#create-a-trial",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#create-a-trial",
    "title": "Starting with PsychoPy",
    "section": "Create a trial:",
    "text": "Create a trial:\nNow let’s try to put everything we have learned in one place and present one rewarding and one non-rewarding trial:\n\nwe present the fixation for 1s\nwe present one of the two cues for 3s\nwe wait 750ms of blank screen\nwe present the reward or the non-reward depending on the cue for 2s.\n\nIn the end we also close the window.\n\n###### 1st Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the winning cue\ncircle.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nwinning.draw()\nwin.flip()\nwinning_sound.play()\ncore.wait(2)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n###### 2nd Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the non-rewarding cue\nsquare.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nlosing.draw()\nwin.flip()\nlosing_sound.play()\ncore.wait(2)  # wait for 2 second\nwin.flip()    # we re-flip at the end to clean the window\n\n\nwin.close()  # let's close the window at the end of the trial"
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#isi",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#isi",
    "title": "Starting with PsychoPy",
    "section": "ISI",
    "text": "ISI\nAmazing, we’ve got two trials! But, these trials are back-to-back, which isn’t typically what we want. More often, we prefer a brief gap between trials, known as the Inter Stimulus Interval (ISI). We could introduce this interval by simply adding a core.wait(1), which would pause the script for a second. However, I’d like to introduce you to an alternative method to wait for this second, which will be useful in future tutorials.\nTo implement this ISI, we’ll create a psychopy core.Clock(). Once initiated, this clock begins to keep track of time, allowing us to check how much time has elapsed since the clock started at any given moment. We’ll then use a while loop to monitor the elapsed time and break this loop once a second has passed.\nThe while loop is a loop that will keep doing the same thing over and over again as long as a certain condition is true, here the passing of 1s.\n\n### ISI\nclock = core.Clock() # start clock\nwhile clock.getTime() &lt; 1:\n    pass\n\nPerfect we can now add this at the end of our trials!!"
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#stop-the-experiment",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#stop-the-experiment",
    "title": "Starting with PsychoPy",
    "section": "Stop the experiment",
    "text": "Stop the experiment\nFantastic, we’ve nearly have our study! However, studies often don’t run to completion, especially when we’re working with infants and children. More often than not, we need to halt the study prematurely. This could be due to the participant becoming fatigued or distracted, or perhaps we need to tweak some settings.\nHow can we accomplish this? Of course, we could just shut down Python and let the experiment crash… but surely, there’s a more elegant solution… And indeed, there is! In fact, there are numerous methods to achieve this, and we’re going to demonstrate one of the most straightforward and flexible ones to you.\nWe can use the event.getKeys() function to ask Psychopy to report any key that has been pressed during our trial. In our case, we will check if the ESC key has been pressed and if it has, we will simply close the window and stop the study.\n\n### Check for closing experiment\nkeys = event.getKeys() # collect list of pressed keys\nif 'escape' in keys:\n    win.close()  # close window\n    core.quit()  # stop study\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can add this check for closing the study at any point during the study. However, we recommend placing it at the end of each trial. This ensures that even if you stop the experiment, you will have a complete trial, making it easier to analyze data since you won't have any incomplete sections of the study.\nAlso, you can use this same method to pause the study or interact with its progress in general."
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#create-an-entire-experiment",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#create-an-entire-experiment",
    "title": "Starting with PsychoPy",
    "section": "Create an entire experiment",
    "text": "Create an entire experiment\nIn an experiment, we want more than 1 trial. Let’s then create an experiment with 10 trials. We just need to repeat what we have done above multiple times. However, we need to randomize the type of trials, otherwise, it would be too easy to learn. To do so, we will create a list of 0 and 1. where 0 would identify a rewarding trial and 1 would index a non-rewarding trial.\nTo properly utilize this list of 0 and 1, we will need to create other lists of our stimuli. This will make it easier to call the right stimuli depending on the trial. We can do so by:\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, loosing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\nPerfect!! Now we can put all the pieces together and run our experiment.\n\n\n\n\n\n\nNote\n\n\n\nIn this final script, we will change the dimension of the window we will use. Since in most of the experiments, we will want to use the entire screen to our disposal, we will set fullscr = True when defining the window. In addition, we will also change the position of the rewarding and non-rewarding stimulus since now the window is bigger.\n\n\n\n# Import some libraries from PsychoPy and others\nimport os\nfrom psychopy import core\nfrom psychopy import visual  \nfrom psychopy import sound\n\n\n\n#%% Load and prepare stimuli\n\n# Setting directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n# Winsize\nwinsize = (1920, 1080)\n\n# create a window\nwin = visual.Window(size = winsize,fullscr=False, units=\"pix\", pos =(0,30), screen=1)\n\n# Load images\nfixation = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\fixation.png', size = (200, 200))\ncircle   = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\circle.png', size = (200, 200))\nsquare   = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\square.png', size = (200, 200))\nwinning  = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\winning.png', size = (200, 200), pos=(560,0))\nloosing  = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\loosing.png', size = (200, 200), pos=(-560,0))\n\n# Load sound\nwinning_sound = sound.Sound('EXP\\\\getting_started_psychopy\\\\winning.wav')\nlosing_sound = sound.Sound('EXP\\\\getting_started_psychopy\\\\loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, loosing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\n\n#%% Trials\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n    fixation.draw()\n    win.flip()\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    core.wait(3)  # wait for 3 seconds\n\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n    sounds[trial].play()\n    core.wait(2)  # wait for 1 second\n    win.flip()    # we re-flip at the end to clean the window\n\n    ### ISI\n    clock = core.Clock() # start clock\n    while clock.getTime() &lt; 1:\n        pass\n      \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        core.quit()  # stop study\n        \nwin.close()\ncore.quit()"
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html",
    "title": "Using I2MC for robust fixation extraction",
    "section": "",
    "text": "When it comes to eye-tracking data, one of the most important things we want to figure out is fixations. Fixations are specific points in an eye-tracking dataset where a person’s gaze remains relatively still and focused on a particular area or object for a period of time. These moments represent critical instances when a person’s visual attention is focused on a particular point of interest.\nTypically, eye-tracking programs come with their own fixation detection algorithms that give us a rough idea of what the person was looking at. But here’s the problem: these tools aren’t always very good when it comes to data from infants and children. Why? Because infants and children can be all over the place! They move their heads, put their hands (or even feet) in front of their faces, close their eyes, or just look away. All of this makes the data a big mess that’s hard to make sense of with regular fixation detection algorithms. Because the data is so messy, it is difficult to tell which data points are part of the same fixation or different fixations.\nBut don’t worry! We’ve got a solution: I2MC.\nI2MC stands for “Identification by Two-Means Clustering”, and it was designed specifically for this kind of problem. It’s designed to deal with all kinds of noise, and even periods of data loss. In this tutorial, we’ll show you how to use I2MC to find fixations. We won’t get into the nerdy stuff about how it works - this is all about the practical side. If you’re curious about the science, you can read the article.\nNow that we’ve introduced I2MC, let’s get our hands dirty and see how to use it!"
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html#import-data",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html#import-data",
    "title": "Using I2MC for robust fixation extraction",
    "section": "Import data",
    "text": "Import data\nNow we will write a simple function to import our data. This step unfortunately will have to be adapted depending on the system you used to collect the data and the data structure you will have in the end. For this tutorial, you can use your data-set (probably you will have to adapt the importing function) or use our data that you can download from here LINK.\nLet’s create step by step our function to import the data\n\n# Load data\nraw_df = pd.read_csv(PATH_TO_DATA, delimiter=',')\n\nAfter reading the data we will create a new data-frame that we will fill with the information needed from our raw_df. this is the point that would change depending on you eye-tracked and data format. you will probably have to change the columns names to be sure to have the 5 relevant ones.\n\n# Create empty dataframe\ndf = pd.DataFrame()\n    \n# Extract required data\ndf['time'] = raw_df['Time']\ndf['L_X'] = raw_df['LeftX']\ndf['L_Y'] = raw_df['LeftY']\ndf['R_X'] = raw_df['RightX']\ndf['R_Y'] = raw_df['RightY']\n\nAfter selecting the relevant data we will perform some very basic processing. - Sometimes there could be weird peaks where one sample is (very) far outside the monitor. Here, we will count as missing any data that is more than one monitor distance outside the monitor. Tobii gives us the validity index of the measured eye, here when the validity is too low (&gt;1) we will consider the sample as missing\n\n# Sometimes we have weird peaks where one sample is (very) far outside the monitor. Here, count as missing any data that is more than one monitor distance outside the monitor.\n\n# Left eye\nlMiss1 = (df['L_X'] &lt; -res[0]) | (df['L_X']&gt;2*res[0])\nlMiss2 = (df['L_Y'] &lt; -res[1]) | (df['L_Y']&gt;2*res[1])\nlMiss  = lMiss1 | lMiss2 | (raw_df['ValidityLeft'] &gt; 1)\ndf.loc[lMiss,'L_X'] = np.NAN\ndf.loc[lMiss,'L_Y'] = np.NAN\n\n# Right eye\nrMiss1 = (df['R_X'] &lt; -res[0]) | (df['R_X']&gt;2*res[0])\nrMiss2 = (df['R_Y'] &lt; -res[1]) | (df['R_Y']&gt;2*res[1])\nrMiss  = rMiss1 | rMiss2 | (raw_df['ValidityRight'] &gt; 1)\ndf.loc[rMiss,'R_X'] = np.NAN\ndf.loc[rMiss,'R_Y'] = np.NAN\n\nPerfect!!!\n\nEverything into a function\nWe have read the data, extracted the relevant information and done some extremely basic processing rejecting data that had to be considered non valid. Now we will wrap this code in a function to make it easier to use with I2MC:\n\n# ===================================================================\n# Import data from Tobii TX300\n# ===================================================================\n\ndef tobii_TX300(fname, res=[1920,1080]):\n    '''\n    Imports data from Tobii TX300\n    \n    Parameters\n    ----------\n    fname : string\n        The file (filepath)\n    res : tuple\n        The (X,Y) resolution of the screen\n    \n    Returns\n    -------\n    df : pandas.DataFrame\n         Gaze data, with columns:\n         t : The sample times from the dataset\n         L_X : X positions from the left eye\n         L_Y : Y positions from the left eye\n         R_X : X positions from the right eye\n         R_Y : Y positions from the right eye\n    '''\n\n    # Load all data\n    raw_df = pd.read_csv(fname, delimiter=',')\n    df = pd.DataFrame()\n    \n    # Extract required data\n    df['time'] = raw_df['Time']\n    df['L_X'] = raw_df['LeftX']\n    df['L_Y'] = raw_df['LeftY']\n    df['R_X'] = raw_df['RightX']\n    df['R_Y'] = raw_df['RightY']\n    \n    ###\n    # Sometimes we have weird peaks where one sample is (very) far outside the\n    # monitor. Here, count as missing any data that is more than one monitor\n    # distance outside the monitor.\n    \n    # Left eye\n    lMiss1 = (df['L_X'] &lt; -res[0]) | (df['L_X']&gt;2*res[0])\n    lMiss2 = (df['L_Y'] &lt; -res[1]) | (df['L_Y']&gt;2*res[1])\n    lMiss  = lMiss1 | lMiss2 | (raw_df['ValidityLeft'] &gt; 1)\n    df.loc[lMiss,'L_X'] = np.NAN\n    df.loc[lMiss,'L_Y'] = np.NAN\n    \n    # Right eye\n    rMiss1 = (df['R_X'] &lt; -res[0]) | (df['R_X']&gt;2*res[0])\n    rMiss2 = (df['R_Y'] &lt; -res[1]) | (df['R_Y']&gt;2*res[1])\n    rMiss  = rMiss1 | rMiss2 | (raw_df['ValidityRight'] &gt; 1)\n    df.loc[rMiss,'R_X'] = np.NAN\n    df.loc[rMiss,'R_Y'] = np.NAN\n    \n    return(df)\n\n\n\nFind our data\nNice!! we have our import function that we will use to read our data. Now, let’s find our data! To do this, we will use the glob library, which is a handy tool for finding files in Python. Before that let’s set our working directory. The working directory is the folder where we have all our scripts and data. We can set it using the os library:\n\nimport os\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\nThis is my directory, you will have something different, you need to change it to where your data are. Once you are done with that we can use glob to find our data files. In the code below, we are telling Python to look for files with a .csv extension in a specific folder on our computer. Let’s import glob and then let’s find the files:\n\nimport glob\ndata_files = glob.glob('C:\\\\Users\\\\tomma\\\\surfdrive - Ghilardi, T. (Tommaso)@surfdrive.surf.nl\\\\Documentation\\Working\\\\Eyetracking\\\\data\\\\**\\\\*.csv', recursive = True)\n\n\n/home/geeks/Desktop/gfg/: This is the path where we want to start our search.\n\\*\\*: This special symbol tells Python to search in all the subfolders (folders within folders) under our starting path.\n/*.csv: We’re asking Python to look for files with names ending in “.csv”.\nrecursive=True: This option means that Python should search for files not just in the immediate subfolders but in all the subfolders within subfolders, and so on.\n\nSo, when we run this code, Python will find and give us a list of all the “.csv” files located in any subfolder within our specified path. This makes it really convenient to find and work with lots of files at once.\n\n\nDefine the output folder\nBefore doing anything else I would suggest creating a folder where we will save the output of I2MC. We will create a folder called i2mc_output. Using the os library we make sure the output folder doesn’t exist (os.path.isdir(output_fodler)) and then we create it (os.mkdir(output_fodler))\n\nimport os\n\n# define the output folder\noutput_fodler = 'DATA\\\\i2mc_output' # define folder path\\name\n\n# Create the folder\nif not os.path.isdir(output_fodler):\n   os.mkdir(output_fodler)\n\n\n\nI2MC settings\nNow that we’ve got our data, know how to import it using glob and we have out output folder, we’re all set to run I2MC. But wait, before we dive in, we need to set up a few things. These settings are like the instructions we give to I2MC before it does its magic. The default settings usually work just fine for most situations. You can keep them as they are and proceed. If you’re curious about what each of these settings does, you can explore the original I2MC article for a detailed understanding. Here I’ve added a general explanation about what each setting does. Once you’ve read through the instructions and have a clear understanding, you can customize the settings to match your specific requirements.\nLet’s define these settings:\n\n# =============================================================================\n# NECESSARY VARIABLES\n\nopt = {}\n# General variables for eye-tracking data\nopt['xres']         = 1920.0                # maximum value of horizontal resolution in pixels\nopt['yres']         = 1080.0                # maximum value of vertical resolution in pixels\nopt['missingx']     = np.NAN          # missing value for horizontal position in eye-tracking data (example data uses -xres). used throughout the algorithm as signal for data loss\nopt['missingy']     = np.NAN          # missing value for vertical position in eye-tracking data (example data uses -yres). used throughout algorithm as signal for data loss\nopt['freq']         = 300.0                 # sampling frequency of data (check that this value matches with values actually obtained from measurement!)\n\n# Variables for the calculation of visual angle\n# These values are used to calculate noise measures (RMS and BCEA) of\n# fixations. The may be left as is, but don't use the noise measures then.\n# If either or both are empty, the noise measures are provided in pixels\n# instead of degrees.\nopt['scrSz']        = [50.9174, 28.6411]    # screen size in cm\nopt['disttoscreen'] = 65.0                  # distance to screen in cm.\n\n# Options of example script\ndo_plot_data = True # if set to True, plot of fixation detection for each trial will be saved as png-file in output folder.\n# the figures works best for short trials (up to around 20 seconds)\n\n# =============================================================================\n# OPTIONAL VARIABLES\n# The settings below may be used to adopt the default settings of the\n# algorithm. Do this only if you know what you're doing.\n\n# # STEFFEN INTERPOLATION\nopt['windowtimeInterp']     = 0.1                           # max duration (s) of missing values for interpolation to occur\nopt['edgeSampInterp']       = 2                             # amount of data (number of samples) at edges needed for interpolation\nopt['maxdisp']              = opt['xres']*0.2*np.sqrt(2)    # maximum displacement during missing for interpolation to be possible\n\n# # K-MEANS CLUSTERING\nopt['windowtime']           = 0.2                           # time window (s) over which to calculate 2-means clustering (choose value so that max. 1 saccade can occur)\nopt['steptime']             = 0.02                          # time window shift (s) for each iteration. Use zero for sample by sample processing\nopt['maxerrors']            = 100                           # maximum number of errors allowed in k-means clustering procedure before proceeding to next file\nopt['downsamples']          = [2, 5, 10]\nopt['downsampFilter']       = False                         # use chebychev filter when downsampling? Its what matlab's downsampling functions do, but could cause trouble (ringing) with the hard edges in eye-movement data\n\n# # FIXATION DETERMINATION\nopt['cutoffstd']            = 2.0                           # number of standard deviations above mean k-means weights will be used as fixation cutoff\nopt['onoffsetThresh']       = 3.0                           # number of MAD away from median fixation duration. Will be used to walk forward at fixation starts and backward at fixation ends to refine their placement and stop algorithm from eating into saccades\nopt['maxMergeDist']         = 30.0                          # maximum Euclidean distance in pixels between fixations for merging\nopt['maxMergeTime']         = 30.0                          # maximum time in ms between fixations for merging\nopt['minFixDur']            = 40.0                          # minimum fixation duration after merging, fixations with shorter duration are removed from output\n\n\n\nRun I2MC\nNow we can finally run I2MC on all our files. To do so we will make for loop that will iterate between all the files and: - create a folder for each subject - import the data with the function created before - run I2MC on the file - save the output file and the plot .\n\n#%% Run I2MC\n\nfor file_idx, file in enumerate(data_files):\n    print('Processing file {} of {}'.format(file_idx + 1, len(data_files)))\n\n    # Extract the name form the file path\n    name = os.path.splitext(os.path.basename(file))[0]\n    \n    # Create the folder the specific subject\n    subj_folder = os.path.join(output_fodler, name)\n    if not os.path.isdir(subj_folder):\n       os.mkdir(subj_folder)\n       \n    # Import data\n    data = tobii_TX300(file, [opt['xres'], opt['yres']])\n\n    # Run I2MC on the data\n    fix,_,_ = I2MC.I2MC(data,opt)\n\n    ## Create a plot of the result and save them\n    if do_plot_data:\n        # pre-allocate name for saving file\n        save_plot = os.path.join(subj_folder, name+'.png')\n        f = I2MC.plot.data_and_fixations(data, fix, fix_as_line=True, res=[opt['xres'], opt['yres']])\n        # save figure and close\n        f.savefig(save_plot)\n        plt.close(f)\n\n    # Write data to file after make it a dataframe\n    fix['participant'] = name\n    fix_df = pd.DataFrame(fix)\n    save_file = os.path.join(subj_folder, name+'.csv')\n    fix_df.to_csv(save_file)"
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html#we-are-done",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html#we-are-done",
    "title": "Using I2MC for robust fixation extraction",
    "section": "WE ARE DONE!!!!!",
    "text": "WE ARE DONE!!!!!\nCongratulations on reaching this point! By now, you should have new files containing valuable information from I2MC.\nBut what exactly does I2MC tell us?\nI2MC provides us with a data frame that contains various pieces of information:\nWhat I2MC Returns:\n\ncutoff: A number representing the cutoff used for fixation detection.\nstart: An array holding the indices where fixations start.\nend: An array holding the indices where fixations end.\nstartT: An array containing the times when fixations start.\nendT: An array containing the times when fixations end.\ndur: An array storing the durations of fixations.\nxpos: An array representing the median horizontal position for each fixation in the trial.\nypos: An array representing the median vertical position for each fixation in the trial.\nflankdataloss: A boolean value (1 or 0) indicating whether a fixation is flanked by data loss (1) or not (0).\nfracinterped: A fraction that tells us the amount of data loss or interpolated data in the fixation data.\n\nIn simple terms, I2MC helps us understand where and for how long a person’s gaze remains fixed during an eye-tracking experiment. This is just the first step. Now that we have our fixations, we’ll need to use them to extract the information we’re interested in. Typically, this involves using the raw data to understand what was happening at each specific time point and using the data from I2MC to determine where the participant was looking at that time. This will be covered in a new tutorial. For now, you’ve successfully completed the preprocessing of your eye-tracking data, extracting a robust estimation of participants’ fixations!!\n\n\n\n\n\n\nWarning\n\n\n\nCaution: This tutorial is simplified and assumes the following:\n\nEach participant has only one file (1 trial).\nAll files contain data.\nThe data is relatively clean (I2MC won’t throw any errors).\nAnd so on.\n\nIf your data doesn’t match these assumptions, you may need to modify the script to handle any discrepancies.\nFor a more comprehensive example and in-depth usage, check out the I2MC repository. It provides a more complete example with data checks for missing data and potential errors. Now that you’ve understood the basics here, interpreting that example should be much easier. If you encounter any issues while running the script, you can give that example a try or reach out to us via email!!!"
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html#entire-script",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html#entire-script",
    "title": "Using I2MC for robust fixation extraction",
    "section": "Entire script",
    "text": "Entire script\nTo make it simple here is the entire script that we wrote together!!!\n\nimport os\nimport glob\nimport I2MC\nimport pandas as pd\n\n\n# =============================================================================\n# Import data from Tobii TX300\n# =============================================================================\n\ndef tobii_TX300(fname, res=[1920,1080]):\n    '''\n    Imports data from Tobii TX300\n\n    Parameters\n    ----------\n    fname : string\n        The file (filepath)\n    res : tuple\n        The (X,Y) resolution of the screen\n\n    Returns\n    -------\n    df : pandas.DataFrame\n         Gaze data, with columns:\n         t : The sample times from the dataset\n         L_X : X positions from the left eye\n         L_Y : Y positions from the left eye\n         R_X : X positions from the right eye\n         R_Y : Y positions from the right eye\n    '''\n\n    # Load all data\n    raw_df = pd.read_csv(fname, delimiter=',')\n    df = pd.DataFrame()\n\n    # Extract required data\n    df['time'] = raw_df['Time']\n    df['L_X'] = raw_df['LeftX']\n    df['L_Y'] = raw_df['LeftY']\n    df['R_X'] = raw_df['RightX']\n    df['R_Y'] = raw_df['RightY']\n\n    ###\n    # Sometimes we have weird peaks where one sample is (very) far outside the\n    # monitor. Here, count as missing any data that is more than one monitor\n    # distance outside the monitor.\n\n    # Left eye\n    lMiss1 = (df['L_X'] &lt; -res[0]) | (df['L_X']&gt;2*res[0])\n    lMiss2 = (df['L_Y'] &lt; -res[1]) | (df['L_Y']&gt;2*res[1])\n    lMiss  = lMiss1 | lMiss2 | (raw_df['ValidityLeft'] &gt; 1)\n    df.loc[lMiss,'L_X'] = np.NAN\n    df.loc[lMiss,'L_Y'] = np.NAN\n\n    # Right eye\n    rMiss1 = (df['R_X'] &lt; -res[0]) | (df['R_X']&gt;2*res[0])\n    rMiss2 = (df['R_Y'] &lt; -res[1]) | (df['R_Y']&gt;2*res[1])\n    rMiss  = rMiss1 | rMiss2 | (raw_df['ValidityRight'] &gt; 1)\n    df.loc[rMiss,'R_X'] = np.NAN\n    df.loc[rMiss,'R_Y'] = np.NAN\n\n    return(df)\n\n\n\n#%% Preparation\n\n# Settign the working directory\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n# Find the files\ndata_files = glob.glob('DATA\\\\RAW\\\\**\\\\*.csv', recursive = True) # find all the files\n\n# define the output folder\noutput_fodler = 'DATA\\\\i2mc_output' # define folder path\\name\n\n# Create the outputfolder\nif not os.path.isdir(output_fodler):\n   os.mkdir(output_fodler)\n\n\n# =============================================================================\n# NECESSARY VARIABLES\n\nopt = {}\n# General variables for eye-tracking data\nopt['xres']         = 1920.0                # maximum value of horizontal resolution in pixels\nopt['yres']         = 1080.0                # maximum value of vertical resolution in pixels\nopt['missingx']     = np.NAN                # missing value for horizontal position in eye-tracking data (example data uses -xres). used throughout the algorithm as signal for data loss\nopt['missingy']     = np.NAN                # missing value for vertical position in eye-tracking data (example data uses -yres). used throughout algorithm as signal for data loss\nopt['freq']         = 300.0                 # sampling frequency of data (check that this value matches with values actually obtained from measurement!)\n\n# Variables for the calculation of visual angle\n# These values are used to calculate noise measures (RMS and BCEA) of\n# fixations. The may be left as is, but don't use the noise measures then.\n# If either or both are empty, the noise measures are provided in pixels\n# instead of degrees.\nopt['scrSz']        = [50.9174, 28.6411]    # screen size in cm\nopt['disttoscreen'] = 65.0                  # distance to screen in cm.\n\n# Options of example script\ndo_plot_data = True # if set to True, plot of fixation detection for each trial will be saved as png-file in output folder.\n# the figures works best for short trials (up to around 20 seconds)\n\n# =============================================================================\n# OPTIONAL VARIABLES\n# The settings below may be used to adopt the default settings of the\n# algorithm. Do this only if you know what you're doing.\n\n# # STEFFEN INTERPOLATION\nopt['windowtimeInterp']     = 0.1                           # max duration (s) of missing values for interpolation to occur\nopt['edgeSampInterp']       = 2                             # amount of data (number of samples) at edges needed for interpolation\nopt['maxdisp']              = opt['xres']*0.2*np.sqrt(2)    # maximum displacement during missing for interpolation to be possible\n\n# # K-MEANS CLUSTERING\nopt['windowtime']           = 0.2                           # time window (s) over which to calculate 2-means clustering (choose value so that max. 1 saccade can occur)\nopt['steptime']             = 0.02                          # time window shift (s) for each iteration. Use zero for sample by sample processing\nopt['maxerrors']            = 100                           # maximum number of errors allowed in k-means clustering procedure before proceeding to next file\nopt['downsamples']          = [2, 5, 10]\nopt['downsampFilter']       = False                         # use chebychev filter when downsampling? Its what matlab's downsampling functions do, but could cause trouble (ringing) with the hard edges in eye-movement data\n\n# # FIXATION DETERMINATION\nopt['cutoffstd']            = 2.0                           # number of standard deviations above mean k-means weights will be used as fixation cutoff\nopt['onoffsetThresh']       = 3.0                           # number of MAD away from median fixation duration. Will be used to walk forward at fixation starts and backward at fixation ends to refine their placement and stop algorithm from eating into saccades\nopt['maxMergeDist']         = 30.0                          # maximum Euclidean distance in pixels between fixations for merging\nopt['maxMergeTime']         = 30.0                          # maximum time in ms between fixations for merging\nopt['minFixDur']            = 40.0                          # minimum fixation duration after merging, fixations with shorter duration are removed from output\n\n\n\n#%% Run I2MC\n\nfor file_idx, file in enumerate(data_files):\n    print('Processing file {} of {}'.format(file_idx + 1, len(data_files)))\n\n    # Extract the name form the file path\n    name = os.path.splitext(os.path.basename(file))[0]\n    \n    # Create the folder the specific subject\n    subj_folder = os.path.join(output_fodler, name)\n    if not os.path.isdir(subj_folder):\n       os.mkdir(subj_folder)\n       \n    # Import data\n    data = tobii_TX300(file, [opt['xres'], opt['yres']])\n\n    # Run I2MC on the data\n    fix,_,_ = I2MC.I2MC(data,opt)\n\n    ## Create a plot of the result and save them\n    if do_plot_data:\n        # pre-allocate name for saving file\n        save_plot = os.path.join(subj_folder, name+'.png')\n        f = I2MC.plot.data_and_fixations(data, fix, fix_as_line=True, res=[opt['xres'], opt['yres']])\n        # save figure and close\n        f.savefig(save_plot)\n        plt.close(f)\n\n    # Write data to file after make it a dataframe\n    fix['participant'] = name\n    fix_df = pd.DataFrame(fix)\n    save_file = os.path.join(subj_folder, name+'.csv')\n    fix_df.to_csv(save_file)"
  },
  {
    "objectID": "CONTENT/EyeTracking/EyetrackingCalibration.html",
    "href": "CONTENT/EyeTracking/EyetrackingCalibration.html",
    "title": "EyetrackingCalibration",
    "section": "",
    "text": "However, using the SDK is not very easy. But don’t worry! We have a solution for you. We can use Psychopy_tobii_infant, a wrapper around the Tobii SDK that is specially designed for running infant-friendly studies. This code collection allows us to use the Tobii SDK with the Psychopy interface.\nTo use Psychopy_tobii_infant, you need to go to this GitHub page: https://github.com/yh-luo/psychopy_tobii_infant. On this page, you can click on &lt;&gt;Code and then on Download ZIP as shown below:\n\n\n\n\n\nPerfect!! Now that we have downloaded the code as a zip file we need to:\n\nextract the file\nidentify the folder ‘psychopy_tobii_infant’\ncopy this folder in the same location of your eye-tracking experiment script\n\nYou should end up like with something like this:\n\n\nNow we are all set and ready to go !!!!!!!!!!\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html",
    "title": "Create an eye-tracking experiment",
    "section": "",
    "text": "This page will show you how to collect eye-tracking data in a simple Psychopy paradigm. We will use the same paradigm that we built together in the Getting started with Psychopy tutorial. If you have not done that tutorial yet, please go through it first."
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#install",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#install",
    "title": "Create an eye-tracking experiment",
    "section": "Install",
    "text": "Install\nTo install the Python Tobii SDK, we can simply run this command in our conda terminal:\npip install tobii_research\nGreat! We have installed the Tobii SDK."
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#connect-to-the-eye-tracker",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#connect-to-the-eye-tracker",
    "title": "Create an eye-tracking experiment",
    "section": "Connect to the eye-tracker",
    "text": "Connect to the eye-tracker\nSo how does this library work, how do we connect to the eye-tracker and collect our data? Very good questions!\nThe tobii_research documentation is quite extensive and describes in detail a lot of functions and data classes that are very useful. However, we don’t need much to start our experiment.\nFirst we need to identify all the eye trackers connected to our computer. Yes, plural, tobii_research will return a list of all the eye trackers connected to our computer. 99.99999999% of the time you will only have 1 eye tracker connected, so we can just select the first (and usually only) eye tracker found.\n\n# Import tobii_research library\nimport tobii_research as tr\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\nPerfect!! We have identified our eye-trackers, and we have selected the first one (and only).\nWe are now ready to use our eye-tracker to collect some data… but how?"
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#collect-data",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#collect-data",
    "title": "Create an eye-tracking experiment",
    "section": "Collect data",
    "text": "Collect data\nTobii_research has a cool way of telling us what data we are collecting at each time point. It uses a callback function. What is a callback function, you ask? It is a function that tobii runs each time it has a new data point. Let’s say we have an eye tracker that collects data at 300Hz (300 samples per second): the function will be called every time the tobii has one of those 300 samples.\nThis callback function will give us a gaze_data object. This object contains multiple information of that collected sample and we can simply select the information we care about. In our case we want:\n\nthe system_time_stamp, our time variable\nthe left_eye.gaze_point.position_on_display_area, it contains the coordinates on the screen of the left eye (both x and y)\nthe right_eye.gaze_point.position_on_display_area, it contains the coordinates on the screen of the right eye (both x and y)\nthe left_eye.pupil.diameter, is the pupil diameter of the left eye\nthe right_eye.pupil.diameter, is the pupil diameter of the right eye\nthe left_eye.gaze_point.validity, this is a value that tells us whether we think the recognition is ok or not\n\nHere is our callback function:\n\ndef gaze_data_callback(gaze_data):\n\n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0]\n    ly = gaze_data.left_eye.gaze_point.position_on_display_area[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0]\n    ry = gaze_data.right_eye.gaze_point.position_on_display_area[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n\nHow we said, this function will be called every time tobii has a new data-point. COOL!! Now we need to tell tobii to run this function we have created. This is very simple, we can just do the following:\n\n# Start the callback function\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\nWe are telling tobii that we are interested in the EYETRACKER_GAZE_DATA and that we want it to pass it to our function gaze_data_callback."
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#triggersevents",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#triggersevents",
    "title": "Create an eye-tracking experiment",
    "section": "Triggers/Events",
    "text": "Triggers/Events\nAs we have seen, our callback function can access the tobii data and tell us what it is for each sample. Just one little piece missing… We want to know what we presented and when. In most studies, we present stimuli that can be pictures, sounds or even videos. For the following analysis, it is important to know at what exact point in time we presented these stimuli.\nLuckily there is a simple way we can achieve this. We can set a text string that our callback function can access and include in our data. To make sure that our callback function can access this variable we will use the global keyword. This makes sure that we can read/modify a variable that exists outside of the function.\nIn this way, each time the callback function will be run, it will also have access to the trigger variable. We save the trigger to the ev variable and we set it back to an empty string \"\" .\nThis means that we can set the trigger to whatever string we want and when we set it it will be picked up by the callback function.\n\ndef gaze_data_callback(gaze_data):\n    global trigger\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n    \n    \ntrigger = ''\n\n# Time passes\n# when you present a stimulus you can set trigger to a string that will be captured by the callabck function\n\ntrigger = 'Presented Stimulus'\n\nPerfect! Now we have 1. a way to access the data from the eye-tracker and 2. know exactly what stimuli we are presenting the participant and when."
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#correct-the-data",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#correct-the-data",
    "title": "Create an eye-tracking experiment",
    "section": "Correct the data",
    "text": "Correct the data\nTobii presents gaze data in a variety of formats. The one we’re most interested in is the Active Display Coordinate System (ADCS). This system maps all gaze data onto a 2D coordinate system that aligns with the Active Display Area. When an eye tracker is used with a monitor, the Active Display Area refers to the display area that doesn’t include the monitor frame. In the ADCS system, the point (0, 0) represents the upper left corner of the screen, and (1, 1) represents the lower right corner.\nWhile this coordinate system is perfectly acceptable, it might cause some confusion when we come to analyze and plot the data. This is because in most systems, the data’s origin is located in the lower left corner, not the upper left. It might seem a bit complicated, but the image below will make everything clear.\n\n\n\n\n\nFor this reason, we typically adjust the data to position the origin in the bottom left corner. This can be achieved by subtracting the gaze coordinates from the maximum window height size.\nIn addition to the origin point issue, the gaze coordinates are reported between 0 and 1. It’s often more convenient to handle data in pixels, so we can transform our data into pixels. This is done by multiplying the obtained coordinates by the pixel dimensions of our screen.\nLastly, we also modify the time column. Tobii provides data in microseconds, but such precision isn’t necessary for our purposes, so we convert it to milliseconds by dividing by 1000.\n\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global winsize\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n    \n    \ntrigger = ''\nwinsize = (1920, 1080)"
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#save-the-data",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#save-the-data",
    "title": "Create an eye-tracking experiment",
    "section": "Save the data",
    "text": "Save the data\nYou might have noticed that we get the data in our callback function but don’t actually save it anywhere. So how to save them? There are two main approaches we can use:\n\nWe could have a saving function inside our callback that could append the new data to a .csv each time the callback is called.\nWe could append the data to a list. Once the experiment is finished we could save our data.\n\nThese two approaches have however some weaknesses. The first could slow down the callback function if our PC is not performing or if we are sampling at a very high sampling rate. The second is potentially faster, but if anything happens during the study which makes python crash (and trust me, it can happen…..) you would lose all your data.\nWhat is the solution you ask? A mixed approach!!!!!!!\nWe can store our data in a list and save it during the less important parts of the study, for example the Inter Stimulus Interval (the time between a trial and another). So let’s write a function to do exactly that.\nLets first create an empty list that we will fill with out data from the callback function. As before, we make sure that our callback function can access this list and append the new data we will use the global keyword.\n\n# Create an empty list we will append our data to\ngaze_data_buffer = []\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global gaze_data_buffer\n    global trigger\n    global winsize\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n        \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,ev))\n\nNow the gaze_data_buffer will be filled with the data we extract. Let’s save this list then.\nWe will first make a copy of the list, and then empty the original list. In this way, we have our data stored, while the original list is empty and can be filled with new data.\nAfter creating a copy, we use pandas to transform the list into a data frame and save it to a csv file. Using mode = 'a' we tell pandas to append the new data to the existing .csv. If this is the first time we are trying to save the data the .csv does not yet exist, so pandas will create a new csv instead.\n\ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names for the dataframe\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = not os.path.isfile(output_path)\n    \n    # Write the DataFrame to an HDF5 file\n    out.to_csv(output_path, mode='a', index =False, header = file_exists)"
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#short-recap-of-the-paradigm",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#short-recap-of-the-paradigm",
    "title": "Create an eye-tracking experiment",
    "section": "Short recap of the paradigm",
    "text": "Short recap of the paradigm\nAs we already mentioned, we will use the experimental design that we created in Getting started with Psychopy as a base and we will add things to it to make it an eye-tracking study. If you don’t remember the paradigm please give it a rapid look as we will not go into much detail about each specific part of it.\nHere a very short summary of what the design was: After a fixation cross, two shapes can be presented: a circle or a square. The circle indicates that a reward will appear on the right of the screen while the square predicts the appearance of an empty cloud on the left."
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#combine-things",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#combine-things",
    "title": "Create an eye-tracking experiment",
    "section": "Combine things",
    "text": "Combine things\nLet’s try to build together the experiment then.\n\nImport and functions\nTo start, let’s import the libraries and define the two functions that we create before\n\nimport os\nimport glob\nimport pandas as pd\n\n# Import some libraries from PsychoPy\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound\n\nimport tobii_research as tr\n\n\n#%% Functions\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global gaze_data_buffer\n    global winsize\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,ev))\n    \n        \ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = not os.path.isfile(output_path)\n    \n    # Write the DataFrame to an HDF5 file\n    out.to_csv(output_path, mode='a', index =False, header = file_exists)\n\n\n\nLoad the stimuli\nNow we are going to set a few settings, such as the screen size, create a Psychopy window, load the stimuli and then prepare the trial definition. This is exactly the same as we did in the previous Psychopy tutorial.\n\n#%% Load and prepare stimuli\n\n# Setting directory of our experiment\nos.chdir('C:\\\\Users\\\\CBCD\\\\Desktop\\\\BCCCD')\n\n# Winsize\nwinsize = (1920, 1080)\n\n# create a window\nwin = visual.Window(size = winsize,fullscr=True, units=\"pix\", pos =(0,30), screen=1)\n\n# Load images\nfixation = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\fixation.png', size = (200, 200))\ncircle   = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\circle.png', size = (200, 200))\nsquare   = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\square.png', size = (200, 200))\nwinning   = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\winning.png', size = (200, 200), pos=(560,0))\nloosing  = visual.ImageStim(win, image='EXP\\\\getting_started_psychopy\\\\loosing.png', size = (200, 200), pos=(-560,0))\n\n# Load sound\nwinning_sound = sound.Sound('EXP\\\\getting_started_psychopy\\\\winning.wav')\nlosing_sound = sound.Sound('EXP\\\\getting_started_psychopy\\\\loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, loosing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\n\nStart recording\nNow we are ready to look for the eye-trackers connected to the computer and select the first one that we find. Once we have selected it, we will launch our callback function to start collecting data.\n\n#%% Record the data\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\n#Start recording\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\n\n\nPresent our stimuli\nThe eye-tracking is running! Let’s show our participant something!\nAs you can see below, after each time we flip our window (remember: flipping means we actually show what we drew), we set the trigger variable to a string that identifies the specific stimulus we are presenting. This will be picked up our callback function.\n\n#%% Trials\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n        \n    fixation.draw()\n    win.flip()\n    trigger = 'Fixation'\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    if trial ==0:\n        trigger = 'Circle'\n    else:\n        trigger = 'Square'\n\n    core.wait(3)  # wait for 3 seconds\n    win.flip()\n\n    ### Wait for saccadic latencty\n    core.wait(0.75)\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n    if trial ==0:\n        trigger = 'Reward'\n    else:\n        trigger = 'NoReward'\n    core.wait(2)  # wait for 1 second\n    win.flip()    # we re-flip at the end to clean the window\n    \n    ### ISI\n    clock = core.Clock()\n    while clock.getTime() &lt; 1:\n        pass\n    \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        core.quit()  # stop study\n\nAs we said before in Save the data, it is best to save the data during our study to avoid any potential data loss. And it is better to do this when there are things of minor interest, such as an ISI. If you remember, in the previous tutorial: Getting started with Psychopy, we created the ISI in a different way than just a clock.wait() and we said that this different method would come in handy later on. This is the moment!\nOur ISI starts the clock and checks when 1 second has passed after starting this clock. Why is this important? Because we can save the data after starting the clock. Since the time that it will take will be variable, we will be simply check how much time has passed after saving the data and wait (using the while clock.getTime() &lt; 1:  pass code) until 1 second has fully passed. This will ensure that we will wait for 1 second in total considering the saving of the data.\n\n### ISI\nclock = core.Clock()\nwrite_buffer_to_file(gaze_data_buffer, 'DATA\\\\RAW\\\\'+ Sub +'.csv')\nwhile clock.getTime() &lt; 1:\n    pass\n\n\n\n\n\n\n\nWarning\n\n\n\nCarefull!!!\nIf saving the data takes more than 1 second, your ISI will also be longer. However, this should not be the case with typical studies where trials are not too long. Nonetheless, it's always a good idea to keep an eye out.\n\n\n\n\nStop recording\nWe’re almost there! We have imported our functions, started collecting data, sent the triggers, and saved the data. The last step will be stop data collection (or python will keep getting an endless amount of data from the eye tracker!). Do do that, We simply unsubscribe from the eye tracker to which we had subscribed to start of data collection:\n\nwin.close()\nEyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\nNote that we also closed the Psychopy window, so that the stimulus presentation is also officially over. Well done!!! Now go and get your data!!! We’ll see you back when it’s time to analyze it."
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html",
    "title": "From fixations to measures",
    "section": "",
    "text": "In the previous two tutorials we collected some eye-tracking data and then we used I2MC to extract the fixations from that data. Let’s load the data we recorded and pre-processed in the previous tutorial. We will import some libraries and read the raw data and the output from I2MC.\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n#%% Settings\n\n# # Settign the working directory\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n# Screen resolution\nscreensize = (1920, 1080) \n\n\n#%% Read and prepare data\n\n# The fixation data extracted from I2MC\nFixations = pd.read_csv('Data\\\\i2mc_output\\\\Test1\\Test1.csv')\n\n# The original RAW data\nRaw_data = pd.read_csv('Data\\\\RAW\\\\Test1.csv')\n\n# Start Recording from 0 and in Seconds\nRaw_data['time'] = Raw_data['time'] - Raw_data.loc[0,'time']\nWhat can we do with just the raw data and the fixations? Not much I am afraid. But we can use these fixations to extract some more meaningful indexes.\nIn this tutorial, we will look at how to extract two variables from our paradigm:\nSo what do these two measures have in common? pregnant pause for you to answer EXACTLY!!! They are both clearly related to the position of our stimuli. For this reason, it is important to define Areas Of Interest (AOIs) on the screen (for example, the locations of the targets). Defining AOIs will allow us to check, for each single fixation, whether it happened in an area that we are interested in."
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html#define-aois",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html#define-aois",
    "title": "From fixations to measures",
    "section": "Define AOIs",
    "text": "Define AOIs\nLet’s define AOIs. We will define two squares around the target locations. To do this, we can simply pass two coordinates for each AOI: the lower left corner and the upper right corner of an imaginary square.\nAn important point to understand is that tobii and Psychopy use two different coordinate systems:\n\nPsychopy has its origin (0,0) in the centre of the window/screen by default.\nTobii reports data with its origin (0,0) in the lower left corner.\n\nThis inconsistency is not a problem per se, but we need to take it into account when defining the AOIs. Let’s try to define the AOIs:\n\n# Screen resolution\nscreensize = (1920, 1080) \n\n# Define the variable realted to AOIs and target position\ndimension_of_AOI = 600/2  #the dimension of the AOIs, divided by 2\nTarget_position = 500 #the position of the targets relative to the centre (e.g., 500 pixels on the right from the centre)\n\n# Create areas of intescreensizet\nAOI1 =[[screensize[0]/2 - Target_position - dimension_of_AOI, screensize[1]/2-dimension_of_AOI], [screensize[0]/2 - Target_position + dimension_of_AOI, screensize[1]/2 + dimension_of_AOI]]\n\nAOI2 =[[screensize[0]/2 + Target_position - dimension_of_AOI, screensize[1]/2-dimension_of_AOI], [screensize[0]/2 + Target_position + dimension_of_AOI, screensize[1]/2 + dimension_of_AOI]]\n\nAOIs = [AOI1, AOI2]\n\nNice!! This step is essential. We have created two AOIs. We will use them to define whether each fixation of our participant was within either of these two AOIs. Let’s get a better idea by just plotting these two AOIs and two random points (600, 500) and (1400,1000).\n\n\nCode\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create a figure\nfig, ax = plt.subplots(1, figsize=(8,4.4))\n\n# Set the limits of the plot\nax.set_xlim(0, 1920)\nax.set_ylim(0, 1080)\n\n# Define the colors for the rectangles\ncolors = ['#46AEB9', '#C7D629']\n\n# Create a rectangle for each area of interest and add it to the plot\nfor i, (bottom_left, top_right) in enumerate(AOIs):\n    width = top_right[0] - bottom_left[0]\n    height = top_right[1] - bottom_left[1]\n    rectangle = patches.Rectangle(bottom_left, width, height, linewidth=2, edgecolor='k', facecolor=colors[i])\n    ax.add_patch(rectangle)\n\nax.plot(600,500,marker='o', markersize=8, color='green')    \nax.plot(1400,1000,marker='o', markersize=8, color='red')    \n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html#points-in-aois",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html#points-in-aois",
    "title": "From fixations to measures",
    "section": "Points in AOIs",
    "text": "Points in AOIs\nAs you can see, we are plotting the two AOIs and two points. One falls into one of them and the other doesn’t. But how can we get Python to tell us if a point falls within one of our AOIs?\nWe can check whether the (x, y) coordinates of the point are within the x and y coordinates of the left bottom and top right corners of the AOI. So imagine we have a point: point and an area: area, we can check if the point falls inside the area by:\n\n# Extract bottom left and top right points\nbottom_left, top_right = area\n\n# Extract the x and y of each point\nbottom_x, bottom_y = bottom_left\ntop_x, top_y = top_right\n\n# Extract the x and y of our point of interest\nx, y = point\n\n# Check if the point is in the area\nbottom_x &lt;= x &lt;= top_x and bottom_y &lt;= y &lt;= top_y\n\nPerfect, this will return True if the point falls inside the area and False if it falls outside. Since we have two AOIs and not just one, we want to make things a bit fancier. We will create a function that checks if a point falls within a list of areas, and tells us which area it falls in.\nWe will run the code above in a loop using enumerate. This extracts two elements to our loop: the index of the element and the element itself. In our case the index of the area and the area itself. This is very useful as we can then use both of these two pieces of information. We will use the actual area to check if our points fall into it. Then, if it does, we will return the index of that area. Conversely, if the point doesn’t fall in any area the function will return -1.\n\n# We define a function that simply takes the a point and a list of areas.\n# This function checks in which area this point is and return the index\n# of the area. If the point is in no area it returns -1\ndef find_area_for_point(point, areas):\n\n    for i, area in enumerate(areas):\n        # Extract bottom left and top right points\n        bottom_left, top_right = area\n        \n        # Extract the x and y of each point\n        bottom_x, bottom_y = bottom_left\n        top_x, top_y = top_right\n        \n        # Extract the x and y of our point of interest\n        x, y = point\n        \n        # Check if the point is in the area\n        if bottom_x &lt;= x &lt;= top_x and bottom_y &lt;= y &lt;= top_y :\n            return(i)\n    return(-1)\n\nNow we have a cool function to check whether a point falls into any of our AOIs. We can use this function to filter the fixations that are in the AOIs: These are the only ones we care about."
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html#first-fixation",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html#first-fixation",
    "title": "From fixations to measures",
    "section": "First fixation",
    "text": "First fixation\nWe have all these latency values, but we only want the first/fastest of each trial. How can we extract this information easily? We will use groupby. Groupby allows us to perform specific functions/commands on grouped sections of a data frame.\nHere we will groupby by Events and Event_trials and for each of these grouped pieces of dataframe we will extract the smallest (min()) value of latency.\n\n# We extract the first fixation of our dataframe for each event\nSaccadicLatency = Correct_Target_fixations.groupby(['Event', 'Event_trial'])['Latency'].min().reset_index()\n\nHere we have our Saccadic latency!!!"
  },
  {
    "objectID": "CONTENT/EyeTracking/Intro_eyetracking.html",
    "href": "CONTENT/EyeTracking/Intro_eyetracking.html",
    "title": "Introduction to eye-tracking",
    "section": "",
    "text": "Eye-tracking is a great tool to study cognition. It is especially suitable for developmental studies, as infants and young children might have advanced cognitive abilities, but little chances to show them (they cannot talk!).\nThis tutorial will teach you all you need to navigate the huge and often confusing eye-tracking world. First, we will introduce how an experiment can (and should) be built, explaining how to easily record eye-tracking data from Python. Then, it will focus on how to analyse the data, reducing the seemingly overwhelming amount of rows and columns in a few variables of interest (such as saccadic latency, looking time or pupil dilation)."
  },
  {
    "objectID": "CONTENT/EyeTracking/Intro_eyetracking.html#what-do-you-want-to-measure",
    "href": "CONTENT/EyeTracking/Intro_eyetracking.html#what-do-you-want-to-measure",
    "title": "Introduction to eye-tracking",
    "section": "What do you want to measure?",
    "text": "What do you want to measure?\nIt is much easier to start an eye-tracking project if you have a clear idea of what you want to measure. Classic paradigms on infant research rely on looking time (How long are infants attending a given stimulus?) and are often called Violation-of-Expectation tasks. They familiarize infants with a given stimulus or situation (e.g. a cat) and, after a given number of presentations (e.g., 8), they present a different stimulus (e.g., a dog). Do infants look longer at the dog, compared to the cat? If so, they were able to spot that something changed.\n\n\n\n\n\n\nImportant\n\n\n\nBeware! This does not mean that they can distinguish cats and dogs, but more simply that they could spot any difference between the two images. A careful control of the stimuli should be in place if we want to make strong conclusions from looking time.\n\n\nAnother very popular eye-tracking measure is saccadic latency. It measures how quickly infants can direct their gaze onto a stimulus, once it is presented on screen. This is a great measure of learning because infants will be faster at looking at a stimulus if they expect it to appear in a given position on the screen. They can even be so fast that they anticipate the correct location of the stimulus, even before the stimulus appears! This is called an anticipatory look.\n\n\n\n\n\n\nImportant\n\n\n\nBeware! Saccadic latencies are not a perfect measure of learning. Infants might be faster at looking at something just because they are more interested (pick interesting stimuli to keep them engaged!), and they might become slower due to boredom or fatigue (introduce variation in the stimuli, so that they become less boring over time!).\n\n\nThe fanciest eye-tracking measure is pupil dilation. It allows us to measure arousal (How interested is the infant in the stimulus?), cognitive effort (How difficult is the task?), and - my special favourite - uncertainty (Does the infant know what will happen next?). However, its fame comes at a price: It is not only the fanciest, but also the most persnickety…\n\n\n\n\n\n\nImportant\n\n\n\nBeware! Stimuli should be carefully designed, controlling their luminance (not too high, and ALWAYS the same) to avoid that task-unrelated variations in light will affect your measurements; They have to be presented in the center of the screen, as pupil dilation measures are inaccurate elsewhere; Pupil dilation is very slow, so the stimulus presentation also has to be slow; A fixation cross has to precede the moment in which pupil dilation is measured so that its signal returns to baseline before you observe the change you care about. But if you feel confident about your paradigm, go for it!!\n\n\nHere is a visual summary of what you can measure:"
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPython.html",
    "href": "CONTENT/GettingStarted/GettingStartedWithPython.html",
    "title": "Starting with Python",
    "section": "",
    "text": "Python is one of the most popular programming languages in general. In data science, it competes with Matlab and R for first place on the podium.\nIn our everyday we often use python to pre-process and analyze the data. In this tutorial we will explain our preferred way of installing python and managing its libraries. There are several ways to install python\nthis is the one we recommend for its simplicity and flexibility"
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPython.html#anaconda",
    "href": "CONTENT/GettingStarted/GettingStartedWithPython.html#anaconda",
    "title": "Starting with Python",
    "section": "Anaconda",
    "text": "Anaconda\nAnaconda is an open-source distribution for python. It is used for data science, machine learning, deep learning, etc. It makes simple to download and organize your python environment.\nTo install Anaconda simply follow this link. Choose a version suitable for you and click on download. Once you complete the download, open the setup.\n\n\n\n\n\nFollow the instructions in the setup.\nAfter the installation is complete, launch the Anaconda navigator. The Anaconda Navigator is a desktop GUI that comes with the anaconda distribution. Navigator allows you to launch common Python programs and easily manage conda packages, environments, and channels without using command-line commands. Navigator can search for packages on Anaconda Cloud or in a local Anaconda Repository."
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPython.html#miniconda",
    "href": "CONTENT/GettingStarted/GettingStartedWithPython.html#miniconda",
    "title": "Starting with Python",
    "section": "Miniconda",
    "text": "Miniconda\nWhile Anaconda is one of the most simple way to install python it is quite heavy. It install multiple programs and libraries that most of the time are unnecessary. For this reason we prefer to use Miniconda.\nMiniconda is a minimal installer for conda. It is a small, bootstrap version of Anaconda that includes only conda, Python, the packages they depend on, and a small number of other useful packages. This means that fewer packages are installed and that we have more control on what to have on our PC. To use Miniconda download the installer from Miniconda\nThe installation is exactly the same as the one for Anaconda. After it has finished the installation you will find the Anaconda Prompt between your programs. We will use the Anaconda Prompt to install the packages and the software that we need."
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPython.html#creating-an-environment-from-an-environment.yml-file",
    "href": "CONTENT/GettingStarted/GettingStartedWithPython.html#creating-an-environment-from-an-environment.yml-file",
    "title": "Starting with Python",
    "section": "Creating an environment from an environment.yml file",
    "text": "Creating an environment from an environment.yml file\nAnother interesting feature of conda is that we can save an environment file from an environment we have created. We can later use this file to recreate the same environment with all its packages. This is very convenient when we want to have several people and several PCs with the same environment. We will provide some environment files in our tutorials. These environments have been tested for their specific use and will provide a standard environment for all the users.\nTo create an environment from a environment file just type:\nconda env create -f environment.yml\nThis again will take some time but it will create the new environment with all the packages specified in it.\nconda env create -f environment.yml"
  }
]