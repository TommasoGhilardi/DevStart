[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "DevStart is an online hands-on manual for anyone who is approaching developmental psychology and developmental cognitive neuroscience for the first time, from master’s students and PhDs to postdocs. \nAssuming no a priori knowledge, this website will guide you through your first steps as a developmental researcher. You’ll find many examples and guidelines on how to grasp the basic principles of developmental psychology research, design and set up a study with different research methods, analyse data, and start programming.\n\n\nThere are many resources on the web to learn how to program, build experiments, and analyse data. However, they often assume basic skills in programming, and they don’t tackle the challenges that we encounter when testing babies and young children. This knowledge is often handed down from Professors and Postdocs to PhD students, or it is acquired through hours and hours of despair. \nWe tried to summarize all the struggles that we encountered during our PhDs and the solutions we came up with. With this website, we aim to offer you a solution to overcome these problems, and we invite anyone to help us and contribute to this open science framework! \n\n\n\nIf you’re an expert on a specific topic and you want to change something or add something, if you don’t agree with our pipeline, or you spot an error in our code, please get in touch! \nYou can find us at:\n\n t.ghilardi@bbk.ac.uk\n\n francesco.poli@donders.ru.nl\n\n g.serino@bbk.ac.uk\n\n\n\n\n\nThis is a story of bullying and love started in 2018 at the Donders Institute in the Netherlands. Tommaso and Francesco were living their best PhD life when Giulia bumped into them, looking for data to write her master’s thesis.\nUnited by a strong passion for research and a creepy attraction to complex and noisy datasets, they immediately got along well. We haven’t stopped collaborating since then.\n\n\n\n\n\n\n\n\n\n\nTommaso Ghilardi is the programmer behind the scenes. He excels in Python and is known for his impeccably clean code. If you explore our website, you’re likely to come across his contributions. Tommaso’s skills aren’t limited to programming—he’s capable of setting up a lab in just an hour. He’s equally comfortable with research methods and a variety of programming languages. However, be cautious about imperfect code; Tommaso has a short fuse for anything less than perfect. He plays a significant role in shaping this website, making it what it is today. Thank you, Tommaso!\n\n\n\nPostdoc at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://tommasoghilardi.github.io\n t.ghilardi@bbk.ac.uk\n @TommasoGhi\n\n\n\n\n\n\n\n\n\n\n\n\nFrancesco Poli is the deep thinker among us. He’s not just any ordinary thinker; he can build computational models to predict behavior and decipher your thoughts in the blink of an eye. Francesco’s abilities extend to creating intricate mathematical theories to explain behavior patterns. He’s adept at juggling multiple projects, though he might occasionally forget to reply to emails amidst his busy schedule. With Francesco, expect the extraordinary.\n\n\n\nPostDoc at the Donders Centre for Cognition.\n https://www.ru.nl/en/people/poli-f\n francesco.poli@donders.ru.nl\n @FrancescPoli\n\n\n\n\n\n\n\n\n\n\n\n\nGiulia Serino brings imagination to the table. Introduced to programming by Francesco and Tommaso, she has surpassed them to become a skilled programmer herself. Her experiments are a blend of art and science, often resembling graphic masterpieces. However, Giulia tends to get lost in the minutiae of her work. She has a reputation for disappearing, yet her curiosity leads her to explore every corner of Google and various repositories, piecing together her unique theories on attention development.\n\n\n\nPhD student in Developmental Cognitive Neuroscience at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://cbcd.bbk.ac.uk/people/students/giulia-serino\n g.serino@bbk.ac.uk\n @GiSerino\n\n\n\n\nWe tried our best to offer the best and most accurate solutions. However, do always check the code and if the outputs make sense. Please get in touch if you spot any errors.\nWe apologize in advance for our poor coding skills. Our scripts are not perfect, and they don’t mean to be. But, as Francesco always says, they work! And we hope they will support you during your PhD.",
    "crumbs": [
      "Welcome to DevStart"
    ]
  },
  {
    "objectID": "index.html#why-did-we-create-it",
    "href": "index.html#why-did-we-create-it",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "There are many resources on the web to learn how to program, build experiments, and analyse data. However, they often assume basic skills in programming, and they don’t tackle the challenges that we encounter when testing babies and young children. This knowledge is often handed down from Professors and Postdocs to PhD students, or it is acquired through hours and hours of despair. \nWe tried to summarize all the struggles that we encountered during our PhDs and the solutions we came up with. With this website, we aim to offer you a solution to overcome these problems, and we invite anyone to help us and contribute to this open science framework!",
    "crumbs": [
      "Welcome to DevStart"
    ]
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "If you’re an expert on a specific topic and you want to change something or add something, if you don’t agree with our pipeline, or you spot an error in our code, please get in touch! \nYou can find us at:\n\n t.ghilardi@bbk.ac.uk\n\n francesco.poli@donders.ru.nl\n\n g.serino@bbk.ac.uk",
    "crumbs": [
      "Welcome to DevStart"
    ]
  },
  {
    "objectID": "index.html#who-are-we",
    "href": "index.html#who-are-we",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "This is a story of bullying and love started in 2018 at the Donders Institute in the Netherlands. Tommaso and Francesco were living their best PhD life when Giulia bumped into them, looking for data to write her master’s thesis.\nUnited by a strong passion for research and a creepy attraction to complex and noisy datasets, they immediately got along well. We haven’t stopped collaborating since then.\n\n\n\n\n\n\n\n\n\n\nTommaso Ghilardi is the programmer behind the scenes. He excels in Python and is known for his impeccably clean code. If you explore our website, you’re likely to come across his contributions. Tommaso’s skills aren’t limited to programming—he’s capable of setting up a lab in just an hour. He’s equally comfortable with research methods and a variety of programming languages. However, be cautious about imperfect code; Tommaso has a short fuse for anything less than perfect. He plays a significant role in shaping this website, making it what it is today. Thank you, Tommaso!\n\n\n\nPostdoc at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://tommasoghilardi.github.io\n t.ghilardi@bbk.ac.uk\n @TommasoGhi\n\n\n\n\n\n\n\n\n\n\n\n\nFrancesco Poli is the deep thinker among us. He’s not just any ordinary thinker; he can build computational models to predict behavior and decipher your thoughts in the blink of an eye. Francesco’s abilities extend to creating intricate mathematical theories to explain behavior patterns. He’s adept at juggling multiple projects, though he might occasionally forget to reply to emails amidst his busy schedule. With Francesco, expect the extraordinary.\n\n\n\nPostDoc at the Donders Centre for Cognition.\n https://www.ru.nl/en/people/poli-f\n francesco.poli@donders.ru.nl\n @FrancescPoli\n\n\n\n\n\n\n\n\n\n\n\n\nGiulia Serino brings imagination to the table. Introduced to programming by Francesco and Tommaso, she has surpassed them to become a skilled programmer herself. Her experiments are a blend of art and science, often resembling graphic masterpieces. However, Giulia tends to get lost in the minutiae of her work. She has a reputation for disappearing, yet her curiosity leads her to explore every corner of Google and various repositories, piecing together her unique theories on attention development.\n\n\n\nPhD student in Developmental Cognitive Neuroscience at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://cbcd.bbk.ac.uk/people/students/giulia-serino\n g.serino@bbk.ac.uk\n @GiSerino",
    "crumbs": [
      "Welcome to DevStart"
    ]
  },
  {
    "objectID": "index.html#warnings",
    "href": "index.html#warnings",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "We tried our best to offer the best and most accurate solutions. However, do always check the code and if the outputs make sense. Please get in touch if you spot any errors.\nWe apologize in advance for our poor coding skills. Our scripts are not perfect, and they don’t mean to be. But, as Francesco always says, they work! And we hope they will support you during your PhD.",
    "crumbs": [
      "Welcome to DevStart"
    ]
  },
  {
    "objectID": "CONTENT/Workshops/BCCCD2024.html",
    "href": "CONTENT/Workshops/BCCCD2024.html",
    "title": "BCCCD2024",
    "section": "",
    "text": "Hello hello!!! This page has been created to provide support and resources for the tutorial and we presented at BCCCD24."
  },
  {
    "objectID": "CONTENT/Workshops/BCCCD2024.html#software",
    "href": "CONTENT/Workshops/BCCCD2024.html#software",
    "title": "BCCCD2024",
    "section": "Software",
    "text": "Software\nIn this tutorial, our primary tool will be Python!! We recommend installing it via Miniconda. Our interaction with Python will primarily be through the Spyder IDE. You’re free to use any IDE of your choice, but if you’d like to follow along more smoothly, we suggest checking out our guide on how to install both Miniconda and Spyder: Getting started with Python.\nBesides Python, we’ll also be utilizing Psychopy! Psychopy is an awesome set of packages and functions designed for conducting psychological experiments. It’s available as a standalone software or can be installed as a Python package.\nBased on our experience, we find it more advantageous to use Psychopy as a Python package due to its increased flexibility. We provide this anaconda environment to easily create a virtual environment with both python, psychopy and all the libraries that we will need. You can find details on how to install a conda environment on this page: Getting started with Psychopy"
  },
  {
    "objectID": "CONTENT/Workshops/BCCCD2024.html#files",
    "href": "CONTENT/Workshops/BCCCD2024.html#files",
    "title": "BCCCD2024",
    "section": "Files",
    "text": "Files\nIn our workshop we will create a cool eye-tracking study, collect the data and analyze them. To make everything smoother we provide HERE the stimuli and the data that we will use for the workshop."
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html",
    "href": "CONTENT/Stats/LinearModels.html",
    "title": "Linear Models",
    "section": "",
    "text": "Welcome to the first tutorial on data analysis!!! Today we are going to talk about one of the most flexible statistical methods: Linear models.\nLet’s be clear, WE ARE NOT STATISTICIANS!!!!\nWe’ll be discussing linear models in a very accessible and practical manner. Our explanations might not align with the rigorous definitions statisticians are accustomed to, and for that, we apologize in advance! However, our aim is to provide a stepping stone for you to grasp the concept of linear models and similar analyses. Let’s get started!",
    "crumbs": [
      "Stats",
      "Linear Models"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html#import-data",
    "href": "CONTENT/Stats/LinearModels.html#import-data",
    "title": "Linear Models",
    "section": "Import data",
    "text": "Import data\nYou can download the data that we will use in this tutorial from here:\n Dataset.csv \nOnce downloaded we need to import it in our R session. Here we read our csv and we print a small preview of it.\n\ndf = read.csv(\"../../resources/Stats/Dataset.csv\")\nhead(df)\n\n  Id    Event TrialN ReactionTime LookingTime\n1  1 NoReward      1     344.1243   1139.8102\n2  1 NoReward      2     361.9389   1156.4140\n3  1 NoReward      3     406.3499   1073.5468\n4  1 NoReward      4     395.2774   1106.5766\n5  1 NoReward      5     382.1649    990.5572\n6  1 NoReward      6     367.3000   1053.6605\n\n\nYou can see that the data is really simple! We have 4 columns:\n\nId column that tell us form which participant the data was collected\nEvent represent which condition we were in\nTrialN the sequential number of trials in the experiment\nReactionTime how quickly participants reacted to the stimuli. We will ignore this variable for now\nLookingTimethe variable of interest for today, the one that we want to model\n\n\n\n\n\n\n\nLong format\n\n\n\nOne important information that we need to keep in mind is that to run lm() (and most models!!) in R we need the data in a long format and not a wide format.\nIn long format, each row represents a single observation. Variables are organized in columns, with one column for the variable names and another for the values. This means that the column you want to model (in the example LookingTime) has 1 row for observation but the other columns usually have repeated entries ( e.g. Id , TrialN, Event)\nWide format, on the other hand, has each row representing a subject or group, with multiple columns for different variables or time-points. While this can be visually appealing for humans, it’s not optimal for our linear modeling needs.\nHere a small comparison of the two formats:\n\n\nLong Format\n\n\n# A tibble: 8 × 4\n  Id    Event    TrialN LookingTime\n  &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;       &lt;dbl&gt;\n1 1     Reward        1        1450\n2 1     Reward        2        1420\n3 1     Reward        3        1390\n4 1     NoReward      1        1380\n5 1     NoReward      2        1350\n6 1     NoReward      3        1320\n7 2     Reward        1        1480\n8 2     Reward        2        1460\n\n\n\n\nWide Format\n\n\n# A tibble: 3 × 7\n  Id    Reward_Trial1 Reward_Trial2 Reward_Trial3 NoReward_Trial1\n  &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1 1              1450          1420          1390            1380\n2 2              1480          1460          1410            1390\n3 3              1520          1490          1450            1410\n# ℹ 2 more variables: NoReward_Trial2 &lt;dbl&gt;, NoReward_Trial3 &lt;dbl&gt;\n\n\n\n\n\nIf your data is currently in wide format, don’t worry! R provides tools like the tidyr package with functions such as pivot_longer() to easily convert your data from wide to long format.",
    "crumbs": [
      "Stats",
      "Linear Models"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html#formula",
    "href": "CONTENT/Stats/LinearModels.html#formula",
    "title": "Linear Models",
    "section": "Formula",
    "text": "Formula\nTo run models in R we usually use formulas! Sounds complex doesn’t it?!? Well it is not, let me guide you through it.\nIn R, model formulas follow a specific structure. On the left side of the formula, we place our dependent variable - the outcome we collected and we’re interested in studying. In this case, it’s the LookingTime column. Next, we use the tilde symbol ~. This tilde tells R that we want to predict the variable on the left using the variables on the right side of the formula. On the right side, we list the independent variables (predictors) we believe may influence our dependent variable. To test whether TrialN predicts LookingTime, we can use the formula:\n\n\nLookingTime ~ TrialN. This basic structure allows us to examine a single predictor.\n\nWe can extend this model by adding another variable, such as Event, to see if it also predicts LookingTime:\n\n\nLookingTime ~ TrialN + Event. This formulation tells the model to assess whether either TrialN and Event predicts LookingTime, treating them as independent predictors.\n\nTo examine the interaction between these variables, we use a slightly different syntax:\n\n\nLookingTime ~ TrialN : Event. This instructs the model to evaluate whether the interaction between the two variables predicts LookingTime.\n\nIt’s important to note that using : only tests the interaction, not the individual effects of each variable. To include both main effects and their interaction, we can use the formula:\n\n\nLookingTime ~ TrialN + Event + TrialN:Event.\n\nR offers a shorthand for this complete model using the * operator. The formula:\n\n\nLookingTime ~ TrialN * Event is equivalent to the longer version above, testing both main effects and the interaction in a more concise format.\n\nThese formulas are for simple linear models. Different types of models add small and different pieces to this basic structure. We will see in the next tutorial how to handle these “add-ons”. Now that we have seen how to make a proper formula let’s use it in our model!!",
    "crumbs": [
      "Stats",
      "Linear Models"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html#run-the-model",
    "href": "CONTENT/Stats/LinearModels.html#run-the-model",
    "title": "Linear Models",
    "section": "Run the model",
    "text": "Run the model\nOK, now we have our amazing data! Let’s run this Linear model.\nIt’s extremely simple. We will use the function lm() and we will pass our data df and the formula we just made together!!\nAfter fitting the model we extract the summary of it. This is how we will get all the information we need.\n\nmod_lm = lm(LookingTime ~ TrialN*Event, data = df)\nsummary(mod_lm)\n\n\nCall:\nlm(formula = LookingTime ~ TrialN * Event, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-772.0 -159.7   -6.1  190.5  607.1 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1390.1655    29.1769  47.646   &lt;2e-16 ***\nTrialN               -7.5998     2.9897  -2.542   0.0113 *  \nEventReward          99.8279    39.3360   2.538   0.0114 *  \nTrialN:EventReward    0.1332     3.7240   0.036   0.9715    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 253.2 on 656 degrees of freedom\n  (140 observations deleted due to missingness)\nMultiple R-squared:  0.05147,   Adjusted R-squared:  0.04713 \nF-statistic: 11.87 on 3 and 656 DF,  p-value: 1.417e-07\n\n\nWoohoo! This was super simple!! 🎉 We can use the output of the model to understand whether the variables are predicting LookingTime. The magic number we’re looking for is the p-value, hiding in the last column of the Coefficients section. If the p-value is below 0.05, we’ve got ourselves an effect! If it’s above, sadly we don’t. AND YES, EVEN IF IT’S 0.051!!! Rules are rules in the p-value game!\nWhat goodies can we spot here? First, our Intercept is significant (don’t worry, we’ll decode this mysterious value later!). Even more exciting, we’ve got a significant effect of TrialN – our continuous variable – (p = 0.0113) and a significant effect of Event (p = 0.0114). Double win! However, the interaction between TrialN and Event is playing hard to get with that p = 0.9715. That’s nowhere near significant!\nThis is already pretty cool, right?!? But here’s the deal - when looking at model outputs, people often get hypnotized by p-values. However, there’s MUCH more to unpack in a model summary! The full story of our data is waiting to be discovered in the complete model output. Let’s dive in together and crack this statistical puzzle!",
    "crumbs": [
      "Stats",
      "Linear Models"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html#estimates",
    "href": "CONTENT/Stats/LinearModels.html#estimates",
    "title": "Linear Models",
    "section": "Estimates",
    "text": "Estimates\nThe Estimate section is probably one of the most important parts of our model summary. While the other columns (t-values and p-values) are just numbers that tell us whether the predictor fits, the estimates tell us HOW they fit!!\nLet’s go together through the most challenging information:\n(Intercept)\nThe intercept often confuses people who approach linear models for the first time. What exactly is it? 🤔\nThe (Intercept) represents the reference levels where all our predictors (TrialN and Event) are 0. While TrialN is easy to understand when it’s 0 (TrialN 0 in our case would be the first trial), you may be scratching your head thinking…how can Event be 0? It’s a categorical variable, it can’t be 0!!!! TrialN == 0…sure….but Event??\nYou are absolutely right! When a model encounters a categorical variable, it cleverly selects the first level of such variable as the reference level. If you take another look at our model summary, you can see that there’s information for the Reward level of the Event variable but nothing about the NoReward level. This is because the NoReward level has been selected by the model as the reference level and is thus represented in the intercept value! 💡\nSo our intercept (1390.1) actually represents the predicted LookingTime when:\n\nWe’re on the first trial (TrialN = 0)\nWe’re in the NoReward condition\n\nThe Standard Error of the estimate (29.1) tells us the precision of the estimate.\n\n\n\n\n\n\nTip\n\n\n\nSince the intercept has a significant p-value, it means that the estimate for the NoReward condition at trial 0 is actually significantly different from 0. In other words, our participants are definitely looking at something during the NoReward condition in the first trial – their LookingTime isn’t zero! This might seem obvious (of course they’re looking!), but statistically confirming this baseline is actually meaningful.\n\n\nOk, all this explanation is great…but it’s much easier to visualize these Estimates!\n\n\n\n\n\n\n\n\nAs you can see, the intercept is pretty straightforward—it gives us the estimate when everything is set to 0, both for continuous and categorical variables. The intercept is the foundation of your model, where all the predictors are at their baseline value (in this case, NoReward was cleverly selected as the reference or 0 level for the categorical variable).\nEvent\nAwesome! Now that we’ve got the intercept down, let’s take a look at the rest of the model output. We’ll skip over the TrialN variable for now and focus on what’s happening with the Event.\nAt first, the results for Event [Reward] might look like they’re giving us the value of looking time for the Reward. Super easy, right?!?\nWell… not exactly! 🤔\nIn linear models, each coefficient shows the difference in relation to the intercept (the 0 or the reference level), not the exact value of the Reward condition.\nIt sounds a bit confusing, but let’s break it down. If we want to understand what is the estimate for a Rewarding event we need to take the Intercept (1390.1) –as we mentioned that is actually the event NoReward – and then just simply add to it the Estimate for the Event [Reward] (99.8). So the model is telling us that a Rewarding event should be 1390.1 + 99.8 = 1489.9.\nSee? Not too bad! Let’s visualize it and make it even clearer!\n\n\n\n\n\n\n\n\nTrialN\nSo, interpreting the coefficients for categorical variables wasn’t too tricky, right? But what about continuous variables like TrialN?\nNo worries, it’s actually pretty straightforward! The coefficient for a continuous variable represents the slope, or the incline, of the line for that variable.\nIn simpler terms, it shows how much the outcome (in this case, LookingTime) changes for each unit increase in the continuous variable (TrialN). So, in our case the coefficient for TrialN is -7.5, this means that for each unit increase in TrialN, the LookingTime is expected to decrease by 7.5 units (assuming all other variables stay the same).\n\n\n\n\n\n\nImportant\n\n\n\nRemember!! This coefficient represents the effect of TrialN specifically when Event is at its reference level (NoReward). In other words, this -7.5 decrease in LookingTime per trial applies specifically to the NoReward condition!\n\n\nEven easier..let’s plot again!\n\n\n\n\n\n\n\n\nInteraction\nAlright, now we’re getting to the final step! Let’s talk about the interaction between TrialN and Event! Now, we’re not just dealing with a single factor or continuous variable, but looking at how they interact with each other. Don’t worry—if you understood the previous steps, this will be a breeze!\nWe’ll take it step by step and look at the interaction in our model parameters. The interaction term between TrialN and Event [Reward] tells us how the relationship between TrialN and LookingTime changes when we switch from the reference Event (NoReward) to the Reward condition.\nTo put it simply:\n\nFor NoReward events, LookingTime decreases by 7.5 units per trial (that’s our main TrialN coefficient)\nThe interaction coefficient (0.13) tells us how this slope changes for Reward events\n\nSo for Reward events, the slope would be: -7.5 + 0.13 = -7.37 units per trial.\nWhat does this mean in real terms? If the interaction coefficient is positive (like in our example), it means participants’ looking time decreases more slowly during Reward trials compared to NoReward trials – the estimate is slightly less negative.\n\n\n\n\n\n\nImportant\n\n\n\nIn this model the interaction effect is extremely small and not significant. This means that there is actually no difference in how the TrialN predicted Looking time in either NoReward or Reward! Looking time decreases at basically the same rate regardless of which event type we’re looking at!!!\n\n\nWhile there is no significant difference, let’s plot the estimated effects for TrialN for both Reward and NoReward conditions to visualize this relationship!\n\n\n[1] time                        est_performance_EventTrial \n[3] est_performance_Interaction\n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDo you notice any difference in how the slopes between the two conditions? Probably not! If you examine the model summary, you’ll see the interaction effect is very small and not statistically significant.\nThis tells us that TrialN predicts Looking time similarly in both NoReward and Reward conditions. In other words, looking time decreases at essentially the same rate regardless of which event type participants observed.\n\n\nI hope that by now you got a decent understanding on how to interpret what a linear model is telling you!! Different kinds of models will add small pieces of information here and there, but the main information will still be there. Thus, if you got here, you are a step closer to becoming a stats genius!!! 🧠📊\nBut wait, we’re not done yet! Now it’s time to check whether our model actually meets the assumptions of linear regression. Remember those plots we generated earlier? Let’s make sure our statistical foundation is solid before we draw any final conclusions!",
    "crumbs": [
      "Stats",
      "Linear Models"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html#run-the-model-1",
    "href": "CONTENT/Stats/LinearModels.html#run-the-model-1",
    "title": "Linear Models",
    "section": "Run the model",
    "text": "Run the model\nOk, this sounds fair right? Before running a model we can just standardize the values of our predictors (in our case TrialN). Yeah..but how to? Our favorite way is to use the standardize() function from the the easystats library.\n\n\n\n\n\n\nNote\n\n\n\nEasystats\nEasystats is a collection of R packages that includes tools dedicated to the post-processing of statistical models. It is made of all these packages: report, correlation, modelbased, bayestestR, effectsize, see, parameters, performance, insight, datawizard. We will extensively use all these package in our tutorials. The cool thing is that you can import all of them by just simply importing the collection Easystats with library(easystats).\nIn this tutorial here we will use the function from the packages datawizardand performance(in the Model checks sections).\n\ndatawizard allows to manipulate, clean, transform, and prepare your data\nperformance is a package to check model performance metrics.\n\n\n\nSo now we import easystats and we use the function standardize()\n\nlibrary(easystats)\ndf$StandardTrialN = standardize(df$TrialN)\nhead(df)\n\n  Id    Event TrialN ReactionTime LookingTime StandardTrialN\n1  1 NoReward      1     344.1243   1139.8102     -1.6464789\n2  1 NoReward      2     361.9389   1156.4140     -1.4731654\n3  1 NoReward      3     406.3499   1073.5468     -1.2998518\n4  1 NoReward      4     395.2774   1106.5766     -1.1265382\n5  1 NoReward      5     382.1649    990.5572     -0.9532246\n6  1 NoReward      6     367.3000   1053.6605     -0.7799111\n\n\nWe have created like this a new column StandardTrialN with the standardized values of the TrialN and now we can run the model again\n\nmod_lm = lm(LookingTime ~ StandardTrialN*Event, data = df)\nsummary(mod_lm)\n\n\nCall:\nlm(formula = LookingTime ~ StandardTrialN * Event, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-772.0 -159.7   -6.1  190.5  607.1 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                1310.3674    16.9387  77.359  &lt; 2e-16 ***\nStandardTrialN              -43.8501    17.2505  -2.542   0.0113 *  \nEventReward                 101.2269    21.1961   4.776 2.21e-06 ***\nStandardTrialN:EventReward    0.7688    21.4872   0.036   0.9715    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 253.2 on 656 degrees of freedom\n  (140 observations deleted due to missingness)\nMultiple R-squared:  0.05147,   Adjusted R-squared:  0.04713 \nF-statistic: 11.87 on 3 and 656 DF,  p-value: 1.417e-07\n\n\nAs you can see the estimates are different!!! Because as we mentioned the reference level and the scale are different now. However do not panic, even if some values are different the relations are the same, we still have negative estimate for the time variable (here StandardTrialN and before TrialN) and positive for the Reward condition. Also the p-values are the same!\nSo there you have it - standardizing your predictors makes your model both more interpretable and more robust, all while keeping the fundamental relationships in your data intact. It’s one of those simple steps that can make a big difference in your statistical journey! 🌟",
    "crumbs": [
      "Stats",
      "Linear Models"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html#statistical-tests",
    "href": "CONTENT/Stats/LinearModels.html#statistical-tests",
    "title": "Linear Models",
    "section": "Statistical tests",
    "text": "Statistical tests\nYou’ve probably noticed that we’ve been relying on visual checks so far. In our view, this is often the best approach, as statistical tests for model assumptions can sometime be overly stringent. However, there may be situations where you need to provide statistical evidence to support your model assumptions. This often happens when a reviewer (let’s call them Reviewer 2, shall we?) insists on seeing numerical proof. Fortunately, easystats has got your back.\nHere are some examples of what you can use:\n\ncheck_normality(mod_lm)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nTo check the normality of our residuals and:\n\ncheck_homogeneity(mod_lm)\n\nOK: There is not clear evidence for different variances across groups (Bartlett Test, p = 0.986).\n\n\nto check homoscedasticity/homogeneity of variance. Again you can find all the function in the performance package (part of the Easystats collection)",
    "crumbs": [
      "Stats",
      "Linear Models"
    ]
  },
  {
    "objectID": "CONTENT/Stats/GeneralisedModels.html",
    "href": "CONTENT/Stats/GeneralisedModels.html",
    "title": "Generalised mixed-effect models",
    "section": "",
    "text": "If you’ve been following our statistical journey, you’re already familiar with our adventures in linear models and the exciting world of linear mixed-effect models!!! Now it’s time to level UP our statistical toolkit with something even MORE POWERFUL: Generalised Linear Mixed-Effects Models (GLMMs)!!! 🎉\nIn this tutorial, we’ll tackle those variables that just WON’T behave normally! Our focus will be on Reaction Time!!! Remember that eye-tracking dataset we’ve been playing with? It doesn’t just have Looking Time—but also saccadic reaction times we’ve been ignoring! These reaction times indicate how quickly participants directed their gaze to the stimuli when they appeared on screen.",
    "crumbs": [
      "Stats",
      "Generalised mixed-effect models"
    ]
  },
  {
    "objectID": "CONTENT/Stats/GeneralisedModels.html#linear-model",
    "href": "CONTENT/Stats/GeneralisedModels.html#linear-model",
    "title": "Generalised mixed-effect models",
    "section": "Linear model",
    "text": "Linear model\nFollowing what we have learned in the previous tutorial we will fit a linear mixed effect model including a random intercept and random slope:\n\nmod_lm &lt;- lmer(\n  ReactionTime ~ StandardTrialN * Event + (1 + StandardTrialN | Id),\n  data = df\n)\n\nNow, before even looking at the summary, let’s check whether the model follows the model assumptions. For that, we can use the function check_model() as we explained in the tutorial on linear models:\n\ncheck_model(mod_lm)\n\n\n\n\n\n\n\nThis does not look too great. The linearity is not really flat…it seems U shaped. The homogeneity of variance as well! In addition we have influential observations (red outliers) and also the normality of residuals is not too great!!\nHowever, the plot I want you to focus on actually is the first one, the posterior predictive check. Do you note anything weird?? As you can see, the model believes there are values below 200ms…but that as we saw earlier is IMPOSSIBLE!! No participant can respond that quickly in our dataset!\nAll these indications and incoherences tell us that the model is fitting poorly!!! Our linear model is struggling to capture the true nature of reaction time data. The model is trying to force our non-normal data into a normal distribution, and it’s clearly not working!",
    "crumbs": [
      "Stats",
      "Generalised mixed-effect models"
    ]
  },
  {
    "objectID": "CONTENT/Stats/GeneralisedModels.html#finding-a-better-family",
    "href": "CONTENT/Stats/GeneralisedModels.html#finding-a-better-family",
    "title": "Generalised mixed-effect models",
    "section": "Finding a better family",
    "text": "Finding a better family\nSo what’s the solution to this mess? If normal distributions don’t work for our data, we need to find a distribution family that better matches reaction times! This is exactly where Generalized Linear Mixed Models (GLMMs) come to the rescue - they allow us to specify different distribution families beyond just Gaussian.\nBut which distribution should we use? The more you get familiarised with distributions, the more you will learn to recognise them. But since we’re not black belt of statistics yet, let’s use the Cullen and Frey graph instead! For this, we will use the fitdistrplus package:\n\nlibrary(fitdistrplus)\n\n# first we need to remove NAs:\nRTs = as.vector(na.omit(df$ReactionTime))\n# then we can plot\ndescdist(RTs, discrete = FALSE, boot = 500)\n\n\n\n\n\n\n\nsummary statistics\n------\nmin:  197.6241   max:  847.4415 \nmedian:  343.4964 \nmean:  365.1186 \nestimated sd:  109.9071 \nestimated skewness:  0.9530788 \nestimated kurtosis:  3.88081 \n\n\nThe Cullen and Frey graph reveals which distribution best matches our data by plotting skewness against kurtosis. Looking at our reaction time data (red dot), we can immediately see it falls far from the normal distribution (asterisk) and instead sits near both the gamma (dashed line) and lognormal (dotted line) distributions, within the gray beta distribution area. The yellow circles represent bootstrap samples that help us assess uncertainty in our estimate. Since these bootstrap points tend to closely follow the gamma distribution line, we have strong evidence to select gamma as our distribution family! This explains why our linear model (which assumes normality) performed so poorly and gives us a clear direction for building a more appropriate model.\n\n\n\n\n\n\nCaution\n\n\n\nBeware! this plot only shows 7 or 8 distributions, and many more exist! For example, do you know what a bernoulli or a binary distribution are? If not we advise you check out https://distribution-explorer.github.io/index.html",
    "crumbs": [
      "Stats",
      "Generalised mixed-effect models"
    ]
  },
  {
    "objectID": "CONTENT/Stats/GeneralisedModels.html#summary",
    "href": "CONTENT/Stats/GeneralisedModels.html#summary",
    "title": "Generalised mixed-effect models",
    "section": "Summary",
    "text": "Summary\nOnce we are confident that the assumptions are respected, we can interpret the results:\n\nsummary(mod_gamma)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: Gamma  ( log )\nFormula: ReactionTime ~ StandardTrialN * Event + (1 + StandardTrialN |      Id)\n   Data: df\n\n     AIC      BIC   logLik deviance df.resid \n  6181.1   6217.0  -3082.5   6165.1      652 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.10383 -0.55231  0.00207  0.56421  2.67485 \n\nRandom effects:\n Groups   Name           Variance Std.Dev. Corr\n Id       (Intercept)    0.006482 0.08051      \n          StandardTrialN 0.003207 0.05663  0.26\n Residual                0.007262 0.08522      \nNumber of obs: 660, groups:  Id, 20\n\nFixed effects:\n                            Estimate Std. Error t value Pr(&gt;|z|)    \n(Intercept)                 5.442146   0.072155  75.423  &lt; 2e-16 ***\nStandardTrialN             -0.283501   0.038504  -7.363  1.8e-13 ***\nEventReward                 0.446262   0.006024  74.081  &lt; 2e-16 ***\nStandardTrialN:EventReward  0.200902   0.006031  33.311  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) StndTN EvntRw\nStandrdTrlN  0.315              \nEventReward -0.062 -0.059       \nStndrdTN:ER -0.032 -0.112  0.345\n\n\nOMG, these results are AMAZING!!! 🎉 Look at that interaction between trial number and Event - it’s significant! This is super important to note because when we have significant interactions, we need to focus on understanding THOSE rather than looking at the main effects in isolation.\nThe interaction tells us that the effect of trial number on reaction time DEPENDS on which event condition we’re in! To really grasp what this pattern means in practical terms, let’s visualize this interaction:\n\nCodeis_pred &lt;- estimate_expectation(mod_gamma, include_random=F)\n\n# 2. Plot the interaction between StandardTrialN and Event\nggplot(is_pred, aes(x = StandardTrialN, \n                    y = Predicted, \n                    color = Event, \n                    group = Event)) +\n  geom_line(size = 1.2) + \n  geom_ribbon(aes(ymin = Predicted - SE, ymax = Predicted + SE, fill = Event),\n              alpha = 0.2, color = \"transparent\") +\n  labs(y = \"Reaction Time (Predicted)\", x = \"Scaled Trial #\") +\n  scale_color_manual(values = c('NoReward' = '#4A6274', 'Reward' = '#E2725A')) +\n  scale_fill_manual(values = c('NoReward' = '#4A6274', 'Reward' = '#E2725A'))+\n  theme_classic(base_size = 16)\n\n\n\n\n\n\n\nReaction times decrease much more across trials for the reward than for the no reward condition! However, we don’t have a complete picture yet. For example, we do not know whether the decrease in reaction times across trials is significant only for the reward condition or also for the no reward condition. To find out, we can estimate the slopes as we did in the previous tutorial: ModelEstimates.qmd\n\nestimate_slopes(mod_gamma, trend = 'Event', by = 'StandardTrialN')\n\nEstimated Marginal Effects\n\nStandardTrialN | Comparison        |  Slope |    SE |           95% CI |     t |      p\n---------------------------------------------------------------------------------------\n-1.65          | Reward - NoReward |  46.99 |  5.41 | [ 36.39,  57.59] |  8.69 | &lt; .001\n-1.28          | Reward - NoReward |  72.46 |  6.13 | [ 60.45,  84.47] | 11.83 | &lt; .001\n-0.92          | Reward - NoReward |  95.17 |  7.03 | [ 81.39, 108.96] | 13.53 | &lt; .001\n-0.55          | Reward - NoReward | 115.44 |  8.13 | [ 99.50, 131.38] | 14.19 | &lt; .001\n-0.18          | Reward - NoReward | 133.60 |  9.53 | [114.92, 152.29] | 14.01 | &lt; .001\n0.18           | Reward - NoReward | 149.96 | 11.31 | [127.79, 172.14] | 13.25 | &lt; .001\n0.55           | Reward - NoReward | 164.78 | 13.50 | [138.31, 191.25] | 12.20 | &lt; .001\n0.92           | Reward - NoReward | 178.29 | 16.09 | [146.75, 209.82] | 11.08 | &lt; .001\n1.28           | Reward - NoReward | 190.68 | 19.04 | [153.36, 228.00] | 10.01 | &lt; .001\n1.65           | Reward - NoReward | 202.11 | 22.31 | [158.39, 245.83] |  9.06 | &lt; .001\n\nMarginal effects estimated for Event\n\n\n[add with same structure as in previous tutorial]\nAlthough we’ve been focusing on the main effects, remember that we also had random intercepts and slopes in our model! We can have a look at the random effects as well:\n\nCodeis_pred_random = estimate_expectation(mod_gamma, include_random =T)\n\nggplot(is_pred_random, aes(x= StandardTrialN, y= Predicted, color= Id, shape = Event))+\n    geom_point(data = df, aes(y= ReactionTime, color= Id), position= position_jitter(width=0.2))+\n    geom_line()+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, fill = Id),color= 'transparent', alpha=0.1)+\n    labs(y='Reaction time', x='# trial')+\n    theme_classic(base_size = 20)+\n    theme(legend.position = 'none')+\n    facet_wrap(~Event)\n\n\n\n\n\n\n\nThe variability is impressive! For example, looking at the no reward condition, we can see that Reaction times are getting slower over time for some infants, but they are getting faster for others! These participants are probably loosing interest in the no reward condition, because they learn across time that nothing interesting is going to appear on the screen after the cue! It’s also really interesting to see how, from the group level estimates, it seemed that the no reward effect was simply weaker than the reward effect, while there’s something more going on: there seem to be more variability in the no reward condition, with a subgroup of infants being especially uninterested in this specific condition only!\nAren’t generalised mixed-effect models pretty cool!?? We get to pick the distribution that better resembles the data and we get to model random effects, and these things really improve the fit of our model! We often get to have stronger main effects (which is very often the main thing we are interested in) and we get the insane bonus of looking at individual differences and unique patterns that we would otherwise completely miss!\nWe hope you’ll get to use GLMMs a lot in your research, and that they will help you find super cool results just like we did here!!!!",
    "crumbs": [
      "Stats",
      "Generalised mixed-effect models"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html",
    "title": "Starting with PsychoPy",
    "section": "",
    "text": "PsychoPy is an open-source software package written in the Python programming language primarily for use in neuroscience and experimental psychology research. It’s one of our favorite ways to create experiments and we will use it through our tutorials.\nSo, let’s start and install PsychoPy!!!",
    "crumbs": [
      "Getting started:",
      "Starting with PsychoPy"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#minicondaminiforge",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#minicondaminiforge",
    "title": "Starting with PsychoPy",
    "section": "Miniconda/Miniforge",
    "text": "Miniconda/Miniforge\nWe recommend installing PsychoPy in a dedicated virtual environment where you can create and run your studies. To ensure optimal performance and avoid compatibility issues, keep this environment as clean as possible. Ideally, install PsychoPy and only the additional libraries required for your study. For any other libraries, for example for analysis, consider creating separate environments to prevent conflicts with PsychoPy\n\nWindowsMac\n\n\nOn windows python 3.8 seems to be a safe bet. Create a new environment:\nconda create -n psychopy python=3.8\n\n\n\n\n\n\nNote\n\n\n\nRemember if you have installed minforge instead of miniconda you can use mamba and conda interchangeably for these steps.\n\n\nonce done we actually need to install psychopy in this new environment. Let’s do that, activate the environment and just type pip install psychopy :\nconda activate psychpy\npip install psychopy\n\n\nOn Mac Python 3.10 seems to work better. Thus create a new environment:\nconda create -n psychopy python=3.10\n\n\n\n\n\n\nNote\n\n\n\nRemember if you have installed minforge instead of miniconda you can use mamba and conda interchangeably for these steps.\n\n\nonce done we actually need to install psychopy in this new environment. Let’s do that, activate the environment and just type pip install psychopy :\nconda activate psychpy\npip install psychopy\n\n\n\nIt may take some time and require your confirmation during the process, but eventually, the setup will complete. Once finished, your psychopy environment will include the PsychoPy library, allowing you to import it easily using import psychopy\n\n\n\n\n\n\nCaution\n\n\n\nIf you are planning to use Spyder as your Ide to interact with PsychoPy you require an additional library to interact with Spyder\nYou can install it by:\nconda install spyder-kernels\nor\npip install spyder-kernels\nIn case you need more information please check our python installation guide",
    "crumbs": [
      "Getting started:",
      "Starting with PsychoPy"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#psychopy-standalone",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#psychopy-standalone",
    "title": "Starting with PsychoPy",
    "section": "Psychopy Standalone",
    "text": "Psychopy Standalone\nIf you’re unable to install PsychoPy using conda/mamba or simply prefer a reliable setup with access to the intuitive PsychoPy GUI, the standalone installer is a great option.\nHowever, it’s important to note that PsychoPy offers two main ways to create experiments:\n\nThe Builder: Ideal for those who prefer a graphical, point-and-click interface. \nThe Coder: Designed for users who prefer to program their experiments from scratch. \n\nIn our tutorials, we will focus on coding experiments directly. If you’re using the PsychoPy standalone installer, you’ll need to follow along using the Coder interface.",
    "crumbs": [
      "Getting started:",
      "Starting with PsychoPy"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html",
    "title": "Pupil data pre-processing",
    "section": "",
    "text": "Welcome to your first step into the world of pupillometry! In this tutorial, we’ll walk you through how to preprocess pupil data using a handy R package called PupillometryR. This package makes it simple to clean and even analyze your pupil data with just a few lines of R code.\nTo keep things straightforward, we’ll be working with a simulated dataset that you can download right here:\nPupilData.zip\nDownload and unzip this folder. This dataset is based on the experimental design we introduced earlier in our eye-tracking series. If you’re not familiar with it or need a quick refresher, we recommend checking out the “Introduction to eye-tracking” guide before diving in.\nThis tutorial serves as a foundation for understanding how to preprocess pupil data. Once you’ve grasped the essentials, we encourage you to explore the full range of functions and features PupillometryR has to offer.",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html#read-the-data",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html#read-the-data",
    "title": "Pupil data pre-processing",
    "section": "Read the data",
    "text": "Read the data\nLet’s begin by importing the necessary libraries and loading the downloaded dataframe.\n\nlibrary(PupillometryR)  # Library to process pupil signal\nlibrary(tidyverse)  # Library to wrangle dataframes\nlibrary(patchwork)\n\nGreat! Now, let’s locate and load all our data files. We’ll use list.files() to identify all the .csv files in our folder. Make sure to update the file path to match the location where your data is stored.\n\ncsv_files = list.files(\n  path       = \"..\\\\..\\\\resources\\\\Pupillometry\\\\RAW\",\n  pattern    = \"\\\\.csv$\",   # regex pattern to match .csv files\n  full.names = TRUE         # returns the full file paths\n)\n\ncsv_files is now a list containing all the .csv files we’ve identified. To better understand our dataset, let’s start by focusing on the first file, representing the first subject, and inspect its structure. This will give us a clear overview before we proceed further.\n\nRaw_data = read.csv(csv_files[1])\nhead(Raw_data) # database peak\n\n  X   Subject      time      L_P      R_P    Event TrialN\n1 1 Subject_1  1.000000 3.187428 3.228510 Fixation      1\n2 2 Subject_1  4.333526 3.153315 3.193957     &lt;NA&gt;     NA\n3 3 Subject_1  7.667052 3.102050 3.142032     &lt;NA&gt;     NA\n4 4 Subject_1 11.000578 3.163670 3.204446     &lt;NA&gt;     NA\n5 5 Subject_1 14.334104 3.152682 3.193316     &lt;NA&gt;     NA\n6 6 Subject_1 17.667630 3.086508 3.126289     &lt;NA&gt;     NA\n\n\nOur dataframe consists of several easily interpretable columns. time represents elapsed time in milliseconds, Subject identifies the participant, and Event indicates when and which stimuli were presented. TrialN tracks the trial number, while L_P and R_P measure pupil dilation for the left and right eyes, respectively, in millimeters.\nLet’s plot the data! Visualizing it first is always a crucial step as it provides an initial understanding of its structure and key patterns.\n\nggplot(Raw_data, aes(x = time, y = R_P)) +\n  geom_line(aes(y = R_P, color = 'Pupil Right'), lwd = 1.2) +\n  geom_line(aes(y = L_P, color = 'Pupil Left'), lwd = 1.2) +\n  geom_vline(data = Raw_data |&gt; dplyr::filter(!is.na(Event)), aes(xintercept = time, linetype = Event), lwd = 1.3) +\n  \n  theme_bw(base_size = 35) +\n  ylim(1, 6) +\n  labs(color= 'Signal', y='Pupil size')+\n  scale_color_manual(\n    values = c('Pupil Right' = '#4A6274', 'Pupil Left' = '#E2725A') )  +\n  theme(\n    legend.position = 'bottom'  ) +\n  guides(\n    color = guide_legend(override.aes = list(lwd = 20)),\n    linetype = guide_legend(override.aes = list(lwd = 1.2))\n  )",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html#prepare-the-data",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html#prepare-the-data",
    "title": "Pupil data pre-processing",
    "section": "Prepare the data",
    "text": "Prepare the data\nNice!! Now we have some sense of our data!! And….you’ve probably noticed two things:\n\nSo many events! That’s intentional — it’s better to have too many triggers than miss something important. When we recorded the data, we saved all possible events to ensure nothing was overlooked. But don’t worry, for our pupil dilation analysis, we only care about two key events: Circle and Square (check the paradigm intro if you need a refresher on why this is)\nSingle-sample events! Like in most studies, events are marked at a single time point (when the stimulus is presented). But PupilometryR needs a different structure — it expects the event value to be repeated for every row while the event is happening.\n\nSo, how do we fix this? First, let’s isolate the rows in our dataframe where the events are Circle or Square. We start by creating a list of the events we care about, then use it to filter our dataframe and keep only the rows related to those events in a new dataframe called Events\n\nEvents_to_keep = c('Circle','Square')\nEvents = filter(Raw_data, Event %in% Events_to_keep) # filter data\nhead(Events) # database peak\n\n      X   Subject       time      L_P      R_P  Event TrialN\n1   221 Subject_1   734.3757       NA       NA Circle      1\n2  2552 Subject_1  8504.8248 3.596057 3.642405 Square      2\n3  4883 Subject_1 16275.2739 3.543367       NA Circle      3\n4  7215 Subject_1 24049.0565 3.164419 3.205205 Circle      4\n5  9546 Subject_1 31819.5055 3.147494 3.188061 Square      5\n6 11877 Subject_1 39589.9546 3.343493 3.386587 Circle      6\n\n\nPerfect! Now onto the second point: we need to repeat the events we just selected for the entire duration we want to analyze. But what’s this duration? We want to cover the full cue presentation (2 seconds), plus an extra 0.1 seconds before the stimulus appears. Why? This pre-stimulus period will serve as our baseline, which we’ll use later in the analysis.\nSo, let’s define how much time to include before and after the stimulus. We’ll also set the framerate of our data (300Hz) and create a time vector that starts from the pre-stimulus period and continues in steps of 1/Hz, with a total length equal to Pre_stim + Post_stim.\n\n# Settings to cut events\nFs = 300 # framerate\nStep = 1000/Fs\n\nPre_stim = 100 # pre stimulus time (100ms)\nPost_stim = 2000 # post stimulus time (2000ms)\nPre_stim_samples = Pre_stim/Step  # pre stimulus in samples\nPost_stim_samples = Post_stim/Step  # post stimulus in samples\n\n# Time vector based on the event duration\nTime = seq(from = -Pre_stim, by=Step, length.out = Pre_stim_samples+Post_stim_samples) # time vector\n\nHere’s where the magic happens. We loop through each event listed in our Events dataframe. Each row in Events corresponds to a specific event (like a “Circle” or “Square” cue) that occurred for a specific subject during a specific trial.\nFor each event, we extract 2 key details:\n\nEvent (to know if it’s a Circle or Square cue)\nTrialN (to know which trial this event is part of)\n\nNext, we identify the rows of interest in our main dataframe. First, we locate the row where the time is closest to the onset of the event. Then, we select a range of rows that fall within the Pre_stim and Post_stim window around the event.\nFinally, we use these identified rows to add the event information. The Time, Event, and TrialN values are repeated across all the rows in this window, ensuring every row in the event window is properly labeled.\n\n# Loop for each event \nfor (trial in 1:nrow(Events)){\n\n    # Extract the information\n    Event = Events[trial,]$Event\n    TrialN = Events[trial,]$TrialN\n    \n    # Event onset information\n    Onset = Events[trial,]$time\n    Onset_index = which.min(abs(Raw_data$time - Onset))\n    \n    # Find the rows to update based on pre post samples\n    rows_to_update = seq(Onset_index - Pre_stim_samples,\n                         Onset_index + Post_stim_samples-1)\n    \n    # Repeat the values of interest for all the rows\n    Raw_data[rows_to_update, 'time'] = Time\n    Raw_data[rows_to_update, 'Event'] = Event\n    Raw_data[rows_to_update, 'TrialN'] = TrialN\n}\n\nPerfect! We’ve successfully extended the event information backward and forward based on our Pre_stim and Post_stim windows. Now, it’s time to clean things up.\nSince we only care about the rows that are part of our trial of interest —and because the event information is now repeated for each row during its duration— we’ll remove all the rows that don’t belong to these event windows. This will leave us with a clean, focused dataset that only contains the data relevant to our analysis.\n\nTrial_data = Raw_data %&gt;% \n    filter(Event %in% Events_to_keep)\n\nFor all subjects\nGreat job making it this far! Fixing the data to make it usable in PupillometryR is definitely one of the trickiest parts. But… we’ve only done this for one subject so far—oops! 😅 No worries, though. Let’s automate this process by putting everything into a loop for each subject. In this loop, we’ll fix the event structure for each subject, store each subject’s processed dataframe in a list, and finally combine all these dataframes into one single dataframe for further analysis. Let’s make it happen!\n\n# Libraries and files --------------------------------------------------------------------\n\nlibrary(PupillometryR)  # Library to process pupil signal\nlibrary(tidyverse)  # Library to wrangle dataframes\nlibrary(patchwork)\n\ncsv_files = list.files(\n  path       = \"..\\\\..\\\\resources\\\\Pupillometry\\\\RAW\",\n  pattern    = \"\\\\.csv$\",   # regex pattern to match .csv files\n  full.names = TRUE         # returns the full file paths\n)\n\n\n# Event settings --------------------------------------------------------------------\n\nFs = 300 # framerate\nStep = 1000/Fs\n\nPre_stim = 100 # pre stimulus time (100ms)\nPost_stim = 2000 # post stimulus time (2000ms)\nPre_stim_samples = Pre_stim/Step  # pre stimulus in samples\nPost_stim_samples = Post_stim/Step  # post stimulus in samples\n\n# Time vector based on the event duration\nTime = seq(from = -Pre_stim, by=Step, length.out = Pre_stim_samples+Post_stim_samples) # time vector\n\n# Event fixes --------------------------------------------------------------------\n\nList_of_subject_dataframes = list() # Empty list to be filled with dataframes\n\n# Loop for each subject\nfor (sub in 1:length(csv_files)) {\n  \n  Raw_data = read.csv(csv_files[sub]) # Raw data\n  Events = filter(Raw_data, Event %in% Events_to_keep) # Events\n  \n  \n  # Loop for each event \n  for (trial in 1:nrow(Events)){\n  \n      # Extract the information\n      Event = Events[trial,]$Event\n      TrialN = Events[trial,]$TrialN\n      \n      # Event onset information\n      Onset = Events[trial,]$time\n      Onset_index = which.min(abs(Raw_data$time - Onset))\n      \n      # Find the rows to update based on pre post samples\n      rows_to_update = seq(Onset_index - Pre_stim_samples,\n                           Onset_index + Post_stim_samples-1)\n      \n      # Repeat the values of interest for all the rows\n      Raw_data[rows_to_update, 'time'] = Time\n      Raw_data[rows_to_update, 'Event'] = Event\n      Raw_data[rows_to_update, 'TrialN'] = TrialN\n  }\n  \n  \n  # Filter only events of interest\n  Trial_data = Raw_data %&gt;% \n    filter(Event %in% Events_to_keep)\n  \n  # Add daframe to list\n  List_of_subject_dataframes[[sub]] = Trial_data\n}\n\n# Combine the list of dataframes into 1 dataframe\nTrial_data = bind_rows(List_of_subject_dataframes)\n\nNow we have our dataset all fixed and organized for each subject. Let’s take a look!\n\nCodeggplot(Trial_data, aes(x = time, y = R_P, group = TrialN)) +\n  geom_line(aes(y = R_P, color = 'Pupil Right'), lwd = 1.2) +\n  geom_line(aes(y = L_P, color = 'Pupil Left'), lwd = 1.2) +\n  geom_vline(aes(xintercept = 0), linetype = 'dashed', color = 'black', lwd = 1.2) +\n  facet_wrap(~Subject) +\n  \n  ylim(1, 6) +\n  scale_color_manual(values = c('Pupil Right' = '#4A6274', 'Pupil Left' = '#E2725A')) +\n  \n  theme_bw(base_size = 35) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nAs you can see, the data structure is now completely transformed. We’ve segmented the data into distinct time windows, with each segment starting at -0.1 seconds (-100 ms) and extending to 2 seconds (2000 ms). This new structure ensures consistency across all segments, making the data ready for further analysis.\nMake PupillometryR data\nOk, now it’s time to start working with PupillometryR! 🎉\nIn the previous steps, we changed our event structure, and you might be wondering — why all that effort? Well, it’s because PupillometryR needs the data in this specific format to do its magic. To get started, we’ll pass our dataframe to the make_pupillometryr_data() function. If you’re already thinking, “Oh no, not another weird object type that’s hard to work with!” — don’t worry! The good news is that the main object it creates is just a regular dataframe. That means we can still interact with like we’re used to. This makes the pre-processing steps much less frustrating. Let’s get started!\n\nPupilR_data = make_pupillometryr_data(data = Trial_data,\n                                 subject = Subject,\n                                 trial = TrialN,\n                                 time = time,\n                                 condition = Event)\n\nHere, we’re simply using the make_pupillometryr_data() function to pass in our data and specify which columns contain the key information. This tells PupillometryR where to find the crucial details, like subject IDs, events, and pupil measurements, so it knows how to structure and process the data properly.\n\n\n\n\n\n\nTip\n\n\n\nIf you have extra columns that you want to keep in your PupillometryR data during preprocessing, you can pass them as a list using the other = c(OtherColumn1, OtherColumn2) argument. This allows you to keep these additional columns alongside your main data throughout most of the preprocessing steps.\nBut here’s a heads-up — not all functions can keep these extra columns every time. For example, downsampling may not retain them since it reduces the number of rows, and it’s not always clear how to summarize extra columns. So, keep that in mind as you plan your analysis!\n\n\nPlot\nOne cool feature of the data created using make_pupillometryr_data() is that it comes with a simple, built-in plot function. This makes it super easy to visualize your data without needing to write tons of code. The plot function works by averaging the data over the group variable. So we can group over subject or condition. Here we use the group variable to focus on the condition and average over the subjects.\nIn this example, we’re plotting the L_P (left pupil) data, grouped by condition. The plot() function is actually just a ggplot2 wrapper, which means you can customize to a certain extent like any other ggplot. That’s why we can add elements to it, like theme_bw(), which gives the plot a cleaner, black-and-white look. Give it a go without adding anything and then learn to customize it!!\n\n\n\n\n\n\nTip\n\n\n\nPro tip! If you want more control over your plots, you can always use ggplot2. Remember, the Pupil data is just a regular dataframe, so you can plot it in any way you like!\n\n\n\nplot(PupilR_data, pupil = L_P, group = 'condition', geom = 'line') +\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn this tutorial, we’ll use two methods to plot our data. We’ll use the PupillometryR plot to visualize the average pupil response by condition, and we’ll also use ggplot to manually plot our data. Both approaches are valid and offer unique benefits.\nThe PupillometryR plot provides a quick overview by automatically averaging pupil responses across condition levels, making it ideal for high-level trend visualization. On the other hand, ggplot gives you full control to visualize specific details or customize every aspect of the plot, allowing for deeper insights and flexibility.",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html#pre-processing",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html#pre-processing",
    "title": "Pupil data pre-processing",
    "section": "Pre-processing",
    "text": "Pre-processing\nNow that we have our pupillometry data in the required format we can actually start the pre-processing!!\nRegress\nThe first step is to regress L_P against R_P (and vice versa) using a simple linear regression. This corrects small inconsistencies in pupil data caused by noise. The regression is done separately for each participant, trial, and time point, ensuring smoother and more consistent pupil dilation measurements.\n\nRegressed_data = regress_data(data = PupilR_data,\n                                pupil1 = L_P,\n                                pupil2 = R_P)\n\nError in `mutate()`:\nℹ In argument: `pupil1newkk = .predict_right(L_P, R_P)`.\nℹ In group 1: `Subject = \"Subject_1\"`, `TrialN = 1`, `Event = \"Circle\"`.\nCaused by error in `lm.fit()`:\n! 0 (non-NA) cases\n\n\nPwa pwa pwaaaaa…!!🤦‍♂️ We got an error!\nWhat’s it saying? It’s telling us that one of the trials is completely full of NAs, and since there’s no data to work with, the function fails. This happens a lot when testing infants — they don’t always do what we expect, like watching the screen. Instead, they move around or look away.\nWe’ll deal with missing data properly later, but for now, we need a quick fix. What can we do? We can simply drop any trials where both pupils (L_P and R_P) are entirely NA. This way, we avoid errors and keep the analysis moving.\nSo let’s filter our data and then redo the last two steps (make PupilR_data and then regress data)\n\n# Filter the trial data\nTrial_data = Trial_data %&gt;%\n    group_by(Subject, TrialN) %&gt;%  # group by Subject and TrialN\n    filter(!all(is.na(L_P) & is.na(R_P))) %&gt;% # filter out if both R_P and L_P are all NA\n    ungroup()  # Remove grouping\n\n# Make pupilloemtryR data\nPupilR_data = make_pupillometryr_data(data = Trial_data,\n                                 subject = Subject,\n                                 trial = TrialN,\n                                 time = time,\n                                 condition = Event)\n# Regress data\nRegressed_data = regress_data(data = PupilR_data,\n                               pupil1 = L_P,\n                                pupil2 = R_P)\n\nAnd now everything worked!! Perfect!\nMean pupil\nAs the next steps we will average the two pupil signals. This will create a new variable called mean_pupil\n\nMean_data = calculate_mean_pupil_size(data = Regressed_data, \n                                       pupil1 = L_P, \n                                       pupil2 = R_P)\n\nplot(Mean_data, pupil = mean_pupil, group = 'condition', geom = 'line')+\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nLowpass\nNow that we have a single pupil signal, we can move on to filtering it. The goal here is to remove fast noise and fluctuations that aren’t relevant to our analysis. Why? Because we know that pupil dilation is a slow physiological signal, and those rapid changes are likely just noise from blinks, eye movements, or measurement errors. By filtering out these fast fluctuations, we can focus on the meaningful changes in pupil dilation that matter for our analysis.\n\nFiltered_data = filter_data(data = Mean_data,\n                             pupil = mean_pupil,\n                             filter = 'median',\n                             degree = 11)\nplot(Filtered_data, pupil = mean_pupil, group = 'condition', geom = 'line')+\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nThere are different ways to filter the data in PupillometryR we suggest you check the actual package website and make decision based on your data (filter_data). Here we use a median filter based on a 11 sample window.\nTrial Rejection\nNow that our data is smaller and smoother, it’s a good time to take a look at it. It doesn’t make sense to keep trials that are mostly missing values, nor does it make sense to keep participants with very few good trials.\nWhile you might already have info on trial counts and participant performance from other sources (like video coding), PupillometryR has a super handy function to check this directly. This way, you can quickly see how many valid trials each participant has and decide which ones to keep or drop.\n\nMissing_data = calculate_missing_data(Filtered_data,\n                                       mean_pupil)\nhead(Missing_data, n=20)\n\n# A tibble: 20 × 3\n   Subject   TrialN Missing\n   &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt;\n 1 Subject_1      2  0.0603\n 2 Subject_1      3  0.125 \n 3 Subject_1      4  0.0746\n 4 Subject_1      5  0.106 \n 5 Subject_1      6  0.0746\n 6 Subject_1      7  0.0635\n 7 Subject_1      8  0.0587\n 8 Subject_1      9  0.0619\n 9 Subject_1     10  0.130 \n10 Subject_2      1  0.143 \n11 Subject_2      2  0.121 \n12 Subject_2      3  0.0619\n13 Subject_2      4  0     \n14 Subject_2      5  0.132 \n15 Subject_2      6  0.137 \n16 Subject_2      7  0.0841\n17 Subject_2      8  0.0968\n18 Subject_2      9  0.0841\n19 Subject_2     10  0.0508\n20 Subject_3      1  0.0587\n\n\nThis gives us a new dataframe that shows the amount of missing data for each subject and each trial. While we could manually decide which trials and subjects to keep or remove, PupillometryR makes it easier with the clean_missing_data() function.\nThis function lets you set two % thresholds — one for trials and one for subjects. Here, we’ll set it to reject trials with more than 25% missing data (keep at least 75% of the data) and reject subjects with more than 25% missing data. This way, we ensure our analysis is based on clean, high-quality data.\n\nClean_data = clean_missing_data(Filtered_data,\n                                 pupil = mean_pupil,\n                                 trial_threshold = .75,\n                                 subject_trial_threshold = .75)\n\nRemoving trials with a proportion missing &gt; 0.75 \n ...removed 3 trials \n\n\nRemoving subjects with a proportion of missing trials &gt; 0.75 \n ...removed 0 subjects \n\n\nSee?! PupillometryR shows us exactly how many trials and subjects are being excluded from our dataframe based on our thresholds. Cool!\n\n\n\n\n\n\nWarning\n\n\n\nNote that this function calculates the percentage of missing trials based only on the trials present in the dataframe. For example, if a participant only completed one trial (and watched it perfectly) before the session had to stop, the percentage would be calculated on that single trial, and the participant wouldn’t be rejected.\nIf you have more complex conditions for excluding participants (e.g., based on total expected trials or additional criteria), you’ll need to handle this manually to ensure subjects are dropped appropriately.\n\n\nFill the signal\nNow our data is clean, but… while the average signal for each condition looks smooth (as seen in our plots), the data for each individual participant is still noisy. We can still spot blinks and missing data in the signal.\nTo handle this, we’ll use interpolation to fill in the missing points. Interpolation “connects the dots” between gaps, creating a more continuous and cleaner signal. This step is crucial because large chunks of missing data can distort our analysis, and interpolation allows us to retain more usable data from each participant.\n\nggplot(Clean_data, aes(x = time, y = mean_pupil, group = TrialN, color= Event))+\n  geom_line( lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  labs(y = 'Pupil Size')+\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nSo to remove these missing values we can interpolate our data. Interpolating is easy with PupillometryR we can simply:\n\nInt_data = interpolate_data(data = Clean_data,\n                             pupil = mean_pupil,\n                             type = 'linear')\n\nggplot(Int_data, aes(x = time, y = mean_pupil, group = TrialN, color = Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  labs(y = 'Pupil Size')+\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nDone!! Well, you’ve probably noticed something strange… When there’s a blink, the pupil signal can rapidly decrease until it’s completely missing. Right now, this drop gets interpolated, and the result is a weird, unrealistic curve where the signal dips sharply and then smoothly recovers. This makes our data look horrible! 😩\nLet’s fix it!\nTo do this, we’ll use PupillometryR’s blink detection functions. There are two main ways to detect blinks:\n\nBased on size — detects pupil size.\nBased on velocity — detects rapid changes in pupil size (which happens during blinks).\n\nHere, we’ll use detection by velocity. We set a velocity threshold to detect when the pupil size changes too quickly. To ensure we capture the full blink, we use extend_forward and extend_back to expand the blink window, including the fast decrease in pupil size. The key idea is to make the entire blink period NA, not just the moment the pupil disappears. This prevents interpolation from creating unrealistic artifacts. When we interpolate, the process skips over the entire blink period, resulting in a cleaner, more natural signal.\n\nBlink_data = detect_blinks_by_velocity(\n    Clean_data,\n    mean_pupil,\n    threshold = 0.1,\n    extend_forward = 70,\n    extend_back = 70)\n\nggplot(Blink_data, aes(x = time, y = mean_pupil, group = TrialN, color=Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  labs(y = 'Pupil Size')+\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nSee !! now the rapid shrinking disappeared and we can now interpolate\n\nInt_data = interpolate_data(data = Blink_data,\n                             pupil = mean_pupil,\n                             type = 'linear')\n\nggplot(Int_data, aes(x = time, y = mean_pupil, group = TrialN, color=Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  labs(y = 'Pupil Size')+\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nLook how beautiful our signal is now!! 😍 Good job!!!\n\n\n\n\n\n\nCaution\n\n\n\nYou won’t always run into blink issues like this. Downsampling and filtering usually handle rapid changes during earlier preprocessing steps. Whether this happens can depend on the tracker, sampling rate, or even the population you’re testing. In this simulated data, we exaggerated the blink effects on purpose to show you how to spot and fix them! Thus, blink detection may not always be necessary. The best approach is to check your data before deciding. And how do you check it? Plotting! Plotting your signal is the best way to see if blinks are causing rapid drops or if you’re just dealing with missing data. Let the data guide your decisions.\n\n\nDownsample\nAs mentioned before, Pupil dilation is a slow signal, so 20Hz is enough — no need for 300Hz. Downsampling reduces file size, speeds up processing, and even naturally smooths the signal, all while preserving the key information we need for analysis. To downsample to 20Hz, we’ll set the timebin size to 50 ms (since 1000/20 = 0.05 seconds = 50 ms) and calculate the median for each time bin.\n\nNewHz = 20\ntimebinSize = 1000/NewHz\n\nDownsampled_data = downsample_time_data(data = Int_data,\n                              pupil = mean_pupil,\n                              timebin_size = timebinSize,\n                              option = 'median')\nplot(Downsampled_data, pupil = mean_pupil, group = 'condition', geom = 'line') +\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nBaseline Correction\nGood job getting this far!! We’re now at the final step of our pre-processing: baseline correction.\nBaseline correction helps remove variability between trials and participants, like differences in baseline pupil size caused by individual differences, fatigue, or random fluctuations. By doing this, we can focus only on the variability caused by our paradigm. This step ensures that any changes we see in pupil size are truly driven by the experimental conditions, not irrelevant noise. To further adjust the data, we’ll subtract the calculated baseline, ensuring the values start at 0 instead of -100. Finally, to make the next steps of analysis easier, we’ll select only the columns of interest, dropping any irrelevant ones.\nLet’s get it done!\n\nBase_data = baseline_data(data = Downsampled_data,\n                           pupil = mean_pupil,\n                           start = -100,\n                           stop = 0)\n\n# Remove the baseline\nFinal_data = subset_data(Base_data, start = 0) %&gt;% \n  select(Subject, Event, TrialN, mean_pupil, time)\n\nLet’s plot it to see what baseline correction and removal are actually doing!! We will plot both the average signal using the plot function (with some addition information about color and theme) and using ggplot to plot the data for each subject separately.\n\nOne = plot(Final_data, pupil = mean_pupil, group = 'condition', geom = 'line')+\n  theme_bw(base_size = 45) +\n  theme(legend.position = 'none')\n\n\nTwo = ggplot(Final_data, aes(x = time, y = mean_pupil, group = TrialN, color = Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  labs(y = 'Pupil Size')+\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n# Using patchwork to put the plot together\nOne / Two\n\n\n\n\n\n\n\nSave and analysis\nThis tutorial will not cover the analysis of pupil dilation. We’ll stop here since, after baseline correction, the data is ready to be explored and analyzed. From this point on, we’ll shift from pre-processing to analysis, so it’s a good idea to save the data as a simple .csv file for easy access and future use.\n\nwrite.csv(Final_data, \"..\\\\..\\\\resources\\\\Pupillometry\\\\Processed\\\\Processed_PupilData.csv\")\n\nThere are multiple ways to analyze pupil data, and we’ll show you some of our favorite methods in a dedicated tutorial: Analyze Pupil Dilation.",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html#cite-pupillometryr",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html#cite-pupillometryr",
    "title": "Pupil data pre-processing",
    "section": "Cite PupillometryR",
    "text": "Cite PupillometryR\nIf you decide to use PupillometryR in your analysis, don’t forget to cite it! Proper citation acknowledges the authors’ work and supports the development of such valuable tools.\n\nCodecitation(\"PupillometryR\")\n\nTo cite PupillometryR in publications use:\n\n  Forbes, S. H. (2020). PupillometryR: An R package for preparing and\n  analysing pupillometry data. Journal of Open Source Software, 5(50),\n  2285. https://doi.org/10.21105/joss.02285\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {PupillometryR: An R package for preparing and analysing pupillometry data},\n    author = {Samuel H. Forbes},\n    journal = {Journal of Open Source Software},\n    year = {2020},\n    volume = {5},\n    number = {50},\n    pages = {2285},\n    doi = {10.21105/joss.02285},\n  }",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html#all-code",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html#all-code",
    "title": "Pupil data pre-processing",
    "section": "All code",
    "text": "All code\nHere below we report the whole code we went trough this tutorial as an unique script to make it easier for you to copy and explore it in it’s entirety.\n\n# Libraries and files --------------------------------------------------------------------\n\nlibrary(PupillometryR)  # Library to process pupil signal\nlibrary(tidyverse)  # Library to wrangle dataframes\nlibrary(patchwork)\n\ncsv_files = list.files(\n  path       = \"..\\\\..\\\\resources\\\\Pupillometry\\\\RAW\",\n  pattern    = \"\\\\.csv$\",   # regex pattern to match .csv files\n  full.names = TRUE         # returns the full file paths\n)\n\n\n# Prepare data --------------------------------------------------------------------\n\n## Event settings --------------------------------------------------------------------\n\n# Settings to cut events\nFs = 300 # framerate\nStep = 1000/Fs\n\nPre_stim = 100 # pre stimulus time (100ms)\nPost_stim = 2000 # post stimulus time (2000ms)\nPre_stim_samples = Pre_stim/Step  # pre stimulus in samples\nPost_stim_samples = Post_stim/Step  # post stimulus in samples\n\n# Time vector based on the event duration\nTime = seq(from = -Pre_stim, by=Step, length.out = (Pre_stim+Post_stim)/Step) # time vector\n\n\n## Event fixes --------------------------------------------------------------------\n\nList_of_subject_dataframes = list() # Empty list to be filled with dataframes\n\n# Loop for each subject\nfor (sub in 1:length(csv_files)) {\n  \n  Raw_data = read.csv(csv_files[sub]) # Raw data\n  Events = filter(Raw_data, Event %in% Events_to_keep) # Events\n\n  \n  # Loop for each event \n  for (trial in 1:nrow(Events)){\n  \n      # Extract the information\n      Event = Events[trial,]$Event\n      TrialN = Events[trial,]$TrialN\n      \n      # Event onset information\n      Onset = Events[trial,]$time\n      Onset_index = which.min(abs(Raw_data$time - Onset))\n      \n      # Find the rows to update based on pre post samples\n      rows_to_update = seq(Onset_index - Pre_stim_samples,\n                           Onset_index + Post_stim_samples-1)\n      \n      # Repeat the values of interest for all the rows\n      Raw_data[rows_to_update, 'time'] = Time\n      Raw_data[rows_to_update, 'Event'] = Event\n      Raw_data[rows_to_update, 'TrialN'] = TrialN\n  }\n  \n  \n  # Filter only events of interest\n  Trial_data = Raw_data %&gt;% \n    filter(Event %in% Events_to_keep)\n  \n  # Add daframe to list\n  List_of_subject_dataframes[[sub]] = Trial_data\n}\n\n# Combine the list of dataframes into 1 dataframe\nTrial_data = bind_rows(List_of_subject_dataframes)\n\n\n### Plot Raw Data -----------------------------------------------------------------\n\nggplot(Raw_data, aes(x = time, y = R_P)) +\n  geom_line(aes(y = R_P, color = 'Pupil Right'), lwd = 1.2) +\n  geom_line(aes(y = L_P, color = 'Pupil Left'), lwd = 1.2) +\n  geom_vline(data = Raw_data |&gt; dplyr::filter(!is.na(Event)), aes(xintercept = time, linetype = Event), lwd = 1.3) +\n  \n  theme_bw(base_size = 45) +\n  ylim(1, 6) +\n  labs(color= 'Signal', y='Pupil size')+\n  scale_color_manual(\n    values = c('Pupil Right' = '#4A6274', 'Pupil Left' = '#E2725A') )  +\n  theme(\n    legend.position = 'bottom'  ) +\n  guides(\n    color = guide_legend(override.aes = list(lwd = 20)),\n    linetype = guide_legend(override.aes = list(lwd = 1.2))\n  )\n\n\n\n# Pre-processing -----------------------------------------------------------------\n\n## Filter Out Trials with all NA -----------------------------------------------------------------\n\nTrial_data = Trial_data %&gt;%\n  group_by(Subject, TrialN) %&gt;%\n  filter(!all(is.na(L_P) & is.na(R_P))) %&gt;%\n  ungroup()\n\n\n## Make PupillometryR Data -----------------------------------------------------------------\nPupilR_data = make_pupillometryr_data(data = Trial_data,\n                                      subject = Subject,\n                                      trial = TrialN,\n                                      time = time,\n                                      condition = Event)\n\n### Plot ------------------------------------------------------------------\nplot(PupilR_data, pupil = L_P, group = 'condition', geom = 'line') +\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Regress Data -----------------------------------------------------------------\n\nRegressed_data = regress_data(data = PupilR_data,\n                               pupil1 = L_P,\n                               pupil2 = R_P)\n\n\n## Calculate Mean Pupil -----------------------------------------------------------------\n\nMean_data = calculate_mean_pupil_size(data = Regressed_data, \n                                       pupil1 = L_P, \n                                       pupil2 = R_P)\n\n### Plot --------------------------------------------------------------------\n\nplot(Mean_data, pupil = mean_pupil, group = 'condition', geom = 'line')+\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Lowpass Filter -----------------------------------------------------------------\n\nfiltered_data = filter_data(data = Mean_data,\n                             pupil = mean_pupil,\n                             filter = 'median',\n                             degree = 11)\n\n### Plot --------------------------------------------------------------------\n\nplot(filtered_data, pupil = mean_pupil, group = 'condition', geom = 'line')+\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Downsample -----------------------------------------------------------------\n\nNewHz = 20\n\ntimebinSize = 1 / NewHz\n\nDownsampled_data = downsample_time_data(data = filtered_data,\n                                         pupil = mean_pupil,\n                                         timebin_size = timebinSize,\n                                         option = 'median')\n\n# Plot --------------------------------------------------------------------\n\nplot(Downsampled_data, pupil = mean_pupil, group = 'condition', geom = 'line') +\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Calculate Missing Data -----------------------------------------------------------------\n\nMissing_data = calculate_missing_data(Downsampled_data, mean_pupil)\n\n\n## Clean Missing Data -----------------------------------------------------------------\n\nClean_data = clean_missing_data(Downsampled_data,\n                                 pupil = mean_pupil,\n                                 trial_threshold = .75,\n                                 subject_trial_threshold = .75)\n\n### Plot --------------------------------------------------------------------\n\nggplot(Clean_data, aes(x = Time, y = mean_pupil, group = TrialN, color= Event))+\n  geom_line( lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n  \n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Detect Blinks -----------------------------------------------------------------\n\nBlink_data = detect_blinks_by_velocity(\n  Clean_data,\n  mean_pupil,\n  threshold = 0.1,\n  extend_forward = 50,\n  extend_back = 50)\n\n### Plot --------------------------------------------------------------------\n\nggplot(Blink_data, aes(x = time, y = mean_pupil, group = TrialN, color=Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n  \n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Interpolate Data -----------------------------------------------------------------\n\nInt_data = interpolate_data(data = Clean_data,\n                             pupil = mean_pupil,\n                             type = 'linear')\n\n### Plot --------------------------------------------------------------------\n\nggplot(Int_data, aes(x = Time, y = mean_pupil, group = TrialN, color = Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20)))\n\n\n# Baseline correction -----------------------------------------------------\n\nBase_data = baseline_data(data = Int_data,\n                           pupil = mean_pupil,\n                           start = -100,\n                           stop = 0)\n\n# Remove the baseline\nFinal_data = subset_data(Base_data, start = 0)\n\n\n### Final plot --------------------------------------------------------------\n\nOne = plot(Final_data, pupil = mean_pupil, group = 'condition')+\n  theme_bw(base_size = 45) +\n  theme(legend.position = 'none')\n\n\nTwo = ggplot(Final_data, aes(x = time, y = mean_pupil, group = TrialN, color = Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n  \n  facet_wrap(~Subject)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\nOne / Two\n\n\n\n# Save data ---------------------------------------------------------------\n\nwrite.csv(Final_data, \"..\\\\..\\\\resources\\\\Pupillometry\\\\Processed\\\\Peocessed_PupilData.csv\")",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/Intro_eyetracking.html",
    "href": "CONTENT/EyeTracking/Intro_eyetracking.html",
    "title": "Introduction to eye-tracking",
    "section": "",
    "text": "Eye tracking is a great tool to study cognition. It is especially suitable for developmental studies, as infants and young children might have advanced cognitive abilities, but little chances to show them (they cannot talk!).\nThis tutorial will teach you all you need to navigate the huge and often confusing eye-tracking world. First, we will introduce how an experiment can (and should) be built, explaining how to easily record eye-tracking data from Python. Then, it will focus on how to analyse the data, reducing the seemingly overwhelming amount of rows and columns in a few variables of interest (such as saccadic latency, looking time, or pupil dilation).",
    "crumbs": [
      "Eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/Intro_eyetracking.html#what-do-you-want-to-measure",
    "href": "CONTENT/EyeTracking/Intro_eyetracking.html#what-do-you-want-to-measure",
    "title": "Introduction to eye-tracking",
    "section": "What do you want to measure?",
    "text": "What do you want to measure?\n\nLooking time\nIt is much easier to start an eye-tracking project if you have a clear idea of what you want to measure. Classic paradigms on infant research rely on looking time (How long are infants attending a given stimulus?) and are often called Violation-of-Expectation tasks. They familiarize infants with a given stimulus or situation (e.g. a cat) and, after a given number of presentations (e.g., 8), they present a different stimulus (e.g., a dog). Do infants look longer at the dog, compared to the cat? If so, they were able to spot that something changed.\n\n\n\n\n\n\nImportant\n\n\n\nBeware! This does not mean that they can distinguish cats and dogs, but more simply that they can spot any difference between the two images. A careful control of the stimuli should be in place if we want to make strong conclusions from looking time.\n\n\n\n\nSaccadic Latency\nAnother very popular eye-tracking measure is saccadic latency. It measures how quickly infants can direct their gaze onto a stimulus, once it is presented on screen. This is a great measure of learning because infants will be faster at looking at a stimulus if they expect it to appear in a given position on the screen. They can even be so fast that they anticipate the correct location of the stimulus, even before the stimulus appears! This is called an anticipatory look.\n\n\n\n\n\n\nImportant\n\n\n\nBeware! Saccadic latencies are not a perfect measure of learning. Infants might be faster at looking at something just because they are more interested (pick interesting stimuli to keep them engaged!), and they might become slower due to boredom or fatigue (introduce variation in the stimuli, so that they become less boring over time!).\n\n\n\n\nPupillometry\nThe fanciest eye-tracking measure is pupil dilation. It allows us to measure arousal (How interested is the infant in the stimulus?), cognitive effort (How difficult is the task?), and - my special favourite - uncertainty (Does the infant know what will happen next?). However, its fame comes at a price: It is not only the fanciest, but also the most persnickety…\n\n\n\n\n\n\nImportant\n\n\n\nBeware! Stimuli should be carefully designed, controlling their luminance (not too high, and ALWAYS the same) to avoid that task-unrelated variations in light will affect your measurements; They have to be presented in the same location on the screen, as pupil size changes depending on screen location; Pupil dilation is very slow, so the stimulus presentation also has to be slow; Often, a fixation cross has to precede the moment in which pupil dilation is measured, so that the pupil size can return to baseline before the event you care about happens. But if you feel confident about your paradigm, go for it!!\n\n\nHere is a visual summary of what you can measure:",
    "crumbs": [
      "Eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html",
    "title": "From fixations to measures",
    "section": "",
    "text": "In the previous two tutorials we collected some eye-tracking data and then we used I2MC to extract the fixations from that data. Let’s load the data we recorded and pre-processed in the previous tutorial. We will import some libraries and read the raw data and the output from I2MC.\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n#%% Settings\n\n# Screen resolution\nscreensize = (1920, 1080) \n\n#%% Settings and reading data\n\n# Define paths using pathlib\nbase_path = Path('../../resources/FromFixationToData/DATA')\n\n# The fixation data extracted from I2MC\nFixations = pd.read_csv(base_path / 'i2mc_output' / 'Adult1' / 'Adult1.csv')\n\n# The original RAW data\nRaw_data = pd.read_csv(base_path / 'RAW' / 'Adult1.csv')\nWhat can we do with just the raw data and the fixations? Not much I am afraid. But we can use these fixations to extract some more meaningful indexes.\nIn this tutorial, we will look at how to extract two variables from our paradigm:\nSo what do these two measures have in common? pregnant pause for you to answer EXACTLY!!! They are both clearly related to the position of our stimuli. For this reason, it is important to define Areas Of Interest (AOIs) on the screen (for example, the locations of the targets). Defining AOIs will allow us to check, for each single fixation, whether it happened in an area that we are interested in.",
    "crumbs": [
      "Eye-tracking",
      "From fixations to measures"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html#define-aois",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html#define-aois",
    "title": "From fixations to measures",
    "section": "Define AOIs",
    "text": "Define AOIs\nLet’s define AOIs. We will define two squares around the target locations. To do this, we can simply pass two coordinates for each AOI: the lower left corner and the upper right corner of an imaginary square.\nAn important point to understand is that tobii and Psychopy use two different coordinate systems:\n\nPsychopy has its origin (0,0) in the centre of the window/screen by default.\nTobii reports data with its origin (0,0) in the lower left corner.\n\nThis inconsistency is not a problem per se, but we need to take it into account when defining the AOIs. Let’s try to define the AOIs:\n\n# Screen resolution\nscreensize = (1920, 1080) \n\n# Define the variable realted to AOIs and target position\ndimension_of_AOI = 600/2  #the dimension of the AOIs, divided by 2\nTarget_position = 500 #the position of the targets relative to the centre (e.g., 500 pixels on the right from the centre)\n\n# Create areas of intescreensizet\nAOI1 =[[screensize[0]/2 - Target_position - dimension_of_AOI, screensize[1]/2-dimension_of_AOI], [screensize[0]/2 - Target_position + dimension_of_AOI, screensize[1]/2 + dimension_of_AOI]]\n\nAOI2 =[[screensize[0]/2 + Target_position - dimension_of_AOI, screensize[1]/2-dimension_of_AOI], [screensize[0]/2 + Target_position + dimension_of_AOI, screensize[1]/2 + dimension_of_AOI]]\n\nAOIs = [AOI1, AOI2]\n\nNice!! This step is essential. We have created two AOIs. We will use them to define whether each fixation of our participant was within either of these two AOIs. Let’s get a better idea by just plotting these two AOIs and two random points (600, 500) and (1400,1000).\n\n\nCode\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create a figure\nfig, ax = plt.subplots(1, figsize=(8,4.4))\n\n# Set the limits of the plot\nax.set_xlim(0, 1920)\nax.set_ylim(0, 1080)\n\n# Define the colors for the rectangles\ncolors = ['#46AEB9', '#C7D629']\n\n# Create a rectangle for each area of interest and add it to the plot\nfor i, (bottom_left, top_right) in enumerate(AOIs):\n    width = top_right[0] - bottom_left[0]\n    height = top_right[1] - bottom_left[1]\n    rectangle = patches.Rectangle(bottom_left, width, height, linewidth=2, edgecolor='k', facecolor=colors[i])\n    ax.add_patch(rectangle)\n\nax.plot(600,500,marker='o', markersize=8, color='green')    \nax.plot(1400,1000,marker='o', markersize=8, color='red')    \n\n# Show the plot\nplt.show()",
    "crumbs": [
      "Eye-tracking",
      "From fixations to measures"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html#points-in-aois",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html#points-in-aois",
    "title": "From fixations to measures",
    "section": "Points in AOIs",
    "text": "Points in AOIs\nAs you can see, we are plotting the two AOIs and two points. One falls into one of them and the other doesn’t. But how can we get Python to tell us if a point falls within one of our AOIs?\nWe can check whether the (x, y) coordinates of the point are within the x and y coordinates of the left bottom and top right corners of the AOI.\n\n\n\n\n\nSo imagine we have a point: point and an area: area, we can check if the point falls inside the area by:\n\n# Extract bottom left and top right points\nbottom_left, top_right = area\n\n# Extract the x and y of each point\nbottom_x, bottom_y = bottom_left\ntop_x, top_y = top_right\n\n# Extract the x and y of our point of interest\nx, y = point\n\n# Check if the point is in the area\nbottom_x &lt;= x &lt;= top_x and bottom_y &lt;= y &lt;= top_y\n\nPerfect, this will return True if the point falls inside the area and False if it falls outside. Since we have two AOIs and not just one, we want to make things a bit fancier. We will create a function that checks if a point falls within a list of areas, and tells us which area it falls in.\nWe will run the code above in a loop using enumerate. This extracts two elements to our loop: the index of the element and the element itself. In our case the index of the area and the area itself. This is very useful as we can then use both of these two pieces of information. We will use the actual area to check if our points fall into it. Then, if it does, we will return the index of that area. Conversely, if the point doesn’t fall in any area the function will return -1.\n\n# We define a function that simply takes the a point and a list of areas.\n# This function checks in which area this point is and return the index\n# of the area. If the point is in no area it returns -1\ndef find_area_for_point(point, areas):\n\n    for i, area in enumerate(areas):\n        # Extract bottom left and top right points\n        bottom_left, top_right = area\n        \n        # Extract the x and y of each point\n        bottom_x, bottom_y = bottom_left\n        top_x, top_y = top_right\n        \n        # Extract the x and y of our point of interest\n        x, y = point\n        \n        # Check if the point is in the area\n        if bottom_x &lt;= x &lt;= top_x and bottom_y &lt;= y &lt;= top_y :\n            return(i)\n    return(-1)\n\nNow we have a cool function to check whether a point falls into any of our AOIs. We can use this function to filter the fixations that are in the AOIs: These are the only ones we care about.",
    "crumbs": [
      "Eye-tracking",
      "From fixations to measures"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html#first-fixation",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html#first-fixation",
    "title": "From fixations to measures",
    "section": "First fixation",
    "text": "First fixation\nWe have all these latency values, but we only want the first/fastest of each trial. How can we extract this information easily? We will use groupby. Groupby allows us to perform specific functions/commands on grouped sections of a data frame.\nHere we will groupby by Events and TrialN and for each of these grouped pieces of dataframe we will extract the smallest (min()) value of latency.\n\n# We extract the first fixation of our dataframe for each event\nSaccadicLatency = Correct_Target_fixations.groupby(['Event', 'TrialN'])['Latency'].min().reset_index()\n\nHere we have our Saccadic latency!!!\nOnce our dataset is ready, we might want to visualise the data. For example, we can plot how saccadic latency changes across trials with seaborn:\n\nimport seaborn as sns # import seaborn\nplt.figure()\n\n# Scatterplot\nax = sns.scatterplot(data=SaccadicLatency, x=\"TrialN\", y=\"Latency\", hue='Event')\n\n# Place the legend \nsns.move_legend(ax,loc=\"upper left\")\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "Eye-tracking",
      "From fixations to measures"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html",
    "title": "Create an eye-tracking experiment",
    "section": "",
    "text": "This page will show you how to collect eye-tracking data in a simple Psychopy paradigm. We will use the same paradigm that we built together in the Getting started with Psychopy tutorial. If you have not done that tutorial yet, please go through it first.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#install",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#install",
    "title": "Create an eye-tracking experiment",
    "section": "Install",
    "text": "Install\nTo install the Python Tobii SDK, we can simply run this command in our conda terminal:\npip install tobii_research\nGreat! We have installed the Tobii SDK.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#connect-to-the-eye-tracker",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#connect-to-the-eye-tracker",
    "title": "Create an eye-tracking experiment",
    "section": "Connect to the eye-tracker",
    "text": "Connect to the eye-tracker\nSo how does this library work, how do we connect to the eye-tracker and collect our data? Very good questions!\nThe tobii_research documentation is quite extensive and describes in detail a lot of functions and data classes that are very useful. However, we don’t need much to start our experiment.\nFirst we need to identify all the eye trackers connected to our computer. Yes, plural, tobii_research will return a list of all the eye trackers connected to our computer. 99.99999999% of the time you will only have 1 eye tracker connected, so we can just select the first (and usually only) eye tracker found.\n\n# Import tobii_research library\nimport tobii_research as tr\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\nPerfect!! We have identified our eye-trackers, and we have selected the first one (and only).\nWe are now ready to use our eye-tracker to collect some data… but how?",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#collect-data",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#collect-data",
    "title": "Create an eye-tracking experiment",
    "section": "Collect data",
    "text": "Collect data\nTobii_research has a cool way of telling us what data we are collecting at each time point. It uses a callback function. What is a callback function, you ask? It is a function that tobii runs each time it has a new data point. Let’s say we have an eye tracker that collects data at 300Hz (300 samples per second): the function will be called every time the tobii has one of those 300 samples.\nThis callback function will give us a gaze_data object. This object contains multiple information of that collected sample and we can simply select the information we care about. In our case we want:\n\nthe system_time_stamp, our time variable\nthe left_eye.gaze_point.position_on_display_area, it contains the coordinates on the screen of the left eye (both x and y)\nthe right_eye.gaze_point.position_on_display_area, it contains the coordinates on the screen of the right eye (both x and y)\nthe left_eye.pupil.diameter, is the pupil diameter of the left eye\nthe right_eye.pupil.diameter, is the pupil diameter of the right eye\nthe left_eye.gaze_point.validity, this is a value that tells us whether we think the recognition is ok or not\n\nHere is our callback function:\n\ndef gaze_data_callback(gaze_data):\n\n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0]\n    ly = gaze_data.left_eye.gaze_point.position_on_display_area[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0]\n    ry = gaze_data.right_eye.gaze_point.position_on_display_area[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n\nHow we said, this function will be called every time tobii has a new data-point. COOL!! Now we need to tell tobii to run this function we have created. This is very simple, we can just do the following:\n\n# Start the callback function\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\nWe are telling tobii that we are interested in the EYETRACKER_GAZE_DATA and that we want it to pass it to our function gaze_data_callback.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#triggersevents",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#triggersevents",
    "title": "Create an eye-tracking experiment",
    "section": "Triggers/Events",
    "text": "Triggers/Events\nAs we have seen, our callback function can access the tobii data and tell us what it is for each sample. Just one little piece missing… We want to know what we presented and when. In most studies, we present stimuli that can be pictures, sounds or even videos. For the following analysis, it is important to know at what exact point in time we presented these stimuli.\nLuckily there is a simple way we can achieve this. We can set a text string that our callback function can access and include in our data. To make sure that our callback function can access this variable we will use the global keyword. This makes sure that we can read/modify a variable that exists outside of the function.\nIn this way, each time the callback function runs, it has access to the trigger variable. At the end of the function, we reset the trigger to an empty string. This ensures that each trigger event appears exactly once in our data, precisely at the time it occurs, rather than being duplicated across multiple samples.\nThis means that we can set the trigger to whatever string we want and when we set it it will be picked up by the callback function.\n\ndef gaze_data_callback(gaze_data):\n    global trigger\n\n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0]\n    ly = gaze_data.left_eye.gaze_point.position_on_display_area[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0]\n    ry = gaze_data.right_eye.gaze_point.position_on_display_area[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n    \n    trigger = '' # empty trigger\n    \n    \ntrigger = ''\n\n# Time passes\n# when you present a stimulus you can set trigger to a string that will be captured by the callabck function\n\ntrigger = 'Presented Stimulus'\n\nPerfect! Now we have\n\na way to access the data from the eye-tracker and\nknow exactly what stimuli we are presenting the participant and when.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#correct-the-data",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#correct-the-data",
    "title": "Create an eye-tracking experiment",
    "section": "Correct the data",
    "text": "Correct the data\nTobii presents gaze data in a variety of formats. The one we’re most interested in is the Active Display Coordinate System (ADCS). This system maps all gaze data onto a 2D coordinate system that aligns with the Active Display Area. When an eye tracker is used with a monitor, the Active Display Area refers to the display area that doesn’t include the monitor frame. In the ADCS system, the point (0, 0) represents the upper left corner of the screen, and (1, 1) represents the lower right corner.\nWhile this coordinate system is perfectly acceptable, it might cause some confusion when we come to analyze and plot the data. This is because in most systems, the data’s origin is located in the lower left corner, not the upper left. It might seem a bit complicated, but the image below will make everything clear.\n\n\n\n\n\nFor this reason, we typically adjust the data to position the origin in the bottom left corner. This can be achieved by subtracting the gaze coordinates from the maximum window height size.\nIn addition to the origin point issue, the gaze coordinates are reported between 0 and 1. It’s often more convenient to handle data in pixels, so we can transform our data into pixels. This is done by multiplying the obtained coordinates by the pixel dimensions of our screen.\nLastly, we also modify the time column. Tobii provides data in microseconds, but such precision isn’t necessary for our purposes, so we convert it to milliseconds by dividing by 1000.\n\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global winsize\n\n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n    \n    trigger = '' # empty trigger\n    \n    \ntrigger = ''\nwinsize = (1920, 1080)",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#save-the-data",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#save-the-data",
    "title": "Create an eye-tracking experiment",
    "section": "Save the data",
    "text": "Save the data\nYou might have noticed that we get the data in our callback function but don’t actually save it anywhere. So how to save them? There are two main approaches we can use:\n\nWe could have a saving function inside our callback that could append the new data to a .csv each time the callback is called.\nWe could append the data to a list. Once the experiment is finished we could save our data.\n\nThese two approaches have however some weaknesses. The first could slow down the callback function if our PC is not performing or if we are sampling at a very high sampling rate. The second is potentially faster, but if anything happens during the study which makes python crash (and trust me, it can happen…..) you would lose all your data.\nWhat is the solution you ask? A mixed approach!!!!!!!\nWe can store our data in a list and save it during the less important parts of the study, for example the Inter Stimulus Interval (the time between a trial and another). So let’s write a function to do exactly that.\nLets first create an empty list that we will fill with out data from the callback function. As before, we make sure that our callback function can access this list and append the new data we will use the global keyword.\n\n# Create an empty list we will append our data to\ngaze_data_buffer = []\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global gaze_data_buffer\n    global trigger\n    global winsize\n\n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,trigger))\n    \n    trigger = '' # empty trigger\n\nNow the gaze_data_buffer will be filled with the data we extract. Let’s save this list then.\nWe will first make a copy of the list, and then empty the original list. In this way, we have our data stored, while the original list is empty and can be filled with new data.\nAfter creating a copy, we use pandas to transform the list into a data frame and save it to a csv file. Using mode = 'a' we tell pandas to append the new data to the existing .csv. If this is the first time we are trying to save the data the .csv does not yet exist, so pandas will create a new csv instead.\n\ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names for the dataframe\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = os.path.isfile(output_path)\n    \n    # Write the DataFrame to a csv file\n    out.to_csv(output_path, mode='a', index =False, header = not file_exists)",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#short-recap-of-the-paradigm",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#short-recap-of-the-paradigm",
    "title": "Create an eye-tracking experiment",
    "section": "Short recap of the paradigm",
    "text": "Short recap of the paradigm\nAs we already mentioned, we will use the experimental design that we created in Getting started with Psychopy as a base and we will add things to it to make it an eye-tracking study. If you don’t remember the paradigm please give it a rapid look as we will not go into much detail about each specific part of it.\nHere a very short summary of what the design was: After a fixation cross, two shapes can be presented: a circle or a square. The circle indicates that a reward will appear on the right of the screen while the square predicts the appearance of an empty cloud on the left.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#combine-things",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#combine-things",
    "title": "Create an eye-tracking experiment",
    "section": "Combine things",
    "text": "Combine things\nLet’s try to build together the experiment then.\n\nImport and functions\nTo start, let’s import the libraries and define the two functions that we create before\n\nimport os\n# Import some libraries from PsychoPy and others\nimport os\nfrom pathlib import Path\nimport pandas as pd\n\n# Import some libraries from PsychoPy\nfrom psychopy import core, event, visual, sound\n\nimport tobii_research as tr\n\n\n#%% Functions\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global gaze_data_buffer\n    global winsize\n\n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,trigger))\n    \n    trigger = '' # empty trigger\n    \n        \ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = os.path.isfile(output_path)\n    \n    # Write the DataFrame to a csv file\n    out.to_csv(output_path, mode='a', index =False, header = not file_exists)\n\n\n\nLoad the stimuli\nNow we are going to set a few settings, such as the screen size, create a Psychopy window, load the stimuli and then prepare the trial definition. This is exactly the same as we did in the previous Psychopy tutorial.\n\n#%% Load and prepare stimuli\n\n# Setting the directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n# Now create a Path object for the stimuli directory\nstimuli_dir = Path('EXP') / 'Stimuli'\n\n# Winsize\nwinsize = (1920, 1080)\n\n# create a window\nwin = visual.Window(size = winsize,fullscr=True, units=\"pix\", pos =(0,30), screen=1)\n\n# Load images \nfixation = visual.ImageStim(win, image=str(stimuli_dir / 'fixation.png'), size=(200, 200))\ncircle = visual.ImageStim(win, image=str(stimuli_dir / 'circle.png'), size=(200, 200))\nsquare = visual.ImageStim(win, image=str(stimuli_dir / 'square.png'), size=(200, 200))\nwinning = visual.ImageStim(win, image=str(stimuli_dir / 'winning.png'), size=(200, 200), pos=(250, 0))\nlosing = visual.ImageStim(win, image=str(stimuli_dir / 'loosing.png'), size=(200, 200), pos=(-250, 0))\n\n# Load sound \nwinning_sound = sound.Sound(str(stimuli_dir / 'winning.wav'))\nlosing_sound = sound.Sound(str(stimuli_dir / 'loosing.wav'))\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\n\nStart recording\nNow we are ready to look for the eye-trackers connected to the computer and select the first one that we find. Once we have selected it, we will launch our callback function to start collecting data.\n\n#%% Record the data\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\n#Start recording\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\n\n\nPresent our stimuli\nThe eye-tracking is running! Let’s show our participant something!\nAs you can see below, after each time we flip our window (remember: flipping means we actually show what we drew), we set the trigger variable to a string that identifies the specific stimulus we are presenting. This will be picked up our callback function.\n\n#%% Trials\n\n# Create an empty list we will append our data to\ngaze_data_buffer = []\n# Create trigger variable\ntrigger = ''\n\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n        \n    fixation.draw()\n    win.flip()\n    trigger = 'Fixation'\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    if trial ==0:\n        trigger = 'Circle'\n    else:\n        trigger = 'Square'\n\n    core.wait(3)  # wait for 3 seconds\n    win.flip()\n\n    ### Wait for saccadic latencty\n    core.wait(0.75)\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n    if trial ==0:\n        trigger = 'Reward'\n    else:\n        trigger = 'NoReward'\n    core.wait(2)  # wait for 1 second\n    win.flip()    # we re-flip at the end to clean the window\n    \n    ### ISI\n    clock = core.Clock()\n    while clock.getTime() &lt; 1:\n        pass\n    \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        core.quit()  # stop study\n\nAs we said before in Save the data, it is best to save the data during our study to avoid any potential data loss. And it is better to do this when there are things of minor interest, such as an ISI. If you remember, in the previous tutorial: Getting started with Psychopy, we created the ISI in a different way than just a clock.wait() and we said that this different method would come in handy later on. This is the moment!\nOur ISI starts the clock and checks when 1 second has passed after starting this clock. Why is this important? Because we can save the data after starting the clock. Since the time that it will take will be variable, we will be simply check how much time has passed after saving the data and wait (using the while clock.getTime() &lt; 1:  pass code) until 1 second has fully passed. This will ensure that we will wait for 1 second in total considering the saving of the data.\n\n### ISI\nclock = core.Clock()\nwrite_buffer_to_file(gaze_data_buffer, Path('DATA') / 'RAW' / (Sub + '.csv'))\nwhile clock.getTime() &lt; 1:\n    pass\n\n\n\n\n\n\n\nWarning\n\n\n\nCareful!!!\nIf saving the data takes more than 1 second, your ISI will also be longer. However, this should not be the case with typical studies where trials are not too long. Nonetheless, it’s always a good idea to keep an eye out.\n\n\n\n\nStop recording\nWe’re almost there! We have imported our functions, started collecting data, sent the triggers, and saved the data. The last step will be stop data collection (or python will keep getting an endless amount of data from the eye tracker!). Do do that, We simply unsubscribe from the eye tracker to which we had subscribed to start of data collection:\n\nwin.close()\nEyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\nNote that we also closed the Psychopy window, so that the stimulus presentation is also officially over. Well done!!! Now go and get your data!!! We’ll see you back when it’s time to analyze it.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/ContentList.html",
    "href": "CONTENT/ContentList.html",
    "title": "Content listing",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nStarting with Python\n\n\n\nSetup\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStarting with PsychoPy\n\n\n\nSetup\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCreate your first paradigm\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to eye-tracking\n\n\n\nEye-tracking\n\n\nTheory\n\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCreate an eye-tracking experiment\n\n\n\nEye-tracking\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing I2MC for robust fixation extraction\n\n\n\nEye-tracking\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFrom fixations to measures\n\n\n\nEye-tracking\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nSep 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCalibrating eye-tracking\n\n\n\nEye-tracking\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nOct 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPupil data pre-processing\n\n\n\nPupillometry\n\n\nR\n\n\nPre-processing\n\n\n\n\n\n\n\n\n\n\nDec 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPupil Data Analysis\n\n\n\nStats\n\n\nR\n\n\nModelling\n\n\nAdditive models\n\n\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Models\n\n\n\nStats\n\n\nR\n\n\nlinear models\n\n\n\n\n\n\n\n\n\n\nMay 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLinear mixed effect modesl\n\n\n\nR\n\n\nStats\n\n\nMixed effects models\n\n\n\n\n\n\n\n\n\n\nMar 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralised mixed-effect models\n\n\n\nR\n\n\nStats\n\n\nMixed effects models\n\n\n\n\n\n\n\n\n\n\nApr 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nModelEstimates\n\n\n\nStats\n\n\nR\n\n\nlinear models\n\n\n\n\n\n\n\nTommaso Ghilardi\n\n\nNov 3, 2026\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "CONTENT/EyeTracking/EyetrackingCalibration.html",
    "href": "CONTENT/EyeTracking/EyetrackingCalibration.html",
    "title": "Calibrating eye-tracking",
    "section": "",
    "text": "Before running an eye-tracking study, we usually need to do a calibration. What is a calibration you ask??\nWell, an eye-tracker works by using illuminators to emit near-infrared light towards the eye. This light reflects off the cornea, creating a visible glint or reflection that the eye-tracker’s sensors can detect. By capturing this reflection, the eye-tracker can determine the position of the eye relative to a screen, allowing it to pinpoint where the user is looking, known as the gaze point.\n\n\n\n\n\n\n\nImage from the tobii website\n\n\nThere are different ways to run a calibration using tobii eye-trackers. Here below the ones we have used:\n\n\nTobii offers a nice software called Tobii Pro, which allows you to run calibrations with considerable flexibility. However, Tobii Pro requires a paid license, which may not always be available to you. If you’re fortunate enough to have a paid license for Tobii Pro, by all means, take advantage of it. But if you don’t, don’t worry! Keep reading, as we have alternative solutions to share with you.\n\n\n\nAnother possibility is to use Tobii pro eye-tracker manager, an alternative software from Tobii. Unlike Tobii Pro, this application is free to use, though its features are more limited. If you’re looking to use Tobii software and don’t require advanced calibration options, this could be a suitable choice.\n\n\n\nFinally, if you are broke (like us!) and/or you want flexibility in running your calibration, one of the best option is to use the Tobii SDK that allows us to interact with the eyetrackers using Python. However, running a calibration using the SDK is pretty complex. We are trying to write a nice script to make it easier but in the meantime we have another solution…. We can use !! Psychopy tobii infants is a nice python code that wraps around the Tobii SDK making it easy to run a calibration, especailly an infant friendly one!! This code collection allows us to use the Tobii SDK with the Psychopy library.\nFinally, if you are broke and you want flexibility in running your calibration, one of the best options is to use the Tobii SDK. How we explained in previous tutorials (Create an eye-tracking experiment), this SDK allows the interaction with eye-trackers using Python. However, implementing a calibration using the SDK can be quite challenging……. We’re are trying to write an easy to explain and run code to perform calibrations, but in the meantime, we have an alternative solution!!\nEnter Psychopy_tobii_infant! This is a series of python functions that effectively wraps around the Tobii SDK, making it much easier to run calibrations, particularly those designed for infants. This code collection enables us to use the Tobii SDK in conjunction with the PsychoPy library, offering a more accessible approach to eye-tracking calibration!!\nIn the following paragraph we will see together how to prepare and use Psychopy tobii infant to run a nice and infant friendly calibration.\n\n\n\n\n\n\nNote\n\n\n\nUse Psychopy tobii infant to run studies\nOne question that you could ask yourself is… can I use Psychopy tobii infants to run my eye-tracking studies?? Well yes!! We have actually used it for few of our studies. Psychopy tobii infants provides an easy intarface between tobii eye-trackers and psychopy.\nOver the years we have personally move away from it but we often come back to it for some of its handy functionalities (e.g. the calibration).\n\n\n\n\nOk, let’s get started!! First thing first we need to download the codes of Psychopy tobii infant. You can find them here on GitHub: https://github.com/yh-luo/psychopy_tobii_infant.\nOn this page, you can click on &lt;&gt;Code and then on Download ZIP as shown below:\n\n\n\n\n\nPerfect!! Now that we have downloaded the code as a zip file we need to:\n\nextract the file\nidentify the folder “psychopy_tobii_infant”\ncopy this folder in the same location of your eye-tracking experiment script\n\nYou should end up like with something like this:\n\n\n\n\n\nNow we are all set and ready to go !!!!!!!!!!\n\n\n\nWe’ll now import the required libraries and custom functions. First, we’ll set the working directory to where our stimuli and the psychopy_tobii_infant folder are located. This allows Python to access these resources. Once set, we can easily import the necessary libraries and the custom functions from psychopy_tobii_infant for our eye-tracking experiment.\n\nimport os\nfrom psychopy import visual, sound\n\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\n\n# import Psychopy tobii infant\nfrom psychopy_tobii_infant import TobiiInfantController\n\nDone!! Now we can use its function to run our code.\n\n\n\nBefore running our calibration we need to prepare a few things!\nFirst, we create a nice window. As we explained before, this is the canvas where we will draw our stimuli.\n\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\nSecond we will import a few stimuli. I will explain later what we need them for, just trust me for now. If you want to follow along these are the stimuli we have used. Just download them and unzip them.\nOnce you have downloaded the stimuli we can use glob to find all the .png in the folder, read the video file and the audio file.\nThe .pngs are images of cute cartoon animals that we will use in the calibration.\n\n\n\n\n\nLet’s find them and put the in a list.\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\nPerfect we have found all the stimuli that we need.\nNow we will use the TobiiInfantController that we have imported before to connect to the eye-tracker. It is extremely simple:\n\nEyeTracker = TobiiInfantController(win)\n\nPerfect we now even have our eye-tracker!!\n\n\n\nThe first step for a proper calibration involves centering the participant’s eyes on the screen and ensuring their gaze is perpendicular to it. This positioning is crucial for accurate data collection.\nYou can see here two classic scenarios of infants in front of a eye-tracker. Again, the important thing is that the head and gaze of the child is perpendicular to the screen\n\n\n\n\n\nBut how to do it?? We can eyeball it but it’s not going to be easy. Luckily we can get some help!!\nUsing EyeTracker.show_status() we can see the eyes of the participant in relation to the screen. Therefore we can move either the eye-tracker or the infant to make sure the gaze is parallel to the screen. Sounds good doesn’t it??!!?? However, most of the times infants won’t look at the screen if nothing interesting is presented on it. Thus we need to capture their attention!!!!!\nWe can use the video we imported before. We set it to autodraw (each time the window is flipped a new frame will be drawn automatically) and the we start it by calling play().\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\nThis is what we will see:\n\n\n\n\n\nIn the center of the screen you will see the video we imported. In this case, a video of funny dancing fruits. But more importantly, at the top we can see the eyes of our participant. We need to make sure that we actually see the eyes and that they are centered in the screen. In addition is important to notice that under the eye position there is a green bar. The black line on it represents the distance of the participant head from the eye-tracker (the more on the right, the further from the screen; the more on the left, the closer to it). Ideally the black line should be in the center (overlapping the white thick line). This would indicate that the head of the participant is at an ideal distance (usually 65cm).\nOnce we are satisfied with the position of the eye-tracker and the infant we can press spacebar to proceed to the next step!! Before doing that we will stop the presentation of\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n\n\nOk, now that we are happy with the setup let’s actually run the calibration. It only takes two steps.\nFirst, we need to define the amount and the position of the points where we will present our stimuli. In infant studies we usually rely on 5 points. Four on the corners of the screen and one in the center.\nThe easiest way to define them is by using Psychopy’s normalized unit.\n\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\n\nAs we usually like to work in pixel units, we could define them in pixels or just transform our CALINORMP using the dimension of our window.\n\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nPerfect! Now we run our calibration by running\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound)\n\nOk, but what is going to happen??? After running this line we will start our calibration. Using the numbers on our keyboard (use 1~9 (depending on the number of calibration points) to present) we will show 1 of the .png stimuli that we have listed in CALISTIMS in the selected position. The presentation of the cartoon will be accompanied by the sound passed with Sound. The cartoon will start to zoom in and out in the attempt to capture the attention of the infant.\n\n\n\n\n\nOnce we are confident that the infant is looking at the cartoon zooming in and out you can press Space to collect the first calibration sample. Once that’s been done, we can move to the following points. Once all point are done we can press Enter. This will show the result of our calibration, e.g:\n\n\n\n\n\nThe red and green lines represent the gaze data from the left and right eyes, respectively. Each cluster of lines corresponds to a calibration point, with the central cluster indicating the main fixation point. Tight convergence of the lines within each cluster signifies high calibration accuracy and precision, while any significant discrepancies or spread indicate potential calibration issues.\nAs you may have noticed, the bottom right corner shows no data. What happened?? Well probably the infant was not looking when we pressed the Spacebar. How to fix it? We can simply press the number on the keyboard related to the point that failed and then press Spacebar. This will restart the calibration for only that point. So we can focus on getting more data for this point to add to our already collected.\nIn case more points are without data –or the data is just bad– we can select multiple points or even all of them (pressing 0). If we are happy with our calibration we can just press the Spacebar without selecting any points.\n\n\n\n\n\n\nTip\n\n\n\nThe match between the infant eye and the position on the screen is made in the exact moment you press the Space button. If you wait too long to press it, the infant might look away and beat you to it! It may appear counterintuitive, but we usually prefer to be quite fast in our calibration process. Since duration doesn’t matter but timing does, you will get better results! The fast switching seems to also capture infants’ attention better, meaning that they are more likely to follow the location of the cartoon with their gaze.\nRemember, infants can easily get bored with our stimuli. Thus, being rapid and sudden in showing them may work to our advantage.\n\n\nWell done!! Calibration is done!!!!\nNow we can start with our eye-tracking study!\nHere below the entire code.\n\nimport os\nfrom psychopy import visual, sound\n\n# import Psychopy tobii infant\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\nfrom psychopy_tobii_infant import TobiiInfantController\n\n\n#%% window and stimuli\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\n\n#%% Center face - screen\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n#%% Calibration\n\n# define calibration points\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound)\nwin.flip()",
    "crumbs": [
      "Eye-tracking",
      "Calibrating eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/EyetrackingCalibration.html#tobii-pro",
    "href": "CONTENT/EyeTracking/EyetrackingCalibration.html#tobii-pro",
    "title": "Calibrating eye-tracking",
    "section": "",
    "text": "Tobii offers a nice software called Tobii Pro, which allows you to run calibrations with considerable flexibility. However, Tobii Pro requires a paid license, which may not always be available to you. If you’re fortunate enough to have a paid license for Tobii Pro, by all means, take advantage of it. But if you don’t, don’t worry! Keep reading, as we have alternative solutions to share with you.",
    "crumbs": [
      "Eye-tracking",
      "Calibrating eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/EyetrackingCalibration.html#tobii-pro-eye-tracker-manager",
    "href": "CONTENT/EyeTracking/EyetrackingCalibration.html#tobii-pro-eye-tracker-manager",
    "title": "Calibrating eye-tracking",
    "section": "",
    "text": "Another possibility is to use Tobii pro eye-tracker manager, an alternative software from Tobii. Unlike Tobii Pro, this application is free to use, though its features are more limited. If you’re looking to use Tobii software and don’t require advanced calibration options, this could be a suitable choice.",
    "crumbs": [
      "Eye-tracking",
      "Calibrating eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/EyetrackingCalibration.html#psychopy-tobii-infant",
    "href": "CONTENT/EyeTracking/EyetrackingCalibration.html#psychopy-tobii-infant",
    "title": "Calibrating eye-tracking",
    "section": "",
    "text": "Finally, if you are broke (like us!) and/or you want flexibility in running your calibration, one of the best option is to use the Tobii SDK that allows us to interact with the eyetrackers using Python. However, running a calibration using the SDK is pretty complex. We are trying to write a nice script to make it easier but in the meantime we have another solution…. We can use !! Psychopy tobii infants is a nice python code that wraps around the Tobii SDK making it easy to run a calibration, especailly an infant friendly one!! This code collection allows us to use the Tobii SDK with the Psychopy library.\nFinally, if you are broke and you want flexibility in running your calibration, one of the best options is to use the Tobii SDK. How we explained in previous tutorials (Create an eye-tracking experiment), this SDK allows the interaction with eye-trackers using Python. However, implementing a calibration using the SDK can be quite challenging……. We’re are trying to write an easy to explain and run code to perform calibrations, but in the meantime, we have an alternative solution!!\nEnter Psychopy_tobii_infant! This is a series of python functions that effectively wraps around the Tobii SDK, making it much easier to run calibrations, particularly those designed for infants. This code collection enables us to use the Tobii SDK in conjunction with the PsychoPy library, offering a more accessible approach to eye-tracking calibration!!\nIn the following paragraph we will see together how to prepare and use Psychopy tobii infant to run a nice and infant friendly calibration.\n\n\n\n\n\n\nNote\n\n\n\nUse Psychopy tobii infant to run studies\nOne question that you could ask yourself is… can I use Psychopy tobii infants to run my eye-tracking studies?? Well yes!! We have actually used it for few of our studies. Psychopy tobii infants provides an easy intarface between tobii eye-trackers and psychopy.\nOver the years we have personally move away from it but we often come back to it for some of its handy functionalities (e.g. the calibration).\n\n\n\n\nOk, let’s get started!! First thing first we need to download the codes of Psychopy tobii infant. You can find them here on GitHub: https://github.com/yh-luo/psychopy_tobii_infant.\nOn this page, you can click on &lt;&gt;Code and then on Download ZIP as shown below:\n\n\n\n\n\nPerfect!! Now that we have downloaded the code as a zip file we need to:\n\nextract the file\nidentify the folder “psychopy_tobii_infant”\ncopy this folder in the same location of your eye-tracking experiment script\n\nYou should end up like with something like this:\n\n\n\n\n\nNow we are all set and ready to go !!!!!!!!!!\n\n\n\nWe’ll now import the required libraries and custom functions. First, we’ll set the working directory to where our stimuli and the psychopy_tobii_infant folder are located. This allows Python to access these resources. Once set, we can easily import the necessary libraries and the custom functions from psychopy_tobii_infant for our eye-tracking experiment.\n\nimport os\nfrom psychopy import visual, sound\n\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\n\n# import Psychopy tobii infant\nfrom psychopy_tobii_infant import TobiiInfantController\n\nDone!! Now we can use its function to run our code.\n\n\n\nBefore running our calibration we need to prepare a few things!\nFirst, we create a nice window. As we explained before, this is the canvas where we will draw our stimuli.\n\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\nSecond we will import a few stimuli. I will explain later what we need them for, just trust me for now. If you want to follow along these are the stimuli we have used. Just download them and unzip them.\nOnce you have downloaded the stimuli we can use glob to find all the .png in the folder, read the video file and the audio file.\nThe .pngs are images of cute cartoon animals that we will use in the calibration.\n\n\n\n\n\nLet’s find them and put the in a list.\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\nPerfect we have found all the stimuli that we need.\nNow we will use the TobiiInfantController that we have imported before to connect to the eye-tracker. It is extremely simple:\n\nEyeTracker = TobiiInfantController(win)\n\nPerfect we now even have our eye-tracker!!\n\n\n\nThe first step for a proper calibration involves centering the participant’s eyes on the screen and ensuring their gaze is perpendicular to it. This positioning is crucial for accurate data collection.\nYou can see here two classic scenarios of infants in front of a eye-tracker. Again, the important thing is that the head and gaze of the child is perpendicular to the screen\n\n\n\n\n\nBut how to do it?? We can eyeball it but it’s not going to be easy. Luckily we can get some help!!\nUsing EyeTracker.show_status() we can see the eyes of the participant in relation to the screen. Therefore we can move either the eye-tracker or the infant to make sure the gaze is parallel to the screen. Sounds good doesn’t it??!!?? However, most of the times infants won’t look at the screen if nothing interesting is presented on it. Thus we need to capture their attention!!!!!\nWe can use the video we imported before. We set it to autodraw (each time the window is flipped a new frame will be drawn automatically) and the we start it by calling play().\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\nThis is what we will see:\n\n\n\n\n\nIn the center of the screen you will see the video we imported. In this case, a video of funny dancing fruits. But more importantly, at the top we can see the eyes of our participant. We need to make sure that we actually see the eyes and that they are centered in the screen. In addition is important to notice that under the eye position there is a green bar. The black line on it represents the distance of the participant head from the eye-tracker (the more on the right, the further from the screen; the more on the left, the closer to it). Ideally the black line should be in the center (overlapping the white thick line). This would indicate that the head of the participant is at an ideal distance (usually 65cm).\nOnce we are satisfied with the position of the eye-tracker and the infant we can press spacebar to proceed to the next step!! Before doing that we will stop the presentation of\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n\n\nOk, now that we are happy with the setup let’s actually run the calibration. It only takes two steps.\nFirst, we need to define the amount and the position of the points where we will present our stimuli. In infant studies we usually rely on 5 points. Four on the corners of the screen and one in the center.\nThe easiest way to define them is by using Psychopy’s normalized unit.\n\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\n\nAs we usually like to work in pixel units, we could define them in pixels or just transform our CALINORMP using the dimension of our window.\n\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nPerfect! Now we run our calibration by running\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound)\n\nOk, but what is going to happen??? After running this line we will start our calibration. Using the numbers on our keyboard (use 1~9 (depending on the number of calibration points) to present) we will show 1 of the .png stimuli that we have listed in CALISTIMS in the selected position. The presentation of the cartoon will be accompanied by the sound passed with Sound. The cartoon will start to zoom in and out in the attempt to capture the attention of the infant.\n\n\n\n\n\nOnce we are confident that the infant is looking at the cartoon zooming in and out you can press Space to collect the first calibration sample. Once that’s been done, we can move to the following points. Once all point are done we can press Enter. This will show the result of our calibration, e.g:\n\n\n\n\n\nThe red and green lines represent the gaze data from the left and right eyes, respectively. Each cluster of lines corresponds to a calibration point, with the central cluster indicating the main fixation point. Tight convergence of the lines within each cluster signifies high calibration accuracy and precision, while any significant discrepancies or spread indicate potential calibration issues.\nAs you may have noticed, the bottom right corner shows no data. What happened?? Well probably the infant was not looking when we pressed the Spacebar. How to fix it? We can simply press the number on the keyboard related to the point that failed and then press Spacebar. This will restart the calibration for only that point. So we can focus on getting more data for this point to add to our already collected.\nIn case more points are without data –or the data is just bad– we can select multiple points or even all of them (pressing 0). If we are happy with our calibration we can just press the Spacebar without selecting any points.\n\n\n\n\n\n\nTip\n\n\n\nThe match between the infant eye and the position on the screen is made in the exact moment you press the Space button. If you wait too long to press it, the infant might look away and beat you to it! It may appear counterintuitive, but we usually prefer to be quite fast in our calibration process. Since duration doesn’t matter but timing does, you will get better results! The fast switching seems to also capture infants’ attention better, meaning that they are more likely to follow the location of the cartoon with their gaze.\nRemember, infants can easily get bored with our stimuli. Thus, being rapid and sudden in showing them may work to our advantage.\n\n\nWell done!! Calibration is done!!!!\nNow we can start with our eye-tracking study!\nHere below the entire code.\n\nimport os\nfrom psychopy import visual, sound\n\n# import Psychopy tobii infant\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\nfrom psychopy_tobii_infant import TobiiInfantController\n\n\n#%% window and stimuli\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\n\n#%% Center face - screen\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n#%% Calibration\n\n# define calibration points\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound)\nwin.flip()",
    "crumbs": [
      "Eye-tracking",
      "Calibrating eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html",
    "title": "Using I2MC for robust fixation extraction",
    "section": "",
    "text": "When it comes to eye-tracking data, a fundamental role is played by fixations. A fixation indicates that a person’s eyes are looking at a particular point of interest for a given amount of time. More specifically, a fixation is a cluster of consecutive data points in an eye-tracking dataset for which a person’s gaze remains relatively still and focused on a particular area or object.\nTypically, eye-tracking programs come with their own fixation detection algorithms that give us a rough idea of what the person was looking at. But here’s the problem: these tools aren’t always very good when it comes to data from infants and children. Why? Because infants and children can be all over the place! They move their heads, put their hands (or even feet) in front of their faces, close their eyes, or just look away. All of this makes the data a big mess that’s hard to make sense of with regular fixation detection algorithms. Because the data is so messy, it is difficult to tell which data points are part of the same fixation or different fixations.\nBut don’t worry! We’ve got a solution: I2MC.\nI2MC stands for “Identification by Two-Means Clustering”, and it was designed specifically for this kind of problem. It’s designed to deal with all kinds of noise, and even periods of data loss. In this tutorial, we’ll show you how to use I2MC to find fixations. We won’t get into the nerdy stuff about how it works - this is all about the practical side. If you’re curious about the science, you can read the original article.\nNow that we’ve introduced I2MC, let’s get our hands dirty and see how to use it!",
    "crumbs": [
      "Eye-tracking",
      "Using I2MC for robust fixation extraction"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html#import-data",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html#import-data",
    "title": "Using I2MC for robust fixation extraction",
    "section": "Import data",
    "text": "Import data\nNow we will write a simple function to import our data. This step unfortunately will have to be adapted depending on the system you used to collect the data and the data structure you will have in the end. For this tutorial, you can use your data-set (probably you will have to adapt the importing function) or use our data that you can download from here.\nLet’s create step by step our function to import the data\n\n# Load data\nraw_df = pd.read_csv(PATH_TO_DATA, delimiter=',')\n\nAfter reading the data we will create a new data-frame that we will fill with the information needed from our raw_df. this is the point that would change depending on you eye-tracked and data format. you will probably have to change the columns names to be sure to have the 5 relevant ones.\n\n# Create empty dataframe\ndf = pd.DataFrame()\n    \n# Extract required data\ndf['time'] = raw_df['time']\ndf['L_X'] = raw_df['L_X']\ndf['L_Y'] = raw_df['L_Y']\ndf['R_X'] = raw_df['R_X']\ndf['R_Y'] = raw_df['R_Y']\n\nAfter selecting the relevant data we will perform some very basic processing. Sometimes there could be weird peaks where one sample is (very) far outside the monitor. Here, we will count as missing any data that is more than one monitor distance outside the monitor. Tobii gives us the validity index of the measured eye, here when the validity is too low (&gt;1) we will consider the sample as missing\n\n###\n# Sometimes we have weird peaks where one sample is (very) far outside the\n# monitor. Here, count as missing any data that is more than one monitor\n# distance outside the monitor.\n\n# Left eye\nlMiss1 = (df['L_X'] &lt; -res[0]) | (df['L_X']&gt;2*res[0])\nlMiss2 = (df['L_Y'] &lt; -res[1]) | (df['L_Y']&gt;2*res[1])\nlMiss  = lMiss1 | lMiss2 | (raw_df['L_V'] == False)\ndf.loc[lMiss,'L_X'] = np.nan\ndf.loc[lMiss,'L_Y'] = np.nan\n\n# Right eye\nrMiss1 = (df['R_X'] &lt; -res[0]) | (df['R_X']&gt;2*res[0])\nrMiss2 = (df['R_Y'] &lt; -res[1]) | (df['R_Y']&gt;2*res[1])\nrMiss  = rMiss1 | rMiss2 | (raw_df['R_V'] == False)\ndf.loc[rMiss,'R_X'] = np.nan\ndf.loc[rMiss,'R_Y'] = np.nan\n\nPerfect!!!\n\nEverything into a function\nWe have read the data, extracted the relevant information and done some extremely basic processing rejecting data that had to be considered non valid. Now we will wrap this code in a function to make it easier to use with I2MC:\n\n# ===================================================================\n# Import data from Tobii TX300\n# ===================================================================\n\ndef tobii_TX300(fname, res=[1920,1080]):\n\n    # Load all data\n    raw_df = pd.read_csv(fname, delimiter=',')\n    df = pd.DataFrame()\n    \n    # Extract required data\n    df['time'] = raw_df['time']\n    df['L_X'] = raw_df['L_X']\n    df['L_Y'] = raw_df['L_Y']\n    df['R_X'] = raw_df['R_X']\n    df['R_Y'] = raw_df['R_Y']\n    \n    \n    ###\n    # Sometimes we have weird peaks where one sample is (very) far outside the\n    # monitor. Here, count as missing any data that is more than one monitor\n    # distance outside the monitor.\n    \n    # Left eye\n    lMiss1 = (df['L_X'] &lt; -res[0]) | (df['L_X']&gt;2*res[0])\n    lMiss2 = (df['L_Y'] &lt; -res[1]) | (df['L_Y']&gt;2*res[1])\n    lMiss  = lMiss1 | lMiss2 | (raw_df['L_V'] == False)\n    df.loc[lMiss,'L_X'] = np.nan\n    df.loc[lMiss,'L_Y'] = np.nan\n    \n    # Right eye\n    rMiss1 = (df['R_X'] &lt; -res[0]) | (df['R_X']&gt;2*res[0])\n    rMiss2 = (df['R_Y'] &lt; -res[1]) | (df['R_Y']&gt;2*res[1])\n    rMiss  = rMiss1 | rMiss2 | (raw_df['R_V'] == False)\n    df.loc[rMiss,'R_X'] = np.nan\n    df.loc[rMiss,'R_Y'] = np.nan\n    \n    return(df)\n\n\n\nFind our data\nNice!! we have our import function that we will use to read our data. Now, let’s find our data! To do this, we will use the glob library, which is a handy tool for finding files in Python. Before that let’s set our working directory. The working directory is the folder where we have all our scripts and data. We can set it using the os library:\n\nimport os\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD\\Workshop')\n\nThis is my directory, you will have something different, you need to change it to where your data are. Once you are done with that we can use glob to find our data files. In the code below, we are telling Python to look for files with a .csv extension in a specific folder on our computer:\n\nfrom pathlib import Path\ndata_files = list(Path().glob('DATA/RAW/**/*.csv'))\n\n\nDATA\\\\RAW\\\\: This is the path where we want to start our search.\n**: This special symbol tells Python to search in all the subfolders (folders within folders) under our starting path.\n*.csv: We’re asking Python to look for files with names ending in “.csv”.\n\nSo, when we run this code, Python will find and give us a list of all the “.csv” files located in any subfolder within our specified path. This makes it really convenient to find and work with lots of files at once.\n\n\nDefine the output folder\nBefore doing anything else, I would suggest creating a folder where we will save the output of I2MC. We will create a folder called i2mc_output. Using pathlib, we can create this directory in a single elegant step:\n\nfrom pathlib import Path\n\n# define the output folder\noutput_folder = Path('DATA') / 'i2mc_output'  # define folder path\\name\n\n# Create the folder (will do nothing if it already exists)\noutput_folder.mkdir(parents=True, exist_ok=True)\n\nThe mkdir() method creates the directory, with two helpful parameters: parents=True ensures that any parent directories are created if they don’t exist yet, and exist_ok=True means the function won’t raise an error if the directory already exists - it will simply do nothing in that case. This approach eliminates the need to check if the directory exists before creating it.\n\n\nI2MC settings\nNow that we’ve got our data, know how to import it using glob and we have out output folder, we’re all set to run I2MC. But wait, before we dive in, we need to set up a few things. These settings are like the instructions we give to I2MC before it does its magic. The default settings usually work just fine for most situations. You can keep them as they are and proceed. If you’re curious about what each of these settings does, you can explore the original I2MC article for a detailed understanding. Here I’ve added a general explanation about what each setting does. Once you’ve read through the instructions and have a clear understanding, you can customize the settings to match your specific requirements.\nLet’s define these settings:\n\n# =============================================================================\n# NECESSARY VARIABLES\n\nopt = {}\n# General variables for eye-tracking data\nopt['xres']         = 1920.0                # maximum value of horizontal resolution in pixels\nopt['yres']         = 1080.0                # maximum value of vertical resolution in pixels\nopt['missingx']     = np.nan          # missing value for horizontal position in eye-tracking data (example data uses -xres). used throughout the algorithm as signal for data loss\nopt['missingy']     = np.nan          # missing value for vertical position in eye-tracking data (example data uses -yres). used throughout algorithm as signal for data loss\nopt['freq']         = 300.0                 # sampling frequency of data (check that this value matches with values actually obtained from measurement!)\n\n# Variables for the calculation of visual angle\n# These values are used to calculate noise measures (RMS and BCEA) of\n# fixations. The may be left as is, but don't use the noise measures then.\n# If either or both are empty, the noise measures are provided in pixels\n# instead of degrees.\nopt['scrSz']        = [50.9174, 28.6411]    # screen size in cm\nopt['disttoscreen'] = 65.0                  # distance to screen in cm.\n\n# Options of example script\ndo_plot_data = True # if set to True, plot of fixation detection for each trial will be saved as a png file in the output folder.\n# the figures works best for short trials (up to around 20 seconds)\n\n# =============================================================================\n# OPTIONAL VARIABLES\n# The settings below may be used to adopt the default settings of the\n# algorithm. Do this only if you know what you're doing.\n\n# # STEFFEN INTERPOLATION\nopt['windowtimeInterp']     = 0.1                           # max duration (s) of missing values for interpolation to occur\nopt['edgeSampInterp']       = 2                             # amount of data (number of samples) at edges needed for interpolation\nopt['maxdisp']              = opt['xres']*0.2*np.sqrt(2)    # maximum displacement during missing for interpolation to be possible\n\n# # K-MEANS CLUSTERING\nopt['windowtime']           = 0.2                           # time window (s) over which to calculate 2-means clustering (choose value so that max. 1 saccade can occur)\nopt['steptime']             = 0.02                          # time window shift (s) for each iteration. Use zero for sample by sample processing\nopt['maxerrors']            = 100                           # maximum number of errors allowed in k-means clustering procedure before proceeding to next file\nopt['downsamples']          = [2, 5, 10]\nopt['downsampFilter']       = False                         # use chebychev filter when downsampling? Its what matlab's downsampling functions do, but could cause trouble (ringing) with the hard edges in eye-movement data\n\n# # FIXATION DETERMINATION\nopt['cutoffstd']            = 2.0                           # number of standard deviations above mean k-means weights will be used as fixation cutoff\nopt['onoffsetThresh']       = 3.0                           # number of MAD away from median fixation duration. Will be used to walk forward at fixation starts and backward at fixation ends to refine their placement and stop the algorithm from eating into saccades\nopt['maxMergeDist']         = 30.0                          # maximum Euclidean distance in pixels between fixations for merging\nopt['maxMergeTime']         = 30.0                          # maximum time in ms between fixations for merging\nopt['minFixDur']            = 40.0                          # minimum fixation duration after merging, fixations with shorter duration are removed from output\n\n\n\nRun I2MC\nNow we can finally run I2MC on all our files. To do so we will make for loop that will iterate between all the files and: - create a folder for each subject - import the data with the function created before - run I2MC on the file - save the output file and the plot.\n\n#%% Run I2MC\nfor file_idx, file in enumerate(data_files):\n    print('Processing file {} of {}'.format(file_idx + 1, len(data_files)))\n    \n    # Extract the name from the file path\n    name = file.stem\n    \n    # Create the folder for the specific subject\n    subj_folder = output_folder / name\n    subj_folder.mkdir(exist_ok=True)\n       \n    # Import data\n    data = tobii_TX300(file, [opt['xres'], opt['yres']])\n    \n    # Run I2MC on the data\n    fix,_,_ = I2MC.I2MC(data,opt)\n    \n    ## Create a plot of the result and save them\n    if do_plot_data:\n        # pre-allocate name for saving file\n        save_plot = subj_folder / f\"{name}.png\"\n        f = I2MC.plot.data_and_fixations(data, fix, fix_as_line=True, res=[opt['xres'], opt['yres']])\n        # save figure and close\n        f.savefig(save_plot)\n        plt.close(f)\n        \n    # Write data to file after make it a dataframe\n    fix['participant'] = name\n    fix_df = pd.DataFrame(fix)\n    save_file = subj_folder / f\"{name}.csv\"\n    fix_df.to_csv(save_file)",
    "crumbs": [
      "Eye-tracking",
      "Using I2MC for robust fixation extraction"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html#we-are-done",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html#we-are-done",
    "title": "Using I2MC for robust fixation extraction",
    "section": "WE ARE DONE!!!!!",
    "text": "WE ARE DONE!!!!!\nCongratulations on reaching this point! By now, you should have new files containing valuable information from I2MC.\nBut what exactly does I2MC tell us?\nI2MC provides us with a data frame that contains various pieces of information:\nWhat I2MC Returns:\n\ncutoff: A number representing the cutoff used for fixation detection.\nstart: An array holding the indices where fixations start.\nend: An array holding the indices where fixations end.\nstartT: An array containing the times when fixations start.\nendT: An array containing the times when fixations end.\ndur: An array storing the durations of fixations.\nxpos: An array representing the median horizontal position for each fixation in the trial.\nypos: An array representing the median vertical position for each fixation in the trial.\nflankdataloss: A boolean value (1 or 0) indicating whether a fixation is flanked by data loss (1) or not (0).\nfracinterped: A fraction that tells us the amount of data loss or interpolated data in the fixation data.\n\nIn simple terms, I2MC helps us understand where and for how long a person’s gaze remains fixed during an eye-tracking experiment. This is just the first step. Now that we have our fixations, we’ll need to use them to extract the information we’re interested in. Typically, this involves using the raw data to understand what was happening at each specific time point and using the data from I2MC to determine where the participant was looking at that time. This will be covered in a new tutorial. For now, you’ve successfully completed the preprocessing of your eye-tracking data, extracting a robust estimation of participants’ fixations!!\n\n\n\n\n\n\nWarning\n\n\n\nCaution: This tutorial is simplified and assumes the following:\n\nEach participant has only one file (1 trial).\nAll files contain data.\nThe data is relatively clean (I2MC won’t throw any errors).\nAnd so on.\n\nIf your data doesn’t match these assumptions, you may need to modify the script to handle any discrepancies.\nFor a more comprehensive example and in-depth usage, check out the I2MC repository. It provides a more complete example with data checks for missing data and potential errors. Now that you’ve understood the basics here, interpreting that example should be much easier. If you encounter any issues while running the script, you can give that example a try or reach out to us via email!!!",
    "crumbs": [
      "Eye-tracking",
      "Using I2MC for robust fixation extraction"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html#entire-script",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html#entire-script",
    "title": "Using I2MC for robust fixation extraction",
    "section": "Entire script",
    "text": "Entire script\nTo make it simple here is the entire script that we wrote together!!!\n\nimport os\nfrom pathlib import Path\n\nimport I2MC\nimport pandas as pd\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# =============================================================================\n# Import data from Tobii TX300\n# =============================================================================\n\ndef tobii_TX300(fname, res=[1920,1080]):\n\n    # Load all data\n    raw_df = pd.read_csv(fname, delimiter=',')\n    df = pd.DataFrame()\n    \n    # Extract required data\n    df['time'] = raw_df['time']\n    df['L_X'] = raw_df['L_X']\n    df['L_Y'] = raw_df['L_Y']\n    df['R_X'] = raw_df['R_X']\n    df['R_Y'] = raw_df['R_Y']\n    \n    \n    ###\n    # Sometimes we have weird peaks where one sample is (very) far outside the\n    # monitor. Here, count as missing any data that is more than one monitor\n    # distance outside the monitor.\n    \n    # Left eye\n    lMiss1 = (df['L_X'] &lt; -res[0]) | (df['L_X']&gt;2*res[0])\n    lMiss2 = (df['L_Y'] &lt; -res[1]) | (df['L_Y']&gt;2*res[1])\n    lMiss  = lMiss1 | lMiss2 | (raw_df['L_V'] == False)\n    df.loc[lMiss,'L_X'] = np.nan\n    df.loc[lMiss,'L_Y'] = np.nan\n    \n    # Right eye\n    rMiss1 = (df['R_X'] &lt; -res[0]) | (df['R_X']&gt;2*res[0])\n    rMiss2 = (df['R_Y'] &lt; -res[1]) | (df['R_Y']&gt;2*res[1])\n    rMiss  = rMiss1 | rMiss2 | (raw_df['R_V'] == False)\n    df.loc[rMiss,'R_X'] = np.nan\n    df.loc[rMiss,'R_Y'] = np.nan\n\n    return(df)\n\n\n\n#%% Preparation\n\n# Settign the working directory\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD\\Workshop')\n\n# Find the files\ndata_files = list(Path().glob('DATA/RAW/**/*.csv'))\n\n# define the output folder\noutput_folder = Path('DATA') / 'i2mc_output'  # define folder path\\name\n\n# Create the folder (will do nothing if it already exists)\noutput_folder.mkdir(parents=True, exist_ok=True)\n\n\n# =============================================================================\n# NECESSARY VARIABLES\n\nopt = {}\n# General variables for eye-tracking data\nopt['xres']         = 1920.0                # maximum value of horizontal resolution in pixels\nopt['yres']         = 1080.0                # maximum value of vertical resolution in pixels\nopt['missingx']     = np.nan                # missing value for horizontal position in eye-tracking data (example data uses -xres). used throughout the algorithm as signal for data loss\nopt['missingy']     = np.nan                # missing value for vertical position in eye-tracking data (example data uses -yres). used throughout algorithm as signal for data loss\nopt['freq']         = 300.0                 # sampling frequency of data (check that this value matches with values actually obtained from measurement!)\n\n# Variables for the calculation of visual angle\n# These values are used to calculate noise measures (RMS and BCEA) of\n# fixations. The may be left as is, but don't use the noise measures then.\n# If either or both are empty, the noise measures are provided in pixels\n# instead of degrees.\nopt['scrSz']        = [50.9174, 28.6411]    # screen size in cm\nopt['disttoscreen'] = 65.0                  # distance to screen in cm.\n\n# Options of example script\ndo_plot_data = True # if set to True, plot of fixation detection for each trial will be saved as png-file in output folder.\n# the figures works best for short trials (up to around 20 seconds)\n\n# =============================================================================\n# OPTIONAL VARIABLES\n# The settings below may be used to adopt the default settings of the\n# algorithm. Do this only if you know what you're doing.\n\n# # STEFFEN INTERPOLATION\nopt['windowtimeInterp']     = 0.1                           # max duration (s) of missing values for interpolation to occur\nopt['edgeSampInterp']       = 2                             # amount of data (number of samples) at edges needed for interpolation\nopt['maxdisp']              = opt['xres']*0.2*np.sqrt(2)    # maximum displacement during missing for interpolation to be possible\n\n# # K-MEANS CLUSTERING\nopt['windowtime']           = 0.2                           # time window (s) over which to calculate 2-means clustering (choose value so that max. 1 saccade can occur)\nopt['steptime']             = 0.02                          # time window shift (s) for each iteration. Use zero for sample by sample processing\nopt['maxerrors']            = 100                           # maximum number of errors allowed in k-means clustering procedure before proceeding to next file\nopt['downsamples']          = [2, 5, 10]\nopt['downsampFilter']       = False                         # use chebychev filter when downsampling? Its what matlab's downsampling functions do, but could cause trouble (ringing) with the hard edges in eye-movement data\n\n# # FIXATION DETERMINATION\nopt['cutoffstd']            = 2.0                           # number of standard deviations above mean k-means weights will be used as fixation cutoff\nopt['onoffsetThresh']       = 3.0                           # number of MAD away from median fixation duration. Will be used to walk forward at fixation starts and backward at fixation ends to refine their placement and stop algorithm from eating into saccades\nopt['maxMergeDist']         = 30.0                          # maximum Euclidean distance in pixels between fixations for merging\nopt['maxMergeTime']         = 30.0                          # maximum time in ms between fixations for merging\nopt['minFixDur']            = 40.0                          # minimum fixation duration after merging, fixations with shorter duration are removed from output\n\n\n\n#%% Run I2MC\n\nfor file_idx, file in enumerate(data_files):\n    print('Processing file {} of {}'.format(file_idx + 1, len(data_files)))\n\n    # Extract the name form the file path\n    name = file.stem    \n    \n    # Create the folder for the specific subject\n    subj_folder = output_folder / name\n    subj_folder.mkdir(exist_ok=True)\n       \n    # Import data\n    data = tobii_TX300(file, [opt['xres'], opt['yres']])\n\n    # Run I2MC on the data\n    fix,_,_ = I2MC.I2MC(data,opt)\n\n    ## Create a plot of the result and save them\n    if do_plot_data:\n        # pre-allocate name for saving file\n        save_plot = subj_folder / f\"{name}.png\"\n        f = I2MC.plot.data_and_fixations(data, fix, fix_as_line=True, res=[opt['xres'], opt['yres']])\n        # save figure and close\n        f.savefig(save_plot)\n        plt.close(f)\n\n    # Write data to file after make it a dataframe\n    fix['participant'] = name\n    fix_df = pd.DataFrame(fix)\n    save_file = subj_folder / f\"{name}.csv\"\n    fix_df.to_csv(save_file)",
    "crumbs": [
      "Eye-tracking",
      "Using I2MC for robust fixation extraction"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilDataAnalysis.html",
    "href": "CONTENT/EyeTracking/PupilDataAnalysis.html",
    "title": "Pupil Data Analysis",
    "section": "",
    "text": "If you have collected and pre-processed your pupil data, the long awaited moment arrived: It’s finally time to analyse your data and get your results!!!\nIn this tutorial we will do two types of analysis. The first one is more simple, and the second is more advanced. For some research questions, simple analyses are enough: they are intuitive and easy to understand. However, pupil data is actually very rich and complex, and more sophisticated analyses can sometimes help to really get the most out of your data and let them shine!\nBefore starting any type of analysis, let’s import the data and take a quick look at it.",
    "crumbs": [
      "Eye-tracking",
      "Pupil Data Analysis"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilDataAnalysis.html#import-data",
    "href": "CONTENT/EyeTracking/PupilDataAnalysis.html#import-data",
    "title": "Pupil Data Analysis",
    "section": "Import data",
    "text": "Import data\nThe data used in this tutorial comes from the pre-processing tutorial of pupil dilation. If you haven’t run that tutorial yet, it’s a good idea to check it out first to ensure your data is prepared and ready for analysis: Pre-processing pupil data. In case you did not save the result of the pre-processing you can download them from here :\n Preprocessed_PupilData.csv \nNow that you have the data, let’s import it along with the necessary libraries. We’ll also ensure that the Event and Subjects columns are properly set as factors (categorical for easier analysis. Here’s how:\n\nlibrary(tidyverse)  # Data manipulation and visualization\nlibrary(easystats)  # Easy statistical modeling\n\ndata = read.csv(\"..\\\\..\\\\resources\\\\Pupillometry\\\\Processed\\\\Processed_PupilData.csv\")\n\n# Make sure Event and Subject are factors\ndata$Event = as.factor(data$Event)\ndata$Subject = as.factor(data$Subject)\n\nhead(data)\n\n  X   Subject  Event TrialN mean_pupil time\n1 1 Subject_1 Square      2 0.04125765    0\n2 2 Subject_1 Square      2 0.10143081   50\n3 3 Subject_1 Square      2 0.12862906  100\n4 4 Subject_1 Square      2 0.16747935  150\n5 5 Subject_1 Square      2 0.21956792  200\n6 6 Subject_1 Square      2 0.27146304  250\n\n\nFor a detailed description of the data, you can have a look at the tutorial on preprocessing pupil data. The key variables to focus on here are the following:\n\nmean_pupil indicates what the pupil size was at every moment in time (every 50 milliseconds, 20Hz). This is our dependent variable.\ntime indicates the specific moment in time within each trial\nTrialN indicates the trial number\nEvent indicates whether the trial contained a circle (followed by a reward) or a square (not followed by a reward). This variable is not numerical, but categorical. We thus set it to factor with as.factor().\nSubject contains a different ID number for each subject. This is also a categorical variable.",
    "crumbs": [
      "Eye-tracking",
      "Pupil Data Analysis"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilDataAnalysis.html#comparing-means",
    "href": "CONTENT/EyeTracking/PupilDataAnalysis.html#comparing-means",
    "title": "Pupil Data Analysis",
    "section": "Comparing means",
    "text": "Comparing means\nIn many paradigms, you have two or more conditions and want to test whether your dependent variable (pupil size in this case!) is significantly different across conditions. In our example paradigm, we may want to test whether, on average, pupil size while looking at the rewarding cue (the circle) is greater than pupil size while looking at the non-rewarding cue (the square). This would mean that even before the reward is presented, infants have learned that a reward will be coming and dilate their pupils in response to it! Pretty cool, uh?\nIf we want to test multiple groups, we can use a t-test, an ANOVA or… A linear model! Here, we’ll be using a special type of linear model, a mixed-effect model - which is infinitely better for many many reasons [add link].\nAdapt the data\nWe want to compare the means across conditions but… We don’t have means yet! We have a much richer dataset, that contains hundreds of datapoints with milliseconds precision. For this first simple analysis, we just want one average measure of pupil dilation for each trial instead. We can compute this using the tidyverse library (that is container of multiple packages) a powerful collection of packages for wrangling and visualuzating dataframes in R.\nHere, we group the data by Subject, Event, and TrialN, then summarize it within these groups by calculating the mean values.\n\naveraged_data = data %&gt;%\n  group_by(Subject, Event, TrialN) %&gt;%\n  summarise(mean_pupil = mean(mean_pupil, na.rm = TRUE))\n\nhead(averaged_data)\n\n# A tibble: 6 × 4\n# Groups:   Subject, Event [2]\n  Subject   Event  TrialN mean_pupil\n  &lt;fct&gt;     &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 Subject_1 Circle      3     0.0397\n2 Subject_1 Circle      4     0.281 \n3 Subject_1 Circle      6     0.0559\n4 Subject_1 Circle      9    -0.0393\n5 Subject_1 Square      2    -0.139 \n6 Subject_1 Square      5     0.0420\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this step, we used the average to calculate an index for each trial, meaning we averaged the pupil dilation over the trial duration. However, this is not the only option. Other approaches include extracting the peak value (max(mean_pupil, na.rm = TRUE)) or calculating the sum of the signal (sum(mean_pupil, na.rm = TRUE)), which can also represent the area under the curve (AUC) for the trial.\n\n\nLinear mixed-effects model\nWith a single value for each participant, condition, and trial (averaged across time points), we are now ready to proceed with our analysis. Even if the word “Linear mixed-effect model” might sound scary, the model is actually very simple. We take our experimental conditions (Event) and check whether they affect pupil size (mean_pupil). To account for individual differences in pupil response intensity, we include participants as a random intercept.\nLet’s give it a go!!!\n\nlibrary(lmerTest)   # Mixed-effect models library\n\n# The actual model\nmodel_avg = lmer(mean_pupil ~ Event + (1|Subject), data = averaged_data)\n\nsummary(model_avg) # summary of the model\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ Event + (1 | Subject)\n   Data: averaged_data\n\nREML criterion at convergence: 17.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.84767 -0.66407  0.02253  0.65595  2.02017 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n Subject  (Intercept) 0.0008353 0.0289  \n Residual             0.0709333 0.2663  \nNumber of obs: 55, groups:  Subject, 6\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)   \n(Intercept)  0.10770    0.05263 17.80244   2.046   0.0558 . \nEventSquare -0.20781    0.07186 48.97131  -2.892   0.0057 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nEventSquare -0.695\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe won’t dive into the detailed interpretation of the results here—this isn’t the right place for that. However, if you are not familiar with linear mixed-effects models you can check our introduction of Linear models and Linear mixed-effects models where we try to break everything down step by step!\n\n\nThe key takeaway here is that there’s a significatn difference between the Event. Specifically, the Square cue appears to result in smaller pupil dilation compared to the Circle event (which serves as the reference level for the intercept). COOL!\nLet’s visualize the effect!!\n\nCode# Create a data grid for Event and time\ndatagrid = get_datagrid(model_avg, by = c('Event'))\n\n# Compute model-based expected values for each level of Event\npred = estimate_expectation(datagrid)\n\n# 'pred' now contains predicted values and confidence intervals for each event condition.\n# We can visualize these predictions and overlay them on the observed data.\n\nggplot() +\n  # Observed data (jittered points to show distribution)\n  geom_jitter(data = averaged_data, aes(x=Event, y=mean_pupil, color=Event), width=0.1, alpha=0.5, size = 5) +\n  \n  # Model-based predictions: points for Predicted values\n  geom_point(data=pred, aes(x=Event, y=Predicted, fill=Event), \n             shape=21, size=10) +\n  \n  # Error bars for the confidence intervals\n  geom_errorbar(data=pred, aes(x=Event, ymin=Predicted - SE, ymax=Predicted + SE, color=Event), \n                width=0.2, lwd=1.5) +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'none') +\n  labs(title=\"Predicted Means vs. Observed Data\",\n       x=\"Condition\",\n       y=\"Baseline-Corrected Pupil Size\")",
    "crumbs": [
      "Eye-tracking",
      "Pupil Data Analysis"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilDataAnalysis.html#analysing-the-time-course-of-pupil-size",
    "href": "CONTENT/EyeTracking/PupilDataAnalysis.html#analysing-the-time-course-of-pupil-size",
    "title": "Pupil Data Analysis",
    "section": "Analysing the time course of pupil size",
    "text": "Analysing the time course of pupil size\nAlthough we have seen how to compare mean values of pupil size, our original data was much richer. By taking averages, we made it simpler but we also lost precious information. Usually, it is better to keep the data as rich as possible, even if that might require more complex analyses. Here we’ll show you one example of a more complex analysis: generalised additive models. Fear not though!!! As usual, we will try to break it down in small digestible bites, and you might realise it’s not actually that complicated after all.\nThe key aspect here is that we will stop taking averages, and analyse the time course of pupil dilation instead. We will analyse how it changes over time with precision in the order of milliseconds!! This is exciting!!!\nThis is something that we cannot do with linear models. For example, in this case linear models would assume that, over the course of a trial, pupil size will only increase linearly over time. The model would be something like this:\n\nlinear_model = lmer(mean_pupil ~ Event * time + (1|Subject), data = data) \n\nNote that, compared to the previous model, we have made two changes: First, we have changed the data. While before we were using averages, now we use the richer data set; Second, we added time as a predictor. We are saying that mean_pupil might be changing linearly across time… But this is very silly!!! To understand how silly it is, let’s have a look at the data over time.\n\nCode# Let's first compute average pupil size at each time point by condition\ndata_avg_time = data %&gt;%\n  group_by(Event, time) %&gt;%\n  summarise(mean_pupil = mean(mean_pupil, na.rm=TRUE))\n\n# Now let's plot these averages over time\nggplot(data_avg_time, aes(x=time, y=mean_pupil, color=Event)) +\n  geom_line(lwd=1.5) +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'bottom',\n        legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) +\n\n  labs(x = \"time (ms)\",\n       y = \"Baseline-Corrected Pupil Size\") \n\n\n\n\n\n\n\nHere’s the data averaged by condition at each time point. As you can clearly see, pupil dilation doesn’t follow a simple linear increase or decrease; the pattern is much more complex. Let’s see how poorly a simple linear model fits this intricate pattern!\n\nCode# Create a data grid for Event and time\ndatagrid = get_datagrid(linear_model, by = c('Event','time'))\n\n# Estimate expectation and uncertainty (Predicted and SE)\nEst = estimate_expectation(linear_model, datagrid)\n\n# Plot predictions with confidence intervals and the observed data\nggplot() +\n  # Real data line\n  geom_line(data = data_avg_time, aes(x=time, y=mean_pupil, color=Event), lwd=1.5) +\n  \n  # Predicted ribbons\n  geom_ribbon(data = Est, aes(x=time, ymin = Predicted - SE, ymax = Predicted + SE,\n                              fill = Event), alpha = 0.2) +\n  \n  # Predicted lines\n  geom_line(data = Est, aes(x=time, y=Predicted, color=Event), lwd=1.8,linetype = \"dashed\") +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'bottom',\n        legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) +\n  labs(title = \"Linear Model Predictions vs. Data\",\n       x = \"time (ms)\",\n       y = \"Baseline-corrected Pupil Size\")\n\n\n\n\n\n\n\nThe estimates from our model don’t really resemble the actual data! To capture all those non-linear, smooth changes over time, we need a more sophisticated approach. Enter Generalized Additive Models (GAMs)—the perfect tool to save the day!\nGeneralized additive model\nHere, we will not get into all the details of generalized additive models (from now on, GAMs). We will just show one example of how they can be used to model pupil size. To do this, we have to abandon linear models and download a new package instead, mgcv (install.packages(\"mgcv\")). This package is similar to the one we used before for linear models but offers greater flexibility, particularly for modeling time-series data and capturing non-linear relationships.\nWhat are GAMs\nOk, cool! GAMs sound awesome… but you might still be wondering what they actually do. Let me show you an example with some figures—that always helps make things clearer!\n\nCodelibrary(patchwork)\n\n# Parameters\namp &lt;- 1; freq &lt;- 1; phase &lt;- 0; rate &lt;- 100; dur &lt;- 2\ntime &lt;- seq(0, dur, by = 1 / rate)\n\n# Sinusoidal wave with noise\nwave &lt;- amp * sin(2 * pi * freq * time + phase) + rnorm(length(time), mean = 0, sd = 0.2)\n\n\n# Plot\none = ggplot()+\n  geom_point(aes(y=wave, x= time),size=3)+\n  theme_bw(base_size = 45)+\n  labs(y='Data')\n\n\ntwo = ggplot()+\n  geom_point(aes(y=wave, x= time),size=3)+\n  geom_smooth(aes(y=wave, x= time), method = 'lm', color='black', lwd=1.5)+\n  theme_bw(base_size = 45)+\n   theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\ntree= ggplot()+\n  geom_point(aes(y=wave, x= time),size=3)+\n  geom_smooth(aes(y=wave, x= time), method = 'gam', color='black', lwd=1.5)+\n  theme_bw(base_size = 45)+\n   theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\none + two + tree\n\n\n\n\n\n\n\nWhen modeling data with many fluctuations, a simple linear model often falls short. In the left plot, we see the raw data with its complex, non-linear pattern. The middle plot illustrates a linear model’s attempt to capture these fluctuations, but it oversimplifies the relationships and fails to reflect the true data structure. Finally, the right plot showcases an additive model, which adapts to the data’s variability by following its fluctuations and accurately capturing the underlying pattern. This demonstrates the strength of additive models in modeling non-linear, smooth changes.\nWell….. This sounds like the same problem we have in our pupil data!!! Let’s go figure\n\n\n\n\n\n\nNote\n\n\n\nLinear models can be extended to capture fluctuations using polynomial terms, but this approach has limitations. Higher-order polynomials can overfit the data, capturing noise instead of meaningful patterns. Additive models, however, use smooth functions like splines to flexibly adapt to data fluctuations without the instability of polynomials, making them a more robust and interpretable choice.\n\n\nRun our GAM\nTo run a GAM, the syntax is relatively similar to what we used in the linear model section.\n\nlibrary(\"mgcv\")\n\n# Additive model\nadditive_model = bam(mean_pupil ~ Event\n                     + s(time, by=Event, k=20)\n                     + s(time, Subject, bs='fs', m=1),\n                     data=data)\n\nLet’s break the formula down:\n\nmean_pupil ~ Event: Here, I treat Condition as a main effect, just like we did before.\ns(time, by=Event, k=20): This is where the magic happens. By wrapping time in s(), we are telling the model: “Don’t assume that changes in pupil size over time are linear. Instead, estimate a smooth, wiggly function.” The argument by=Event means: “Do this separately for each condition, so that each condition gets its own smooth curve over time.” Finally, k=20 controls how wiggly the curve can be (technically, how many ‘knots’ or flexibility points the smoothing function is allowed to have). In practice, we are allowing the model to capture complex, non-linear patterns of pupil size changes over time for each condition.\ns(time, Subject, bs='fs', m=1): Here, we go one step further and acknowledge that each participant might have their own unique shape of the time course. By using bs='fs', I am specifying a ‘factor smooth’, which means: “For each subject, estimate their own smooth function over time.” Setting m=1 is a specific parameter choice that defines how we penalize wiggliness. Essentially, this term is allowing us to capture individual differences in how pupil size changes over time, over and above the general pattern captured by the main smooth. It’s something like the random effect we have seen before in the linear mixed-effect model.\n\nNow that we have run our first GAM, we can see how well it predicts the data!\n\nCode# Data grid\ndatagrid = get_datagrid(additive_model, length = 100, include_random = T)\n\n# Estimate expectation and uncertainty (Predicted and SE)\nEst = estimate_expectation(additive_model, datagrid, exclude=c(\"s(time,Subject)\"))\n\n\n# Plot predictions with confidence intervals and the observed data\nggplot() +\n  # Real data line\n  geom_line(data = data_avg_time, aes(x=time, y=mean_pupil, color=Event), size=1.5) +\n  \n  # Predicted ribbons\n  geom_ribbon(data = Est, aes(x=time, ymin = CI_low, ymax = CI_high,\n                              fill = Event), alpha = 0.2) +\n  \n  # Predicted lines\n  geom_line(data = Est, aes(x=time, y=Predicted, color=Event), size=1.8, linetype = \"dashed\") +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'bottom',\n        legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) +\n  labs(title = \"Additive model Predictions vs. Data\",\n       x = \"time (ms)\",\n       y = \"Baseline-corrected Pupil Size\")\n\n\n\n\n\n\n\nThis looks so much better!!! The line fit so much better to the data!! We can also have a look at whether the effect of our experimental condition is significant:\n\nsummary(additive_model) \n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nmean_pupil ~ Event + s(time, by = Event, k = 20) + s(time, Subject, \n    bs = \"fs\", m = 1)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.10594    0.03649   2.903  0.00373 ** \nEventSquare -0.20353    0.01333 -15.267  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                       edf Ref.df     F  p-value    \ns(time):EventCircle  3.959  4.883 6.306 1.31e-05 ***\ns(time):EventSquare  1.512  1.748 5.269   0.0197 *  \ns(time,Subject)     20.334 53.000 4.408  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.215   Deviance explained = 22.4%\nfREML = 594.43  Scale est. = 0.097128  n = 2200\n\n\nThe fixed effects (Parametric coefficients) show a strong negative effect for Eventquare, indicating that pupil size for Square is significantly lower than for Circle. This suggests that pupil size is greater when expecting a rewarding stimulus compared to a non-rewarding one.\nThe smooth terms indicate whether the non-linear relationships modeled by s() explain significant variance in the data. A significant smooth term confirms that the function captures meaningful, non-linear patterns beyond random noise or simpler terms. While fixed effects are typically more important for hypothesis testing, it’s crucial to ensure the model specification captures the data’s fluctuations accurately.\nYou did it!!! You started from a simpler model and little by little you built a very complex Generalized Additive Model!! Amazing work!!!\n\n\n\n\n\n\nWarning\n\n\n\nThis is just a very basic tutorial!\nThere are additional checks and considerations to keep in mind when using additive models to model pupil dilation data. We plan to extend this tutorial over time to include more details.\nLuckily, there are researchers who have already explored and explained these steps thoroughly. This paper, in particular, has greatly informed our approach. It dives deeper into the use of GAMs (still with the mgcv package), reviewing techniques for fitting models, addressing auto-correlations, and ensuring the accuracy and robustness of your GAMs. We highly recommend reading this paper to deepen your understanding!",
    "crumbs": [
      "Eye-tracking",
      "Pupil Data Analysis"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html",
    "title": "Create your first paradigm",
    "section": "",
    "text": "We will create a very simple and basic experiment that will be the stepping stone for some of the future tutorials. In the future tutorials we will show you how to extend and make this tutorial in a real experiment.\n\n\n\n\n\n\nStimuli!\n\n\n\nYou can download from here the stimuli that we will use in this example. They are very simple and basic stimuli:\n\na fixation cross\ntwo cues (a circle and a square)\na reward (a cute medal)\na non-reward (a cartoon of an exploding empty cloud)\na sound of winning at an arcade game\na sound of losing at an arcade game\n\n\n\nIn this tutorial, we will create an experiment in which, after the fixation cross, one of the two cues is presented. The cues will indicate whether we will receive a reward or not and where this will appear. After the circle is presented as a cue, the medal will be presented on the right. After the square is presented as a cue, the empty cloud will be presented on the left. Thus, if you follow the cued indication you will be able to predict the location of the next stimuli and whether or not it will be rewarding. Here below you can find a graphic representation of the design:\n\n\n\nFirst things first, let’s import the relevant libraries and define the path to where our stimuli are. PsychoPy has a lot of different modules that allow us to interface with different types of stimuli and systems. For this tutorial we need the following:\n\n# Import some libraries from PsychoPy and others\nimport os\nfrom pathlib import Path\nfrom psychopy import core, event, visual,sound\n\n\n\n\nThe next step is to create the window. The window is what we will show the stimuli in; it is the canvas on which to draw objects. For now, we will create a small window of 960*540 pixels. In this way, we will able to see the stimuli and still interact with the rest of our PC interface. In a real experiment, we would probably set the window dimension to the entirety of the display (fullscr=True) and maybe on a secondary screen (screen = 1).\n\n# Winsize\nwinsize = (960, 540)\n\n# create a window\nwin = visual.Window(size = winsize, fullscr=False, units=\"pix\", pos =(0,30), screen=0)\n\nNow let’s import the stimuli that we will present in this tutorial. We have 5 stimuli:\n\na fixation cross that we will use to catch the attention of our participants\na circle that will be our cue that signals a rewarding trial\na square that will be our cue that signals a non-rewarding trial\na cartoon of a medal that will be our reward\na cartoon of an empty cloud that will be our non-reward\n\nOn top of these visual stimuli, we will also import two sounds that will help us signal the type of trials. So:\n\na tada! winning sound\na papapaaa! losing sound\n\n\n\n\n\n\n\nTip\n\n\n\nWhen importing a visual stimulus we need to pass to the importing function in which window it will be displayed. In our case, we will pass all of them the “win” window that we just created.\n\n\n\n\n\n\n\n\nPATHS\n\n\n\nWhen working with file paths in Python, it’s important to remember that Windows and macOS/Linux use different conventions for their file paths:\n\nWindowsMac\n\n\nWindows file paths typically use backslashes (\\). However, in Python, a backslash is used as an escape character. To handle this, you have two options:\n\nUse double backslashes to avoid Python interpreting the backslash as an escape character:\n'C:\\\\Users\\\\tomma\\\\Desktop'\nAlternatively, use a raw string by prefixing the path with r, which tells Python to treat backslashes as literal characters:\nr'C:\\Users\\tomma\\Desktop'\n\n\n\nmacOS and Linux use forward slashes (/) for their file paths, which are also compatible with Python’s string handling. You can use the path directly. There’s no need for double slashes or raw strings in macOS/Linux paths.\n'/Users/tomma/Desktop'\n\n\n\n\n\nPython 3.4+ includes the pathlib module which provides a more intuitive, object-oriented approach to working with file paths that works across all operating systems. Once you create a Path object, the most powerful feature of pathlib is the ability to use the forward slash / operator to join paths together. This makes constructing file paths much more intuitive and readable:\n\nfrom pathlib import Path\n\n# Create a path object\nbase_dir = Path('C:/Users/tomma/Desktop')  # Works on Windows too!\n\n# Join paths with the / operator\nfull_path = base_dir / 'project' / 'data.csv'\n\n# Convert to string if needed for other libraries\nstr_path = str(full_path)\n\nThe / operator works just like you’d expect when writing file paths: it simply joins path components together, automatically handling separators correctly for your operating system. This means:\n\nNo need to remember to use different separators on different systems (\\ on Windows vs. / on Mac)\nNo issues with escape characters in strings (the \\ in Windows paths normally needs escaping in Python strings)\n\nFor our tutorial, we’ll use pathlib for better cross-platform compatibility, but it’s good to understand how paths work!\n\n\n\n\n#%% Load and prepare stimuli\n\n# Setting the directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n# Now create a Path object for the stimuli directory\nstimuli_dir = Path('EXP') / 'Stimuli'\n\n# Load images \nfixation = visual.ImageStim(win, image=str(stimuli_dir / 'fixation.png'), size=(200, 200))\ncircle = visual.ImageStim(win, image=str(stimuli_dir / 'circle.png'), size=(200, 200))\nsquare = visual.ImageStim(win, image=str(stimuli_dir / 'square.png'), size=(200, 200))\nwinning = visual.ImageStim(win, image=str(stimuli_dir / 'winning.png'), size=(200, 200), pos=(250, 0))\nlosing = visual.ImageStim(win, image=str(stimuli_dir / 'loosing.png'), size=(200, 200), pos=(-250, 0))\n\n# Load sound \nwinning_sound = sound.Sound(str(stimuli_dir / 'winning.wav'))\nlosing_sound = sound.Sound(str(stimuli_dir / 'loosing.wav'))\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create a list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\nNote that in this simple experiment, we will present the reward always on the right and the non-rewards always on the left that’s why when we import the two rewards we set their pos to (250,0) and (-250,0). The first value indicates the number of pixels on the x-axis and the second is the number of pixels on the y-axis.\n\n\n\nNow we want to show a stimulus in the center of our window. To do so, we will have to use the function “draw”. As the name suggests this function draws the stimulus that we want on the window that we have created.\nLet’s start with displaying the fixation cross in the center.\n\n# Draw the fixation\nfixation.draw()\n\nDo you see the fixation cross?????? Probably not!! This is because we have drawn the fixation cross but we have not refreshed the window. Psychopy allows you to draw as many stimuli as you want on a window but the changes are only shown when you “refresh” the window. To do so we need to use the “flip” function.\n\nwin.flip()\n\nPerfect!!!! The fixation cross is there. Before each flip, we need to draw our objects. Otherwise, we will only see the basic window with nothing in it. Let’s try!!! flip the window now.\n\n# Flipping the window (refreshing)\nwin.flip()\n\nThe fixation is gone again! Exactly as predicted. Flipping the window allows us to draw and show something new in each frame. This means that the speed limit of our presentation is the actual frame rate of our display. If we have a 60Hz display we can present an image 60 times in a second.\nSo if we want to present our fixation for an entire second we would have to draw and flip it 60 times (our display has a refresh rate of 60Hz)! Let’s try:\n\nfor _ in range(60):\n    fixation.draw()\n    win.flip()\nwin.flip() # we re-flip at the end to clean the window\n\nNow we have shown the fixation for 1 second and then it disappeared. Nice!! However, you probably have already figured out that what we have done was unnecessary. If we want to present a static stimulus for 1s we could have just drawn it, flipped the window, and then waited for 1s. But now you have an idea of how to show animated stimuli or even videos!!! AMAZING!!!.\nNow let’s try to show the fixation for 1s by just waiting.\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n\n\n\nWe have seen how to show a stimulus let’s now play the sounds that we have imported. This is extremely simple, we can just play() them:\n\nwinning_sound.play()\ncore.wait(2)\nlosing_sound.play()\n\nGreat now we have played our two sounds!!\n\n\n\n\n\n\nWarning\n\n\n\nWhen playing a sound the script will continue and will not wait for the sound to have finished playing. So if you play two sounds one after without waiting the two sounds will play overlapping. That’s why we have used core.wait(2), this tells PsychoPy to wait 2 seconds after starting to play the sound.\n\n\n\n\n\nNow let’s try to put everything we have learned in one place and present one rewarding and one non-rewarding trial:\n\nwe present the fixation for 1s\nwe present one of the two cues for 3s\nwe wait 750ms of blank screen\nwe present the reward or the non-reward depending on the cue for 2s.\n\nIn the end, we also close the window.\n\n###### 1st Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the winning cue\ncircle.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nwinning.draw()\nwin.flip()\nwinning_sound.play()\ncore.wait(2)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n###### 2nd Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the non-rewarding cue\nsquare.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nlosing.draw()\nwin.flip()\nlosing_sound.play()\ncore.wait(2)  # wait for 2 second\nwin.flip()    # we re-flip at the end to clean the window\n\n\nwin.close()  # let's close the window at the end of the trial\n\n\n\n\nWe’ve now completed a trial, but having trials run back-to-back is often not ideal. Typically, we insert a brief pause between trials—this is called the Inter-Stimulus Interval (ISI).One common way to implement an ISI is to use a simple wait function, for example:\n\ncore.wait(1)\n\nThis is straightforward, but in some cases you might want more control over the timing. For example, if you need to account for any slight delays during a trial, you can use a clock to measure elapsed time.\nTo implement this ISI, we’ll create a PsychoPy core.Clock(). Once initiated, this clock starts keeping track of time, letting us know how much time has elapsed. After kicking off the clock, we can do some other tasks, then wait for the remaining time to reach our 1-second ISI.\n\n### ISI\nclock = core.Clock()  # start the clock\n\nprint('Here we can do other stuff')\n\ncore.wait(1 - clock.getTime()) # wait for remaining time\n\nPerfect—this method essentially waits for 1 second just like a simple core.wait(1). But here’s the cool part: since we’re using the clock, we can run other code in the meantime without stopping everything entirely. This flexibility will be super handy in future tutorials—you’ll see!\nLet’s add this at the end of our trials!!\n\n\n\nFantastic, we’ve nearly have our study! However, studies often don’t run to completion, especially when we’re working with infants and children. More often than not, we need to halt the study prematurely. This could be due to the participant becoming fatigued or distracted, or perhaps we need to tweak some settings.\nHow can we accomplish this? Of course, we could just shut down Python and let the experiment crash… but surely, there’s a more elegant solution… And indeed, there is! In fact, there are numerous methods to achieve this, and we’re going to demonstrate one of the most straightforward and flexible ones to you.\nWe can use the event.getKeys() function to ask Psychopy to report any key that has been pressed during our trial. In our case, we will check if the ESC key has been pressed and if it has, we will simply close the window and stop the study.\n\n### Check for closing experiment\nkeys = event.getKeys() # collect list of pressed keys\nif 'escape' in keys:\n    win.close()  # close window\n    core.quit()  # stop study\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can add this check for closing the study at any point during the study. However, we recommend placing it at the end of each trial. This ensures that even if you stop the experiment, you will have a complete trial, making it easier to analyze data since you won’t have any incomplete sections of the study.\nAlso, you can use this same method to pause the study or interact with its progress in general.\n\n\n\n\n\nIn an experiment, we want more than 1 trial. Let’s then create an experiment with 10 trials. We just need to repeat what we have done above multiple times. However, we need to randomize the type of trials, otherwise, it would be too easy to learn. To do so, we will create a list of 0 and 1. where 0 would identify a rewarding trial and 1 would index a non-rewarding trial.\nTo properly utilize this list of 0 and 1, we will need to create other lists of our stimuli. This will make it easier to call the right stimuli depending on the trial. We can do so by:\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\nPerfect!! Now we can put all the pieces together and run our experiment.\n\n\n\n\n\n\nCaution\n\n\n\nIn this final script, we will change the dimension of the window we will use. Since in most of the experiments, we will want to use the entire screen to our disposal, we will set fullscr = True when defining the window. In addition, we will also change the position of the rewarding and non-rewarding stimulus since now the window is bigger.\nIf you are testing this script on your laptop and do not want to lose the ability to interact with it until the experiment is finished, keep the same window size and position as the previous lines of code.\n\n\n\n# Import some libraries from PsychoPy and others\nimport os\nfrom pathlib import Path\nfrom psychopy import core, event, visual, sound\n\n#%% Load and prepare stimuli\n\n# Setting the directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n# Now create a Path object for the stimuli directory\nstimuli_dir = Path('EXP') / 'Stimuli'\n\n\n# Winsize\nwinsize = (1920, 1080)\n\n# create a window\nwin = visual.Window(size = winsize, fullscr=False, units=\"pix\", pos =(0,30), screen=1)\n\n\n# Load images \nfixation = visual.ImageStim(win, image=str(stimuli_dir / 'fixation.png'), size=(200, 200))\ncircle = visual.ImageStim(win, image=str(stimuli_dir / 'circle.png'), size=(200, 200))\nsquare = visual.ImageStim(win, image=str(stimuli_dir / 'square.png'), size=(200, 200))\nwinning = visual.ImageStim(win, image=str(stimuli_dir / 'winning.png'), size=(200, 200), pos=(250, 0))\nlosing = visual.ImageStim(win, image=str(stimuli_dir / 'loosing.png'), size=(200, 200), pos=(-250, 0))\n\n# Load sound \nwinning_sound = sound.Sound(str(stimuli_dir / 'winning.wav'))\nlosing_sound = sound.Sound(str(stimuli_dir / 'loosing.wav'))\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create a list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\n\n#%% Trials\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n    fixation.draw()\n    win.flip()\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    core.wait(3)  # wait for 3 seconds\n\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n    sounds[trial].play()\n    core.wait(2)  # wait for 1 second\n    win.flip()    # we re-flip at the end to clean the window\n\n    ### ISI\n    clock = core.Clock()  # start the clock\n    core.wait(1 - clock.getTime()) # wait for remaining time\n      \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        core.quit()  # stop study\n        \nwin.close()\ncore.quit()",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#preparation",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#preparation",
    "title": "Create your first paradigm",
    "section": "",
    "text": "First things first, let’s import the relevant libraries and define the path to where our stimuli are. PsychoPy has a lot of different modules that allow us to interface with different types of stimuli and systems. For this tutorial we need the following:\n\n# Import some libraries from PsychoPy and others\nimport os\nfrom pathlib import Path\nfrom psychopy import core, event, visual,sound",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#stimuli-1",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#stimuli-1",
    "title": "Create your first paradigm",
    "section": "",
    "text": "The next step is to create the window. The window is what we will show the stimuli in; it is the canvas on which to draw objects. For now, we will create a small window of 960*540 pixels. In this way, we will able to see the stimuli and still interact with the rest of our PC interface. In a real experiment, we would probably set the window dimension to the entirety of the display (fullscr=True) and maybe on a secondary screen (screen = 1).\n\n# Winsize\nwinsize = (960, 540)\n\n# create a window\nwin = visual.Window(size = winsize, fullscr=False, units=\"pix\", pos =(0,30), screen=0)\n\nNow let’s import the stimuli that we will present in this tutorial. We have 5 stimuli:\n\na fixation cross that we will use to catch the attention of our participants\na circle that will be our cue that signals a rewarding trial\na square that will be our cue that signals a non-rewarding trial\na cartoon of a medal that will be our reward\na cartoon of an empty cloud that will be our non-reward\n\nOn top of these visual stimuli, we will also import two sounds that will help us signal the type of trials. So:\n\na tada! winning sound\na papapaaa! losing sound\n\n\n\n\n\n\n\nTip\n\n\n\nWhen importing a visual stimulus we need to pass to the importing function in which window it will be displayed. In our case, we will pass all of them the “win” window that we just created.\n\n\n\n\n\n\n\n\nPATHS\n\n\n\nWhen working with file paths in Python, it’s important to remember that Windows and macOS/Linux use different conventions for their file paths:\n\nWindowsMac\n\n\nWindows file paths typically use backslashes (\\). However, in Python, a backslash is used as an escape character. To handle this, you have two options:\n\nUse double backslashes to avoid Python interpreting the backslash as an escape character:\n'C:\\\\Users\\\\tomma\\\\Desktop'\nAlternatively, use a raw string by prefixing the path with r, which tells Python to treat backslashes as literal characters:\nr'C:\\Users\\tomma\\Desktop'\n\n\n\nmacOS and Linux use forward slashes (/) for their file paths, which are also compatible with Python’s string handling. You can use the path directly. There’s no need for double slashes or raw strings in macOS/Linux paths.\n'/Users/tomma/Desktop'\n\n\n\n\n\nPython 3.4+ includes the pathlib module which provides a more intuitive, object-oriented approach to working with file paths that works across all operating systems. Once you create a Path object, the most powerful feature of pathlib is the ability to use the forward slash / operator to join paths together. This makes constructing file paths much more intuitive and readable:\n\nfrom pathlib import Path\n\n# Create a path object\nbase_dir = Path('C:/Users/tomma/Desktop')  # Works on Windows too!\n\n# Join paths with the / operator\nfull_path = base_dir / 'project' / 'data.csv'\n\n# Convert to string if needed for other libraries\nstr_path = str(full_path)\n\nThe / operator works just like you’d expect when writing file paths: it simply joins path components together, automatically handling separators correctly for your operating system. This means:\n\nNo need to remember to use different separators on different systems (\\ on Windows vs. / on Mac)\nNo issues with escape characters in strings (the \\ in Windows paths normally needs escaping in Python strings)\n\nFor our tutorial, we’ll use pathlib for better cross-platform compatibility, but it’s good to understand how paths work!\n\n\n\n\n#%% Load and prepare stimuli\n\n# Setting the directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n# Now create a Path object for the stimuli directory\nstimuli_dir = Path('EXP') / 'Stimuli'\n\n# Load images \nfixation = visual.ImageStim(win, image=str(stimuli_dir / 'fixation.png'), size=(200, 200))\ncircle = visual.ImageStim(win, image=str(stimuli_dir / 'circle.png'), size=(200, 200))\nsquare = visual.ImageStim(win, image=str(stimuli_dir / 'square.png'), size=(200, 200))\nwinning = visual.ImageStim(win, image=str(stimuli_dir / 'winning.png'), size=(200, 200), pos=(250, 0))\nlosing = visual.ImageStim(win, image=str(stimuli_dir / 'loosing.png'), size=(200, 200), pos=(-250, 0))\n\n# Load sound \nwinning_sound = sound.Sound(str(stimuli_dir / 'winning.wav'))\nlosing_sound = sound.Sound(str(stimuli_dir / 'loosing.wav'))\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create a list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\nNote that in this simple experiment, we will present the reward always on the right and the non-rewards always on the left that’s why when we import the two rewards we set their pos to (250,0) and (-250,0). The first value indicates the number of pixels on the x-axis and the second is the number of pixels on the y-axis.",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#show-a-visual-stimulus",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#show-a-visual-stimulus",
    "title": "Create your first paradigm",
    "section": "",
    "text": "Now we want to show a stimulus in the center of our window. To do so, we will have to use the function “draw”. As the name suggests this function draws the stimulus that we want on the window that we have created.\nLet’s start with displaying the fixation cross in the center.\n\n# Draw the fixation\nfixation.draw()\n\nDo you see the fixation cross?????? Probably not!! This is because we have drawn the fixation cross but we have not refreshed the window. Psychopy allows you to draw as many stimuli as you want on a window but the changes are only shown when you “refresh” the window. To do so we need to use the “flip” function.\n\nwin.flip()\n\nPerfect!!!! The fixation cross is there. Before each flip, we need to draw our objects. Otherwise, we will only see the basic window with nothing in it. Let’s try!!! flip the window now.\n\n# Flipping the window (refreshing)\nwin.flip()\n\nThe fixation is gone again! Exactly as predicted. Flipping the window allows us to draw and show something new in each frame. This means that the speed limit of our presentation is the actual frame rate of our display. If we have a 60Hz display we can present an image 60 times in a second.\nSo if we want to present our fixation for an entire second we would have to draw and flip it 60 times (our display has a refresh rate of 60Hz)! Let’s try:\n\nfor _ in range(60):\n    fixation.draw()\n    win.flip()\nwin.flip() # we re-flip at the end to clean the window\n\nNow we have shown the fixation for 1 second and then it disappeared. Nice!! However, you probably have already figured out that what we have done was unnecessary. If we want to present a static stimulus for 1s we could have just drawn it, flipped the window, and then waited for 1s. But now you have an idea of how to show animated stimuli or even videos!!! AMAZING!!!.\nNow let’s try to show the fixation for 1s by just waiting.\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#play-a-sound",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#play-a-sound",
    "title": "Create your first paradigm",
    "section": "",
    "text": "We have seen how to show a stimulus let’s now play the sounds that we have imported. This is extremely simple, we can just play() them:\n\nwinning_sound.play()\ncore.wait(2)\nlosing_sound.play()\n\nGreat now we have played our two sounds!!\n\n\n\n\n\n\nWarning\n\n\n\nWhen playing a sound the script will continue and will not wait for the sound to have finished playing. So if you play two sounds one after without waiting the two sounds will play overlapping. That’s why we have used core.wait(2), this tells PsychoPy to wait 2 seconds after starting to play the sound.",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#create-a-trial",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#create-a-trial",
    "title": "Create your first paradigm",
    "section": "",
    "text": "Now let’s try to put everything we have learned in one place and present one rewarding and one non-rewarding trial:\n\nwe present the fixation for 1s\nwe present one of the two cues for 3s\nwe wait 750ms of blank screen\nwe present the reward or the non-reward depending on the cue for 2s.\n\nIn the end, we also close the window.\n\n###### 1st Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the winning cue\ncircle.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nwinning.draw()\nwin.flip()\nwinning_sound.play()\ncore.wait(2)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n###### 2nd Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the non-rewarding cue\nsquare.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nlosing.draw()\nwin.flip()\nlosing_sound.play()\ncore.wait(2)  # wait for 2 second\nwin.flip()    # we re-flip at the end to clean the window\n\n\nwin.close()  # let's close the window at the end of the trial",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#isi",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#isi",
    "title": "Create your first paradigm",
    "section": "",
    "text": "We’ve now completed a trial, but having trials run back-to-back is often not ideal. Typically, we insert a brief pause between trials—this is called the Inter-Stimulus Interval (ISI).One common way to implement an ISI is to use a simple wait function, for example:\n\ncore.wait(1)\n\nThis is straightforward, but in some cases you might want more control over the timing. For example, if you need to account for any slight delays during a trial, you can use a clock to measure elapsed time.\nTo implement this ISI, we’ll create a PsychoPy core.Clock(). Once initiated, this clock starts keeping track of time, letting us know how much time has elapsed. After kicking off the clock, we can do some other tasks, then wait for the remaining time to reach our 1-second ISI.\n\n### ISI\nclock = core.Clock()  # start the clock\n\nprint('Here we can do other stuff')\n\ncore.wait(1 - clock.getTime()) # wait for remaining time\n\nPerfect—this method essentially waits for 1 second just like a simple core.wait(1). But here’s the cool part: since we’re using the clock, we can run other code in the meantime without stopping everything entirely. This flexibility will be super handy in future tutorials—you’ll see!\nLet’s add this at the end of our trials!!",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#stop-the-experiment",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#stop-the-experiment",
    "title": "Create your first paradigm",
    "section": "",
    "text": "Fantastic, we’ve nearly have our study! However, studies often don’t run to completion, especially when we’re working with infants and children. More often than not, we need to halt the study prematurely. This could be due to the participant becoming fatigued or distracted, or perhaps we need to tweak some settings.\nHow can we accomplish this? Of course, we could just shut down Python and let the experiment crash… but surely, there’s a more elegant solution… And indeed, there is! In fact, there are numerous methods to achieve this, and we’re going to demonstrate one of the most straightforward and flexible ones to you.\nWe can use the event.getKeys() function to ask Psychopy to report any key that has been pressed during our trial. In our case, we will check if the ESC key has been pressed and if it has, we will simply close the window and stop the study.\n\n### Check for closing experiment\nkeys = event.getKeys() # collect list of pressed keys\nif 'escape' in keys:\n    win.close()  # close window\n    core.quit()  # stop study\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can add this check for closing the study at any point during the study. However, we recommend placing it at the end of each trial. This ensures that even if you stop the experiment, you will have a complete trial, making it easier to analyze data since you won’t have any incomplete sections of the study.\nAlso, you can use this same method to pause the study or interact with its progress in general.",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#create-an-entire-experiment",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#create-an-entire-experiment",
    "title": "Create your first paradigm",
    "section": "",
    "text": "In an experiment, we want more than 1 trial. Let’s then create an experiment with 10 trials. We just need to repeat what we have done above multiple times. However, we need to randomize the type of trials, otherwise, it would be too easy to learn. To do so, we will create a list of 0 and 1. where 0 would identify a rewarding trial and 1 would index a non-rewarding trial.\nTo properly utilize this list of 0 and 1, we will need to create other lists of our stimuli. This will make it easier to call the right stimuli depending on the trial. We can do so by:\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\nPerfect!! Now we can put all the pieces together and run our experiment.\n\n\n\n\n\n\nCaution\n\n\n\nIn this final script, we will change the dimension of the window we will use. Since in most of the experiments, we will want to use the entire screen to our disposal, we will set fullscr = True when defining the window. In addition, we will also change the position of the rewarding and non-rewarding stimulus since now the window is bigger.\nIf you are testing this script on your laptop and do not want to lose the ability to interact with it until the experiment is finished, keep the same window size and position as the previous lines of code.\n\n\n\n# Import some libraries from PsychoPy and others\nimport os\nfrom pathlib import Path\nfrom psychopy import core, event, visual, sound\n\n#%% Load and prepare stimuli\n\n# Setting the directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n# Now create a Path object for the stimuli directory\nstimuli_dir = Path('EXP') / 'Stimuli'\n\n\n# Winsize\nwinsize = (1920, 1080)\n\n# create a window\nwin = visual.Window(size = winsize, fullscr=False, units=\"pix\", pos =(0,30), screen=1)\n\n\n# Load images \nfixation = visual.ImageStim(win, image=str(stimuli_dir / 'fixation.png'), size=(200, 200))\ncircle = visual.ImageStim(win, image=str(stimuli_dir / 'circle.png'), size=(200, 200))\nsquare = visual.ImageStim(win, image=str(stimuli_dir / 'square.png'), size=(200, 200))\nwinning = visual.ImageStim(win, image=str(stimuli_dir / 'winning.png'), size=(200, 200), pos=(250, 0))\nlosing = visual.ImageStim(win, image=str(stimuli_dir / 'loosing.png'), size=(200, 200), pos=(-250, 0))\n\n# Load sound \nwinning_sound = sound.Sound(str(stimuli_dir / 'winning.wav'))\nlosing_sound = sound.Sound(str(stimuli_dir / 'loosing.wav'))\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create a list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\n\n#%% Trials\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n    fixation.draw()\n    win.flip()\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    core.wait(3)  # wait for 3 seconds\n\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n    sounds[trial].play()\n    core.wait(2)  # wait for 1 second\n    win.flip()    # we re-flip at the end to clean the window\n\n    ### ISI\n    clock = core.Clock()  # start the clock\n    core.wait(1 - clock.getTime()) # wait for remaining time\n      \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        core.quit()  # stop study\n        \nwin.close()\ncore.quit()",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPython.html",
    "href": "CONTENT/GettingStarted/GettingStartedWithPython.html",
    "title": "Starting with Python",
    "section": "",
    "text": "Python is one of the most popular programming languages in general. In data science, it competes with Matlab and R for first place on the podium.\nIn our everyday we often use python to pre-process and analyze the data. In this tutorial we will explain our preferred way of installing python and managing its libraries. There are several ways to install python this is the one we recommend for its simplicity and flexibility.",
    "crumbs": [
      "Getting started:",
      "Starting with Python"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPython.html#miniconda",
    "href": "CONTENT/GettingStarted/GettingStartedWithPython.html#miniconda",
    "title": "Starting with Python",
    "section": "Miniconda",
    "text": "Miniconda\nMiniconda is our favorite way to install Python, and for good reason! While you’ve probably heard of Anaconda – that feature-packed GUI many folks use to manage Python, packages, and environments – Miniconda is its sleek, lightweight counterpart.\nWhat makes Miniconda special? It skips all the extra GUI elements and pre-installed packages that come with Anaconda. The result? A much lighter installation that doesn’t weigh down your system! With Miniconda, you get just the essentials: Conda, Python, and a few critical dependencies. This minimal setup gives you greater control over what gets installed, keeping everything lean and efficient. Plus, it includes the default Conda package manager and channels, so you still have full access to the official Conda repository for all your package needs.\n\nWindowsMac\n\n\nTo use Miniconda download the installer from Miniconda (remember to scroll down under Anaconda)\nThe installation process is similar to that of Anaconda. Once the installation is complete, you will find the Anaconda Prompt among your programs. This prompt serves as your interface for installing Python packages and creating environments. To verify everything’s working properly, simply type conda. You’ll be greeted with a comprehensive list of available conda commands ready for you to explore and use (we will see below how to do so).\n\n\n\n\n\n\n\nTo use Miniconda download the installer from Miniconda (remember to scroll down under Anaconda)\n\n\n\n\n\n\nCaution\n\n\n\n⚠️ Important\nPlease make sure that you select the correct version for your system!!! Select the version depending if you have Apple Silicon or an Intel processor.\n\n\nOnce the installation is complete, you’ll have the full power of Miniconda at your fingertips through your terminal! To verify everything’s working properly, simply open your terminal and type conda. You’ll be greeted with a comprehensive list of available conda commands ready for you to explore and use (we will see below how to do so).",
    "crumbs": [
      "Getting started:",
      "Starting with Python"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearMixedModels.html",
    "href": "CONTENT/Stats/LinearMixedModels.html",
    "title": "Linear mixed effect modesl",
    "section": "",
    "text": "Welcome to this introduction to Linear Mixed-effects Models (LMM)!! In this tutorial we will use R to run some simple LMM and we will try to understand together how to leverage these model for our analysis.\n        LMMs are amazing tools that have saved our asses countless times during our PhDs and Postdocs. They'll probably continue to be our trusty companions forever.\nThis tutorial introduces the statistical concept of Hierarchical Modeling, often called Mixed Effects Modeling. This approach shines when dealing with nested data—situations where observations are grouped in meaningful ways, like students within schools or measurements across individuals.\nSounds like a mouthful, right? Don’t worry! Let’s kick things off with something a little more fun: Simpson’s Paradox.\nSimpson’s Paradox is a statistical head-scratcher. It’s when a trend you see in your data suddenly flips—or even vanishes—when you split the data into groups. Ready to see it in action? Let’s dive in!\nImagine we’re looking at how years of experience impact salary at a university. Here’s some simulated data to make it fun.\nCodelibrary(easystats)\nlibrary(tidyverse)\nlibrary(patchwork)\nset.seed(1234)\ndata &lt;- simulate_simpson(n = 10, groups = 5, r = 0.5,difference = 1.5) %&gt;% \n  mutate(V2= (V2 +abs(min(V2)))*10000) %&gt;% \n  rename(Department = Group)\n\n# Lookup vector: map old values to new ones\nlookup &lt;- c(G_1 = \"Informatics\", G_2 = \"English\", \n            G_3 = \"Sociology\", G_4 = \"Biology\", G_5 = \"Statistics\")\n\n# Replace values using the lookup vector\ndata$Department &lt;- lookup[as.character(data$Department)]\n\n\none = ggplot(data, aes(x = V1, y = V2)) +\n  geom_point()+\n  geom_smooth(method = 'lm')+\n  labs(y='Salary', x='Year of experience', title = \"A. Linear model\")+\n  theme_bw(base_size = 20)\n\ntwo = ggplot(data, aes(x = V1, y = V2)) +\n  geom_point(aes(color = Department)) +\n  geom_smooth(aes(color = Department), method = \"lm\", alpha = 0.3) +\n  geom_smooth(method = \"lm\", alpha = 0.3)+\n  labs(y='Salary', x='Year of experience', title = \"B. Linear model acounting for grouping structure\")+\n  theme_bw(base_size = 20)+\n  theme(legend.position = 'bottom')\n\n(one / two)\nTake a look at the first plot. Whoa, wait a minute—this says the more years of experience you have, the less you get paid! What kind of backwards world is this? Before you march into HR demanding answers, let’s look a little closer.\nNow, check out the second plot. Once we consider the departments—Informatics, English, Sociology, Biology, and Statistics—a different story emerges. Each department shows a positive trend between experience and salary. In other words, more experience does mean higher pay, as it should!\nSo what happened? The first plot ignored the hierarchical structure of the data. By lumping all departments together, it completely missed the real relationship hiding in plain sight. This is why Hierarchical Modeling is so powerful—it helps us avoid embarrassing statistical blunders like this one. It allows us to correctly analyze data with nested structures and uncover the real patterns.\nNow, this example is a bit of an extreme case. In real life, you’re less likely to find such wildly opposite effects. However, the influence of grouping structures on your analysis is very real—and often subtle. Ignoring them can lead to misleading conclusions.\nReady to explore how Mixed Effects Modeling helps us account for these nested structures? Let’s dive in and get hands-on!",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearMixedModels.html#settings-and-data",
    "href": "CONTENT/Stats/LinearMixedModels.html#settings-and-data",
    "title": "Linear mixed effect modesl",
    "section": "Settings and data",
    "text": "Settings and data\nIn this section, we’ll set up our working environment by loading the necessary libraries and importing the dataset. You’ll likely already have this dataset available if you completed the previous linear models tutorial. If not, don’t worry—you can easily download it using the link below:\n\n\nlibrary(lme4)\nlibrary(lmerTest)\n\nlibrary(tidyverse)\nlibrary(easystats)\n\nThe lme4 package is the go-to library for running Linear Mixed Models (LMM) in R. To make your life easier, there’s also the lmerTest package, which enhances lme4 by allowing you to extract p-values and providing extra functions to better understand your models. In my opinion, you should always use lmerTest alongside lme4—it just makes everything smoother!\nTo run our Linear Mixed Effects Model, these are the key packages we’ll use. On top of that, the tidyverse suite will help with data wrangling and visualization, while easystats will let us easily extract and summarize the important details from our models. Let’s get started!\nRead the data\n\ndf = read.csv(\"../../resources/Stats/Dataset.csv\")\ndf$Id = factor(df$Id) # make sure subject_id is a factor\ndf$StandardTrialN = standardize(df$TrialN) # create standardize trial column\n\nAfter importing the data, we’ve ensured that Id is treated as a factor rather than a numerical column and that we have a standardized column of TrialN.",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearMixedModels.html#linear-model",
    "href": "CONTENT/Stats/LinearMixedModels.html#linear-model",
    "title": "Linear mixed effect modesl",
    "section": "Linear Model",
    "text": "Linear Model\nWhile we already seen how to run a linear model we will rerun it here as a comparison to the next steps. In case something is not clear about this lm() please go back to the previous tutorial on linear models.\n\nmod_lm = lm(LookingTime ~ StandardTrialN*Event, data = df)\nsummary(mod_lm)\n\n\nCall:\nlm(formula = LookingTime ~ StandardTrialN * Event, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-772.0 -159.7   -6.1  190.5  607.1 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                1310.3674    16.9387  77.359  &lt; 2e-16 ***\nStandardTrialN              -43.8501    17.2505  -2.542   0.0113 *  \nEventReward                 101.2269    21.1961   4.776 2.21e-06 ***\nStandardTrialN:EventReward    0.7688    21.4872   0.036   0.9715    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 253.2 on 656 degrees of freedom\n  (140 observations deleted due to missingness)\nMultiple R-squared:  0.05147,   Adjusted R-squared:  0.04713 \nF-statistic: 11.87 on 3 and 656 DF,  p-value: 1.417e-07\n\n\nWe won’t delve into the details of the model results in this tutorial, as we have already cover it in the previous one. However we want to point one thing about the data we run it on!!\n\nstr(df)\n\n'data.frame':   800 obs. of  6 variables:\n $ Id            : Factor w/ 20 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Event         : chr  \"NoReward\" \"NoReward\" \"NoReward\" \"NoReward\" ...\n $ TrialN        : int  1 2 3 4 5 6 7 8 9 10 ...\n $ ReactionTime  : num  344 362 406 395 382 ...\n $ LookingTime   : num  1140 1156 1074 1107 991 ...\n $ StandardTrialN: 'dw_transformer' num  -1.646 -1.473 -1.3 -1.127 -0.953 ...\n  ..- attr(*, \"center\")= num 10.5\n  ..- attr(*, \"scale\")= num 5.77\n  ..- attr(*, \"robust\")= logi FALSE\n\n\nWait a minute! Look at our data - we have an Id column! 👀 This column tells us which participant each trial belongs to. As each subject experienced all trial conditions, we have multiple data points per person. This is similar to the departments in the previous example…its a grouping variable\nWait..but then we should have taken it into consideration!!!\nInstead there was nothing about Id in our lm()…there is nothing in the formula about Id….\nYes we did not account for this grouping structure…let’s fix that!! But how do we do so? Well, at this point it’s obvious…with Mixed effects models!! Let’s dive in..",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearMixedModels.html#mixed-effects",
    "href": "CONTENT/Stats/LinearMixedModels.html#mixed-effects",
    "title": "Linear mixed effect modesl",
    "section": "Mixed Effects",
    "text": "Mixed Effects\nRandom Intercept\nAlright, let’s start with Random Intercepts! What are they? Well, the name gives it away—they’re just intercepts…but with a twist! 🤔\nIf you recall your knowledge of linear models, you’ll remember that each model has one intercept—the point where the model crosses the y-axis (when x=0).\nBut what makes random intercepts special? They allow the model to have different intercepts for each grouping variable—in this case, the Ids. This means we’re letting the model assume that each subject may have a slightly different baseline performance.\nHere’s the idea:\n\nOne person might naturally be a bit better.\nSomeone else could be slightly worse.\nAnd me? Well, let’s just say I’m starting from rock bottom.\n\nHowever, even though we’re starting from different baselines, the rate of improvement over trials can still be consistent across subjects.\nThis approach helps us capture variation in the starting performance, acknowledging that people are inherently different but might still follow a similar overall pattern of improvement. It’s a simple yet powerful way to model individual differences!\nNow, let’s look at how to include this in our mixed model.\nModel\nTo run a linear mixed-effects model, we’ll use the lmer function from the lme4 package. it Functions very similarly to the lm function we used before: you pass a formula and a dataset, but with one important addition: specifying the random intercept.\nThe formula is nearly the same as a standard linear model, but we include (1|subject_id) to tell the model that each subject should have its own unique intercept. This accounts for variations in baseline performance across individuals.\n\n\n\n\n\n\nCaution\n\n\n\nWhen specifying random intercepts (like (1|Group)), your grouping variables must be factors! If a grouping variable is numeric, R will wrongly treat it as a continuous variable rather than discrete categories. Character variables are automatically fine, but numeric grouping variables must be explicitly converted using factor().\n\n\n\nmod_rintercept =lmer(LookingTime ~ StandardTrialN * Event+ (1|Id ), data= df,  na.action = na.exclude)\nsummary(mod_rintercept)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: LookingTime ~ StandardTrialN * Event + (1 | Id)\n   Data: df\n\nREML criterion at convergence: 7665.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4289 -0.6265  0.0007  0.6061  4.1655 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Id       (Intercept) 61071    247.13  \n Residual              5669     75.29  \nNumber of obs: 660, groups:  Id, 20\n\nFixed effects:\n                           Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)                1291.113     55.518   19.248  23.256 1.49e-15 ***\nStandardTrialN              -61.378      5.321  637.281 -11.536  &lt; 2e-16 ***\nEventReward                 120.849      6.552  637.315  18.446  &lt; 2e-16 ***\nStandardTrialN:EventReward   18.901      6.535  637.176   2.892  0.00396 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) StndTN EvntRw\nStandrdTrlN  0.043              \nEventReward -0.079 -0.362       \nStndrdTN:ER -0.035 -0.812  0.299\n\n\nWow! Now the model is showing us something new compared to the simple linear model. We observe an interaction between Event and StandardTrialN. By letting the intercept vary for each subject, the model is able to capture nuances in the data that a standard linear model might miss.\nTo understand this interaction, let’s plot it and see how performance changes across trials for each condition.\n\nCodei_pred = estimate_expectation(mod_rintercept, include_random=T)\n\nggplot(i_pred, aes(x= StandardTrialN, y= Predicted, color= Id, shape = Event))+\n    geom_point(data = df, aes(y= LookingTime, color= Id), position= position_jitter(width=0.2))+\n    geom_line()+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, fill = Id),color= 'transparent', alpha=0.1)+\n    labs(y='Looking time', x='# trial')+\n    theme_modern(base_size = 20)+\n    theme(legend.position = 'none')+\n    facet_wrap(~Event)\n\n\n\n\n\n\n\nAs you can see here, each color represents a different subject, and we’ve divided the plot into two panels - one for each type of event - to make visualization simpler. Cool isn’t it??.\nNow, you might be thinking, “This looks interesting, but my plot is going to be a mess with all these individual estimates!” Well, don’t worry! While what we’ve plotted is how the data is modeled by our mixed-effects model, the random effects are actually used to make more accurate estimates—but the model still returns an overall estimate.\nThink of it like this: the random effects allow the model to account for individual differences between subjects. But instead of just showing all the individual estimates in the plot, the model takes these individual estimates for each subject and returns the average of these estimates to give you a cleaner, more generalizable result.\nwe can plot the actual estimate of the model:\n\nCodei_pred = estimate_expectation(mod_rintercept, include_random =F)\n\nggplot(i_pred, aes(x= StandardTrialN, y= Predicted))+\n    geom_point(data = df, aes(y= LookingTime, color= Id, shape = Event), position= position_jitter(width=0.2))+\n    geom_line(aes(group= Event),color= 'blue', lwd=1.4)+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, group= Event),color= 'transparent', alpha=0.1)+\n    labs(y='Looking time', x='# trial')+\n    theme_bw(base_size = 20)+\n  theme(legend.position = 'none')+\n  facet_wrap(~Event)\n\n\n\n\n\n\n\nSlope\nCoool!!!!!!! So far, we’ve modeled a different intercept for each subject, which lets each subject have their own baseline level of performance. But here’s the catch: our model assumes that everyone improves over the trials in exactly the same way, with the same slope. That doesn’t sound quite right, does it? We know that some people may get better faster than others, or their learning might follow a different pattern.\nModel\nThis is where we can model random slopes to capture these individual differences in learning rates. By adding (0 + StandardTrialN | Id), we’re telling the model that while the intercept (starting point) is the same for everyone, the rate at which each subject improves (the slope) can vary.\nThis way, we’re allowing each subject to have their own slope in addition to their own intercept, making the model more flexible and reflective of real-world variations in learning!\n\n\n\n\n\n\nCaution\n\n\n\nAny variable used as a random slope (before the |) must also be included as a fixed effect in your model. The fixed effect estimates the overall effect, while the random slope captures how that effect varies across groups. Without the fixed effect, you’re modeling deviations from zero instead of from an average, which rarely makes theoretical sense.\n\n\n\nmod_rslope =lmer(LookingTime ~ StandardTrialN * Event+ (0 + StandardTrialN | Id ), data= df)\nsummary(mod_rslope)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: LookingTime ~ StandardTrialN * Event + (0 + StandardTrialN |      Id)\n   Data: df\n\nREML criterion at convergence: 9136.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3355 -0.6449  0.0801  0.7321  2.7171 \n\nRandom effects:\n Groups   Name           Variance Std.Dev.\n Id       StandardTrialN  2407     49.06  \n Residual                61817    248.63  \nNumber of obs: 660, groups:  Id, 20\n\nFixed effects:\n                            Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)                1310.8278    16.9264  655.9797  77.443  &lt; 2e-16 ***\nStandardTrialN              -43.8937    20.5097   65.7394  -2.140   0.0361 *  \nEventReward                 100.7614    21.0343  652.7701   4.790 2.06e-06 ***\nStandardTrialN:EventReward    0.7738    21.3998  655.1091   0.036   0.9712    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) StndTN EvntRw\nStandrdTrlN  0.354              \nEventReward -0.804 -0.284       \nStndrdTN:ER -0.337 -0.683  0.277\n\n\nThe results aren’t too different from the intercept-only model, but let’s take a closer look at what we’ve actually modeled.\n\nCodes_pred = estimate_expectation(mod_rslope, include_random =T)\n\nggplot(s_pred, aes(x= StandardTrialN, y= Predicted, color= Id, shape = Event))+\n    geom_point(data = df, aes(y= LookingTime, color= Id), position= position_jitter(width=0.2))+\n    geom_line()+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, fill = Id),color= 'transparent', alpha=0.1)+\n    labs(y='Looking time', x='# trial')+\n    theme_modern(base_size = 20)+\n    theme(legend.position = 'none')+\n    facet_wrap(~Event)\n\n\n\n\n\n\n\nIntercept + Slope\nThat plot does look nuts, and it’s a clear signal that something is off. Why? Because by modeling only the random slopes while keeping the intercepts fixed, we’re essentially forcing all subjects to start from the same baseline. That’s clearly unrealistic for most real-world data.\nIn real life, the intercept and slope often go hand-in-hand for each subject.\nModel\nTo make the model more realistic, we can model both the random intercept and the random slope together. We simply modify the random effects part of the formula to (trial_number | subject_id).\nNow, we are telling the model to estimate both a random intercept (baseline performance) and a random slope (rate of improvement). This captures the full variability in how each subject learns over time!\n\nmod_rinterraction = lmer(LookingTime ~ StandardTrialN * Event+ (1 + StandardTrialN | Id ), data= df)\nsummary(mod_rinterraction)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: LookingTime ~ StandardTrialN * Event + (1 + StandardTrialN |      Id)\n   Data: df\n\nREML criterion at convergence: 6948.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.10408 -0.69312 -0.01849  0.66571  2.84041 \n\nRandom effects:\n Groups   Name           Variance Std.Dev. Corr\n Id       (Intercept)    64540    254.05       \n          StandardTrialN  4162     64.51   0.55\n Residual                 1631     40.39       \nNumber of obs: 660, groups:  Id, 20\n\nFixed effects:\n                           Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)                1286.142     56.888   19.076  22.608 3.09e-15 ***\nStandardTrialN              -67.118     14.741   20.203  -4.553 0.000189 ***\nEventReward                 126.844      3.630  619.185  34.940  &lt; 2e-16 ***\nStandardTrialN:EventReward   26.037      3.639  619.368   7.156 2.36e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) StndTN EvntRw\nStandrdTrlN  0.545              \nEventReward -0.044 -0.085       \nStndrdTN:ER -0.022 -0.170  0.346\n\n\nNow, let’s visualize how the model is modeling the data:\n\nCodeis_pred = estimate_expectation(mod_rinterraction, include_random =T)\n\nggplot(is_pred, aes(x= StandardTrialN, y= Predicted, color= Id, shape = Event))+\n    geom_point(data = df, aes(y= LookingTime, color= Id), position= position_jitter(width=0.2))+\n    geom_line()+\n    geom_ribbon(aes(ymin=Predicted-SE, ymax=Predicted+SE, fill = Id),color= 'transparent', alpha=0.1)+\n    labs(y='Looking time', x='# trial')+\n    theme_modern(base_size = 20)+\n    theme(legend.position = 'none')+\n    facet_wrap(~Event)\n\n\n\n\n\n\n\nWhile is not super apparent from the data you can see that different subject have different slopes meaning that they all not grow at the same rate",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearMixedModels.html#summary-of-mixed-models",
    "href": "CONTENT/Stats/LinearMixedModels.html#summary-of-mixed-models",
    "title": "Linear mixed effect modesl",
    "section": "Summary of mixed models",
    "text": "Summary of mixed models\nNow that we’ve seen how to run mixed-effects models, it’s time to focus on interpreting the summary output. While we’ve been building models, we haven’t delved into what the summary actually tells us or which parts of it deserve our attention. Let’s fix that!\nTo start, we’ll use our final model and inspect its summary. This will give us a chance to break it down step by step and understand the key information it provides. Here’s how to check the summary:\n\nsummary(mod_rinterraction)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: LookingTime ~ StandardTrialN * Event + (1 + StandardTrialN |      Id)\n   Data: df\n\nREML criterion at convergence: 6948.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.10408 -0.69312 -0.01849  0.66571  2.84041 \n\nRandom effects:\n Groups   Name           Variance Std.Dev. Corr\n Id       (Intercept)    64540    254.05       \n          StandardTrialN  4162     64.51   0.55\n Residual                 1631     40.39       \nNumber of obs: 660, groups:  Id, 20\n\nFixed effects:\n                           Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)                1286.142     56.888   19.076  22.608 3.09e-15 ***\nStandardTrialN              -67.118     14.741   20.203  -4.553 0.000189 ***\nEventReward                 126.844      3.630  619.185  34.940  &lt; 2e-16 ***\nStandardTrialN:EventReward   26.037      3.639  619.368   7.156 2.36e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) StndTN EvntRw\nStandrdTrlN  0.545              \nEventReward -0.044 -0.085       \nStndrdTN:ER -0.022 -0.170  0.346\n\n\nThe Random effects section in the model summary shows how variability is accounted for by the random effects. The Groups column indicates the grouping factor (e.g., subject), while the Name column lists the random effects (e.g., intercept and slope). The Variance column represents the variability for each random effect—higher values indicate greater variation in how the effect behaves across groups. The Std.Dev. column is simply the standard deviation of the variance, showing the spread in the same units as the data.\nThe Corr column reflects the correlation between random effects, telling us whether different aspects of the data (e.g., intercepts and slopes) tend to move together. A negative correlation would suggest that higher intercepts (starting points) are associated with smaller slopes (slower learning rates), while a positive correlation would suggest the opposite.\nThe Residual section shows the unexplained variability after accounting for the fixed and random effects.\nThe key takeaway here is that random effects capture the variability in the data that can’t be explained by the fixed effects alone. If the variance for a random effect is low, it suggests the random effect isn’t adding much to the model and may be unnecessary. On the other hand, high variance indicates that the random effect is important for capturing group-level differences and improving the model’s accuracy.",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearMixedModels.html#model-comparison",
    "href": "CONTENT/Stats/LinearMixedModels.html#model-comparison",
    "title": "Linear mixed effect modesl",
    "section": "Model comparison",
    "text": "Model comparison\nBut how can we be sure the random effects are helping our model? One of the easiest ways is to check the variance explained by the random effects. As we said if the variance related to the random effects is too small, it probably isn’t contributing much to the model. If it’s high, it’s likely helping the model by capturing important variability in the data.\nAnother method is to compare the performance of different models. One of the best indices for this is the Akaike Information Criterion (AIC). AIC gives a relative measure of how well a model fits the data, while penalizing the number of parameters in the model. Lower AIC values indicate better models, as they balance goodness-of-fit with model complexity.\nYou can compare the AIC of different models using the following:\n\ncompare_performance(mod_lm, mod_rintercept, mod_rslope, mod_rinterraction, metrics='AIC')\n\n# Comparison of Model Performance Indices\n\nName              |           Model |  AIC (weights)\n----------------------------------------------------\nmod_lm            |              lm | 9184.1 (&lt;.001)\nmod_rintercept    | lmerModLmerTest | 7702.4 (&lt;.001)\nmod_rslope        | lmerModLmerTest | 9178.3 (&lt;.001)\nmod_rinterraction | lmerModLmerTest | 6990.1 (&gt;.999)\n\n\nAs you can see, the best model based on AIC is the one with both intercept and slope. This is a good way to check if and which random effect structure is necessary for our model.\n\n\n\n\n\n\nWarning\n\n\n\nNever decide if your random effect structure is good by just looking at p-values! P-values are not necessarily related to how well the model fits your data. Always use model comparison and fit indices like AIC to guide your decision.",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "CONTENT/Stats/LinearMixedModels.html#formulary",
    "href": "CONTENT/Stats/LinearMixedModels.html#formulary",
    "title": "Linear mixed effect modesl",
    "section": "Formulary",
    "text": "Formulary\nIn this tutorial, we introduced linear mixed-effects models. However, these models can be far more versatile and complex than what we’ve just explored. The lme4 package allows you to specify various models to suit diverse research scenarios. While we won’t dive into every possibility, here’s a handy reference for the different random effects structures you can specify\n\n\nFormula\nDescription\n\n\n\n(1|s)\nRandom intercepts for unique level of the factor s.\n\n\n(1|s) + (1|i)\nRandom intercepts for each unique level of s and for each unique level of i.\n\n\n(1|s/i)\nRandom intercepts for factor s and i, where the random effects for i are nested in s. This expands to (1|s) + (1|s:i), i.e., a random intercept for each level of s, and each unique combination of the levels of s and i. Nested random effects are used in so-called multilevel models. For example, s might refer to schools, and i to classrooms within those schools.\n\n\n(a|s)\nRandom intercepts and random slopes for a, for each level of s. Correlations between the intercept and slope effects are also estimated. (Identical to (a*b|s).)\n\n\n(a*b|s)\nRandom intercepts and slopes for a, b, and the a:b interaction, for each level of s. Correlations between all the random effects are estimated.\n\n\n(0+a|s)\nRandom slopes for a for each level of s, but no random intercepts.\n\n\n(a||s)\nRandom intercepts and random slopes for a, for each level of s, but no correlations between the random effects (i.e., they are set to 0). This expands to: (0+a|s) + (1|s).",
    "crumbs": [
      "Stats",
      "Linear mixed effect modesl"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html",
    "href": "CONTENT/Stats/ModelEstimates.html",
    "title": "ModelEstimates",
    "section": "",
    "text": "If you are here it means you have already gone trough our tutorial on: Linear models, Linear mixed effect models and Generalized linear models. Well done!! You are nearly there my young stats padawan.",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html#model",
    "href": "CONTENT/Stats/ModelEstimates.html#model",
    "title": "ModelEstimates",
    "section": "Model",
    "text": "Model\nFirst thing firts, what do we need?? Well a model…\nI’ll be brief here as you should already be familiar with this. Before diving in we import relevant libraries, read the data, set categorical variables as factors, standardize the continuous variables, and finally run our model.\n\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(easystats)\nlibrary(tidyverse)\n\ndf = read.csv(\"../../resources/Stats/Dataset.csv\")\ndf$Id = factor(df$Id) # make sure subject_id is a factor\ndf$StandardTrialN = standardize(df$TrialN) # create standardize trial column\n\nmod = lmer(LookingTime ~ StandardTrialN * Event+ (1 + StandardTrialN | Id ), data= df)\nsummary(mod)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: LookingTime ~ StandardTrialN * Event + (1 + StandardTrialN |      Id)\n   Data: df\n\nREML criterion at convergence: 6948.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.10408 -0.69312 -0.01849  0.66571  2.84041 \n\nRandom effects:\n Groups   Name           Variance Std.Dev. Corr\n Id       (Intercept)    64540    254.05       \n          StandardTrialN  4162     64.51   0.55\n Residual                 1631     40.39       \nNumber of obs: 660, groups:  Id, 20\n\nFixed effects:\n                           Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)                1286.142     56.888   19.076  22.608 3.09e-15 ***\nStandardTrialN              -67.118     14.741   20.203  -4.553 0.000189 ***\nEventReward                 126.844      3.630  619.185  34.940  &lt; 2e-16 ***\nStandardTrialN:EventReward   26.037      3.639  619.368   7.156 2.36e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) StndTN EvntRw\nStandrdTrlN  0.545              \nEventReward -0.044 -0.085       \nStndrdTN:ER -0.022 -0.170  0.346\n\n\nThis is not a tutorial about the model itself—you’ve already done that, right? 😉 The only thing to keep in mind is that in our model we have main effect of the predictors and their interaction.",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html#predictions",
    "href": "CONTENT/Stats/ModelEstimates.html#predictions",
    "title": "ModelEstimates",
    "section": "Predictions",
    "text": "Predictions\nNow we can extract the predictions from our model. We want to see what our model predicts the values of Looking time would be for each row of our dataframe. Here is where the Easystats package (specifically the modelbased sub-package) comes to the rescue with the function estimate_expectation().\n\nPred = estimate_expectation(mod)\nhead(Pred)\n\nModel-based Predictions\n\nStandardTrialN | Event    | Id | Predicted |    SE |             95% CI | Residuals\n-----------------------------------------------------------------------------------\n-1.65          | NoReward | 1  |   1136.00 | 48.16 | [1041.43, 1230.56] |      3.81\n-1.47          | NoReward | 1  |   1117.24 | 48.58 | [1021.84, 1212.63] |     39.18\n-1.30          | NoReward | 1  |   1098.48 | 49.14 | [1002.00, 1194.96] |    -24.93\n-1.13          | NoReward | 1  |   1079.72 | 49.81 | [ 981.91, 1177.54] |     26.85\n-0.95          | NoReward | 1  |   1060.96 | 50.61 | [ 961.58, 1160.35] |    -70.41\n-0.78          | NoReward | 1  |   1042.21 | 51.53 | [ 941.03, 1143.38] |     11.45\n\nVariable predicted: LookingTime\n\n\nLook!! The function gave us a new dataframe with all the levels of Event and of StandardTrialN and ID that we had in our dataframe. OKOK I will agree….this looks so similar to the original dataframe we had…so why even bother???? Can’t we just use the original dataframe?? Well because the predictions represent what our model thinks the data should look like based on the patterns it discovered, after accounting for random effects and noise!\nLet’s visualize the difference between the raw data and the predictions\n\n\n\n\n\n\n\n\nAs you can see the raw data and the predicted data are very similar but not the same!! The predictions represent our model’s “best guess” at what the looking times should be based on the fixed effects (StandardTrialN and Event) and random effects (individual participant differences). This smooths out some of the noise in the original data and gives us a clearer picture of the underlying patterns!",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html#predictions-for-reference-values",
    "href": "CONTENT/Stats/ModelEstimates.html#predictions-for-reference-values",
    "title": "ModelEstimates",
    "section": "Predictions for reference values",
    "text": "Predictions for reference values\nSo far, we’ve extracted predictions based on our raw data. While useful for comparing model predictions with actual observations, this approach has limitations when we want to visualize and understand core patterns.\nWhen visualizing model results using predictions from every observation:\n\nEach participant would have their own line\nIndividual variations would create visual clutter\nThe main effects could get lost in the noise\n\nWe need a way to highlight the key patterns our model has discovered! Here comes the by argument to the rescue!!! The by argument allows us to create predictions on a reference subset of predictor values, focusing only on the variables we’re interested in exploring.\nFactors\n\nestimate_expectation(mod, by = 'Event')\n\nModel-based Predictions\n\nEvent    | Predicted |    SE |                 CI\n-------------------------------------------------\nNoReward |   1297.28 | 55.59 | [1188.12, 1406.44]\nReward   |   1419.80 | 55.56 | [1310.71, 1528.90]\n\nVariable predicted: LookingTime\nPredictors modulated: Event\nPredictors controlled: StandardTrialN (-0.17)\n\n\nThis gives us predictions for each level of the Event factor while:\n\nSetting StandardTrialN to its mean value\nSetting random effects to zero (removing participant-specific variation)\n\nThe result? A clean representation of the “typical” looking time for each Event type, perfect for visualizing the main effect!\n\n\n\n\n\n\nTip\n\n\n\nIn this examples (and the next ones), we explored a very simple case. We just passed the predictors we were interested in, and the function gave us all the levels of that predictor. However, we can also specify exactly which level we want instead of having the function automatically return all the ones it thinks are relevant. There are different ways to do this - the simplest is:\n\nestimate_expectation(mod, by = 'Event == Reward')\n\nModel-based Predictions\n\nPredicted\n---------\n1297.28  \n\nVariable predicted: LookingTime\nPredictors modulated: Event == Reward\nPredictors controlled: StandardTrialN (-0.17), Event (NoReward)\n\n\nRemember to learn more at the modelbased website !!\n\n\nContinuous\nWhen we pass a continuous variable to the by argument, the function automatically extracts a range of values across that predictor. We can control how many values to sample using the length parameter. For example, setting length=3 will give us predictions at just three points along the range of our continuous variable, while a larger value would provide a more detailed sampling.\n\nestimate_expectation(mod, by = 'StandardTrialN', length=3)\n\nModel-based Predictions\n\nStandardTrialN | Predicted\n--------------------------\n-1.65          |   1396.62\n0.00           |   1286.14\n1.65           |   1175.67\n\nVariable predicted: LookingTime\nPredictors modulated: StandardTrialN\nPredictors controlled: Event (NoReward)\n\nestimate_expectation(mod, by = 'StandardTrialN', length=6)\n\nModel-based Predictions\n\nStandardTrialN | Predicted\n--------------------------\n-1.65          |   1396.62\n-0.99          |   1352.45\n-0.33          |   1308.22\n0.33           |   1264.06\n0.99           |   1219.83\n1.65           |   1175.67\n\nVariable predicted: LookingTime\nPredictors modulated: StandardTrialN\nPredictors controlled: Event (NoReward)\n\n\nAs you can see from function messages, the function can’t meaningfully “average” a categorical variable like Event, so it uses its reference level (NoReward in our case) for making predictions.\nFactor x Continuous\nWant to see how factors and continuous variables interact? Simply include both in the by argument:\n\nestimate_expectation(mod, by = c('StandardTrialN','Event'), length=6)\n\nModel-based Predictions\n\nStandardTrialN | Event    | Predicted |    SE |                 CI\n------------------------------------------------------------------\n-1.65          | NoReward |   1396.62 | 48.16 | [1302.05, 1491.18]\n-0.99          | NoReward |   1352.45 | 50.44 | [1253.40, 1451.50]\n-0.33          | NoReward |   1308.22 | 54.39 | [1201.41, 1415.03]\n0.33           | NoReward |   1264.06 | 59.67 | [1146.89, 1381.23]\n0.99           | NoReward |   1219.83 | 65.97 | [1090.29, 1349.37]\n1.65           | NoReward |   1175.67 | 73.01 | [1032.30, 1319.03]\n-1.65          | Reward   |   1480.60 | 48.12 | [1386.11, 1575.09]\n-0.99          | Reward   |   1453.57 | 50.43 | [1354.54, 1552.61]\n-0.33          | Reward   |   1426.50 | 54.37 | [1319.74, 1533.26]\n0.33           | Reward   |   1399.47 | 59.60 | [1282.44, 1516.50]\n0.99           | Reward   |   1372.40 | 65.83 | [1243.14, 1501.66]\n1.65           | Reward   |   1345.37 | 72.78 | [1202.45, 1488.28]\n\nVariable predicted: LookingTime\nPredictors modulated: StandardTrialN, Event\n\n\nThis creates predictions for all combinations of Event levels and StandardTrialN values - perfect for visualizing interaction effects!\n\n\n\n\n\n\nTip\n\n\n\nIn this tutorial we focused on the estimate_expectation() function. However, modelbased offers additional functions that return similar things but with slightly different defaults and behaviors.\nFor example, if we run estimate_relation(mod), instead of returning predictions based on the values in our original dataframe, it will automatically return predictions on a reference grid of all predictors. This is equivalent to running estimate_expectation(mod, by = c('StandardTrialN','Event')) that we saw before.\nThese functions accomplish very similar tasks but may be more convenient depending on what you’re trying to do. The different function names reflect their slightly different default behaviors, saving you time when you need different types of predictions. Don’t be intimidated by these differences - explore them further at the modelbased website to learn them better.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAll these predictions allow for simple direct plotting. These are ggplot objects that can be customized by adding other arguments (like changing the theme). Note that there’s a limit to how well these automatic plots handle complexity - with too many predictors, you might need to create custom plots instead.\n\nplot(\n  estimate_expectation(mod, by = c('StandardTrialN','Event'), length=10))+\n  theme_classic(base_size = 20)+\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\nI hope you now have a solid grasp of predictions and how to extract them at specific predictor levels! This powerful approach gives you the flexibility to explore and visualize your eye-tracking data in meaningful ways.",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html#plot",
    "href": "CONTENT/Stats/ModelEstimates.html#plot",
    "title": "ModelEstimates",
    "section": "Plot",
    "text": "Plot\nThe estimate_means() function also comes with its own built-in plotting capability, automatically generating a clean ggplot visualization of your estimated means.\n\nplot(Est_means)+\n  theme_classic(base_size = 20)+\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nThis creates a simple yet informative ggplot showing our estimated means with confidence intervals. Of course, we can also use the dataframe that the function returns to create more customized and refined plots if needed.",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html#plot-1",
    "href": "CONTENT/Stats/ModelEstimates.html#plot-1",
    "title": "ModelEstimates",
    "section": "Plot",
    "text": "Plot\nestimate_contrast() has a plotting function as well! However, it’s slightly more complex as the plot function requires both our estimated contrasts and means. If we pass both, the plot will show which level is different from the other using lighthouse plots - visualizations that highlight significant differences between groups.\n\nplot(Contrast, Est_means)+\n  theme_classic(base_size = 20)+\n  theme(legend.position = 'bottom')",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html#factor-x-continuous-1",
    "href": "CONTENT/Stats/ModelEstimates.html#factor-x-continuous-1",
    "title": "ModelEstimates",
    "section": "Factor x Continuous",
    "text": "Factor x Continuous\nWhile estimate_contrast() is usually very useful for checking differences between levels of a categorical variable, it can also be used to estimate contrasts for continuous variables. However, where in our opinion the function really shines is when examining interactions between categorical and continuous predictors like we have in our model!!!\n\nContrastbyTrial = estimate_contrasts(mod, contrast = 'Event', by = 'StandardTrialN', p_adjust = 'hochberg')\nContrastbyTrial\n\nMarginal Contrasts Analysis\n\nLevel1 | Level2   | StandardTrialN | Difference |   SE |           95% CI\n-------------------------------------------------------------------------\nReward | NoReward |          -1.65 |      83.99 | 5.83 | [ 72.53,  95.44]\nReward | NoReward |          -1.28 |      93.49 | 4.82 | [ 84.03, 102.95]\nReward | NoReward |          -0.92 |     103.02 | 3.99 | [ 95.19, 110.85]\nReward | NoReward |          -0.55 |     112.55 | 3.49 | [105.70, 119.40]\nReward | NoReward |          -0.18 |     122.08 | 3.46 | [115.29, 128.87]\nReward | NoReward |           0.18 |     131.61 | 3.91 | [123.93, 139.29]\nReward | NoReward |           0.55 |     141.14 | 4.71 | [131.89, 150.39]\nReward | NoReward |           0.92 |     150.67 | 5.71 | [139.45, 161.88]\nReward | NoReward |           1.28 |     160.20 | 6.83 | [146.79, 173.60]\nReward | NoReward |           1.65 |     169.70 | 8.01 | [153.98, 185.42]\n\nLevel1 | t(652) |      p\n------------------------\nReward |  14.40 | &lt; .001\nReward |  19.41 | &lt; .001\nReward |  25.83 | &lt; .001\nReward |  32.28 | &lt; .001\nReward |  35.31 | &lt; .001\nReward |  33.65 | &lt; .001\nReward |  29.97 | &lt; .001\nReward |  26.38 | &lt; .001\nReward |  23.47 | &lt; .001\nReward |  21.20 | &lt; .001\n\nVariable predicted: LookingTime\nPredictors contrasted: Event\nPredictors averaged: Id\np-value adjustment method: Hochberg (1988)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhile we’ve been using contrast to specify one predictor and by for another, estimate_contrasts() is more flexible than we’ve shown. The contrast = argument can accept multiple predictors, calculating contrasts between all combinations of their levels. You can also mix and match with multiple predictors in contrast = while still using the by = argument to see how these combined contrasts change across levels of another variable. While this generates many comparisons at once (which isn’t always desirable), it can be valuable for exploring complex interaction patterns in your data.\nThe word is your contrast!\n\n\nThis gives us the contrast between the levels of Event for a set of values of StandardTrialN. So we can check whether the difference between Reward and NoReward actually changes over the course of trials!! I’ll plot it to make it simpler to understand:\n\n\n\n\n\n\n\n\nThis plot shows that the two levels are always significantly different from each other (the confidence intervals never touch the dashed zero line) and that the difference is always positive - looking time for Reward is consistently higher than for NoReward across all trial numbers. SUUPER COOL EH!!! I agree!!",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html#contrast-of-slopes",
    "href": "CONTENT/Stats/ModelEstimates.html#contrast-of-slopes",
    "title": "ModelEstimates",
    "section": "Contrast of slopes",
    "text": "Contrast of slopes\nOK now we know the difference between levels of Event and also how this difference may change over time… the last step? We could check whether the slopes of StandardTrialN is different between the two conditions!\nTo do so, we can again use the estimate_contrast() function but inverting the arguments we used last time. So StandardTrialN moves to the contrast argument and Event goes to the by argument. Super easy:\n\nestimate_contrasts(mod, contrast = 'StandardTrialN', by = 'Event')\n\nMarginal Contrasts Analysis\n\nLevel1 | Level2   | Difference |   SE |         95% CI |    t |      p\n----------------------------------------------------------------------\nReward | NoReward |      26.04 | 3.64 | [18.90, 33.17] | 7.15 | &lt; .001\n\nVariable predicted: LookingTime\nPredictors contrasted: StandardTrialN\nPredictors averaged: StandardTrialN (-0.17), Id\np-values are uncorrected.\n\n\nThis shows us that the effect of StandardTrialN is actually different between the two levels. This is something we already knew from the model summary (remember the significant interaction term?), but this approach gives us the precise difference between the two slopes while averaging over all other possible effects. This becomes particularly valuable when working with more complex models that have multiple predictors and interactions.",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html#factor-x-continuous-2",
    "href": "CONTENT/Stats/ModelEstimates.html#factor-x-continuous-2",
    "title": "ModelEstimates",
    "section": "Factor x Continuous",
    "text": "Factor x Continuous\nSimilar to the estimate_contrast function, estimate_slopes really shines when exploring interactions between continuous and categorical variables.\nRemember! Our model indicated an interaction between StandardTrialN and Event, which means the rate of change (slope) should differ between the two Event types. However, from just knowing there’s an interaction, we can’t determine exactly what these slopes are doing. One condition might show a steeper decline than the other, one might be flat while the other decreases, or they might even go in opposite directions. The model just tells us they’re different, not how they’re different (or at least is not so simple)n\nTo visualize exactly how these slopes differ, we can include Event with the by argument:\n\nSlopes = estimate_slopes(mod, trend = 'StandardTrialN', by = 'Event')\nSlopes\n\nEstimated Marginal Effects\n\nEvent    |  Slope |    SE |           95% CI |     t |      p\n-------------------------------------------------------------\nNoReward | -67.12 | 14.73 | [-95.98, -38.26] | -4.56 | &lt; .001\nReward   | -41.08 | 14.56 | [-69.61, -12.55] | -2.82 |  0.005\n\nMarginal effects estimated for StandardTrialN\nType of slope was dY/dX\n\n\nNow we get the average effect for both Events. So we see that both of the lines significantly decrease!\nWe can also plot it with:\n\nplot(Slopes)+\n  geom_hline(yintercept = 0, linetype = 'dashed')+\n  theme_classic(base_size = 20)+\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nHere we see that the slopes for both Event types are negative (below zero) and statistically significant (confidence intervals don’t cross the dashed zero line). This confirms that looking time reliably decreases as trials progress, regardless of whether it’s a Reward or NoReward event.",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html#datagrid",
    "href": "CONTENT/Stats/ModelEstimates.html#datagrid",
    "title": "ModelEstimates",
    "section": "Datagrid",
    "text": "Datagrid\nThe first step in custom plotting is creating the right reference grid for your predictions. We’ll use the get_datagrid() function, which takes your model as its first argument and the by = parameter to specify which predictors you want in your visualization.\nLet’s extract a reference grid for our StandardTrialN * Event interaction:\n\nget_datagrid(mod, by = c('Event', 'StandardTrialN'))\n\nVisualisation Grid\n\nEvent    | StandardTrialN | Id\n------------------------------\nNoReward |          -1.65 |  0\nNoReward |          -1.28 |  0\nNoReward |          -0.92 |  0\nNoReward |          -0.55 |  0\nNoReward |          -0.18 |  0\nNoReward |           0.18 |  0\nNoReward |           0.55 |  0\nNoReward |           0.92 |  0\nNoReward |           1.28 |  0\nNoReward |           1.65 |  0\nReward   |          -1.65 |  0\nReward   |          -1.28 |  0\nReward   |          -0.92 |  0\nReward   |          -0.55 |  0\nReward   |          -0.18 |  0\nReward   |           0.18 |  0\nReward   |           0.55 |  0\nReward   |           0.92 |  0\nReward   |           1.28 |  0\nReward   |           1.65 |  0\n\n\nLooking at the output, you’ll see we get both levels of Event (Reward and NoReward) and a range of values for StandardTrialN (10 values by default spanning the range of our data). Notice that the Id column is set to 0. This is how the function indicates that random effects aren’t included—we’re focusing on the fixed effects that represent the “average response” across all subjects.\n\n\n\n\n\n\nCaution\n\n\n\nDifferent models declare non-level for the random effects in different ways, but datagrid will adapt to the specific model you are running. Just do not be scared if the values are not 0.\n\n\nThe datagrid is where we can really flex our customization muscles. Instead of accepting the default values, we can request specific levels of a factor, custom ranges for continuous variables, or statistical landmarks of continuous variables.\nFor example, instead of the default range of values, we could request 'StandardTrialN = [quartiles]' which would give us the lower-hinge, median, and upper-hinge of the continuous variable. Cool, right? There are many functions and statistical aspects we can extract - check the documentation of get_datagrid() for the full range of possibilities.\nLet’s create a more sophisticated datagrid to demonstrate:\n\nGrid = get_datagrid(mod, by = c('Event', 'StandardTrialN = [fivenum]'))\nhead(as.data.frame(Grid), n=20 )\n\n      Event StandardTrialN Id\n1  NoReward         -1.646  0\n2  NoReward         -0.953  0\n3  NoReward         -0.260  0\n4  NoReward          0.607  0\n5  NoReward          1.646  0\n6    Reward         -1.646  0\n7    Reward         -0.953  0\n8    Reward         -0.260  0\n9    Reward          0.607  0\n10   Reward          1.646  0\n\n\nWhat we’ve done here is request both levels of Event, the five-number summary for StandardTrialN (minimum, lower-hinge, median, upper-hinge, maximum), and all different subjects by setting include_random = TRUE. Why would we want such a complex grid? Well first thing first to make it copmlex…… and aso we want to see what is happening for each subject!! Let’s now use this grid……",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html#prediction-on-grid",
    "href": "CONTENT/Stats/ModelEstimates.html#prediction-on-grid",
    "title": "ModelEstimates",
    "section": "Prediction on grid",
    "text": "Prediction on grid\nNow that we have our grid, we can pass it to the get_predicted() function. This will give us all the predictions on the grid!!\n\nget_predicted(mod, Grid, ci = T)\n\nPredicted values:\n\n [1] 1396.617 1350.105 1303.592 1245.401 1175.666 1480.604 1452.135 1423.666\n [9] 1388.049 1345.367\n\nNOTE: Confidence intervals, if available, are stored as attributes and can be accessed using `as.data.frame()` on this output.\n\n\nAs you notice, this gives us a vector of predictions, but we also need the confidence intervals to make a proper plot. As the message mentions, those are ‘hidden’ and need to be accessed using as.data.frame().\n\nPred = as.data.frame(get_predicted(mod, Grid, ci = T))\nhead(Pred)\n\n  Predicted       SE CI_low CI_high\n1  1396.617 48.15794   -Inf     Inf\n2  1350.105 50.61384   -Inf     Inf\n3  1303.592 54.89119   -Inf     Inf\n4  1245.401 62.22156   -Inf     Inf\n5  1175.666 73.01111   -Inf     Inf\n6  1480.604 48.12085   -Inf     Inf\n\n\nNow we can merge the two to have a final dataframe that has all the information:\n\ndb = bind_cols(Grid, Pred)\nhead(db)\n\nVisualisation Grid\n\nEvent    | StandardTrialN | Id | Predicted |    SE | CI_low | CI_high\n---------------------------------------------------------------------\nNoReward |          -1.65 |  0 |   1396.62 | 48.16 |   -Inf |     Inf\nNoReward |          -0.95 |  0 |   1350.10 | 50.61 |   -Inf |     Inf\nNoReward |          -0.26 |  0 |   1303.59 | 54.89 |   -Inf |     Inf\nNoReward |           0.61 |  0 |   1245.40 | 62.22 |   -Inf |     Inf\nNoReward |           1.65 |  0 |   1175.67 | 73.01 |   -Inf |     Inf\nReward   |          -1.65 |  0 |   1480.60 | 48.12 |   -Inf |     Inf",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Stats/ModelEstimates.html#plot-2",
    "href": "CONTENT/Stats/ModelEstimates.html#plot-2",
    "title": "ModelEstimates",
    "section": "Plot",
    "text": "Plot\nNow we have our dataframe we can do whatever we want with it. Let’s plot it:\n\nggplot(db, aes(x = StandardTrialN, y = Predicted, color = Event, fill = Event ))+\n  geom_ribbon(aes(ymin = Predicted-SE, ymax = Predicted+SE), alpha = .4)+\n  geom_line(lwd = 1.2 )+\n  theme_classic(base_size = 20)+\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nWell, you did it! You’ve survived the wild jungle of model-based estimation! By breaking down the prediction process into customizable steps, you’ve gained complete control over how to extract and visualize the patterns in your eye-tracking data. Whether you use the convenient built-in functions or create custom plots from scratch, you now have the tools to answer sophisticated research questions and present your findings beautifully. Happy modeling!",
    "crumbs": [
      "Stats",
      "ModelEstimates"
    ]
  },
  {
    "objectID": "CONTENT/Workshops/GAP_2024.html",
    "href": "CONTENT/Workshops/GAP_2024.html",
    "title": "Bridging the Technological Gap Workshop (GAP)",
    "section": "",
    "text": "Hello hello!!! This page has been created to provide support and resources for the tutorial that will take place during the Bridging the Technological Gap Workshop."
  },
  {
    "objectID": "CONTENT/Workshops/GAP_2024.html#python",
    "href": "CONTENT/Workshops/GAP_2024.html#python",
    "title": "Bridging the Technological Gap Workshop (GAP)",
    "section": "Python",
    "text": "Python\nIn this tutorial, our primary tool will be Python!! There are lots of ways to install python. We recommend installing it via Miniconda. However, for this workshop, the suggested way to install Python is using Anaconda.\nYou might ask….Then which installation should I follow? Well, it doesn’t really matter! Miniconda is a minimal installation of Anaconda. It lacks the GUI, but has all the main features. So follow whichever one you like more!\nOnce you have it installed, we need a few more things. For the Gaze Tracking & Pupillometry Workshop (the part we will be hosting) we will need some specific libraries and files. We have tried our best to make everything as simple as possible:\n\nLibraries\nWe will be working with a conda environment (a self-contained directory that contains a specific collection of Python packages and dependencies, allowing you to manage different project requirements separately). To create this environment and install all the necessary libraries, all you need is this file:\n Psychopy.yml \nOnce you have downloaded the file, simply open the anaconda/miniconda terminal and type conda env create -f, then simply drag and drop the downloaded file onto the terminal. This will copy the filename with its absolute path. In my case it looked something like this:\n\n\n\n\n\nNow you will be asked to confirm a few things (by pressing Y) and after a while of downloading and installing you will have your new workshop environment called Psychopy!\nNow you should see a shortcut in your start menu called Spyder(psychopy), just click on it to open spyder in our newly created environment. If you don’t see it, just reopen the anaconda/miniconda terminal, activate your new environment by typing conda activate psychopy and then just type spyder.\n\n\nFiles\nWe also need some files if you want to run the examples with us. Here you can download the zip files with everything you need:\n Files \nOnce downloaded, simply extract the file by unzipping it. For our workshop we will work together in a folder that should look like this:\n\n\n\n\n\nIf you have a similar folder… you are ready to go!!!!"
  },
  {
    "objectID": "CONTENT/Workshops/GAP_2024.html#videos",
    "href": "CONTENT/Workshops/GAP_2024.html#videos",
    "title": "Bridging the Technological Gap Workshop (GAP)",
    "section": "Videos",
    "text": "Videos\nWe received several questions about working with videos and PsychoPy while doing eye-tracking. It can be quite tricky, but here are some tips:\n\nMake sure you’re using the right codec.\nIf you need to change the codec of the video, you can re-encode it using a tool like\nHandbrake (remember to set the constant framerate in the video option)\n\nBelow, you’ll find a code example that adapts our Create an eye-tracking experiment tutorial to work with a video file. The main differences are:\n\nWe’re showing a video after the fixation.\nWe’re saving triggers to our eye-tracking data and also saving the frame index at each sample (as a continuous number column).\n\n\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\n\n# Import some libraries from PsychoPy\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound\n\nimport tobii_research as tr\n\n\n#%% Functions\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global gaze_data_buffer\n    global winsize\n    global frame_indx\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,trigger, frame_indx))\n    trigger = ''\n    \ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event', 'FrameIndex']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = not os.path.isfile(output_path)\n    \n    # Write the DataFrame to an HDF5 file\n    out.to_csv(output_path, mode='a', index =False, header = file_exists)\n    \n    \n    \n#%% Load and prepare stimuli\n\nos.chdir(r'C:\\Users\\tomma\\Desktop\\EyeTracking\\Files')\n\n# Winsize\nwinsize = (960, 540)\n\n# create a window\nwin = visual.Window(size = winsize,fullscr=False, units=\"pix\", screen=0)\n\n\n# Load images and video\nfixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\nVideo = visual.MovieStim(win, filename='EXP\\\\Stimuli\\\\Video60.mp4',  loop=False, size=[600,380],volume =0.4, autoStart=True)  \n\n\n# Define the trigger and frame index variable to pass to the gaze_data_callback\ntrigger = ''\nframe_indx = np.nan\n\n\n\n#%% Record the data\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\n#Start recording\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\n# Crate empty list to append data\ngaze_data_buffer = []\n\nTrials_number = 10\nfor trial in range(Trials_number):\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n    \n    fixation.draw()\n    win.flip()\n    trigger = 'Fixation'\n    core.wait(1)  # wait for 1 second\n\n    Video.play()\n    trigger = 'Video'\n    while not Video.isFinished:\n\n        # Draw the video frame\n        Video.draw()\n\n        # Flip the window and add index to teh frame_indx\n        win.flip()\n        \n        # add which frame was just shown to the eyetracking data\n        frame_indx = Video.frameIndex\n        \n    Video.stop()\n    win.flip()\n\n\n    ### ISI\n    win.flip()    # we re-flip at the end to clean the window\n    clock = core.Clock()\n    write_buffer_to_file(gaze_data_buffer, 'DATA\\\\RAW\\\\Test.csv')\n    while clock.getTime() &lt; 1:\n        pass\n    \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        Eyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\n        core.quit()  # stop study\n      \nwin.close() # close window\nEyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\ncore.quit() # stop study"
  },
  {
    "objectID": "CONTENT/Workshops/GAP_2024.html#calibration",
    "href": "CONTENT/Workshops/GAP_2024.html#calibration",
    "title": "Bridging the Technological Gap Workshop (GAP)",
    "section": "Calibration",
    "text": "Calibration\nWe received a question about the calibration. How to change the focus time that the eye-tracking uses to record samples for each calibration point. Luckily, the function from the Psychopy_tobii_infant repository allows for an additional argument that specifies how long we want the focus time (default = 0.5s). Thus, you can simply change it by running it with a different value.\nHere below we changed the example of Calibrating eye-tracking by increasing the focus_time to 2s. You can increase or decrease it based on your needs!!\n\nimport os\nfrom psychopy import visual, sound\n\n# import Psychopy tobii infant\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\nfrom psychopy_tobii_infant import TobiiInfantController\n\n\n#%% window and stimuli\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\n\n#%% Center face - screen\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n#%% Calibration\n\n# define calibration points\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound, focus_time=2)\nwin.flip()"
  }
]