[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "DevStart is an online hands-on manual for anyone who is approaching developmental psychology and developmental cognitive neuroscience for the first time, from master’s students and PhDs to postdocs. \nAssuming no a priori knowledge, this website will guide you through your first steps as a developmental researcher. You’ll find many examples and guidelines on how to grasp the basic principles of developmental psychology research, design and set up a study with different research methods, analyse data, and start programming.\n\n\nThere are many resources on the web to learn how to program, build experiments, and analyse data. However, they often assume basic skills in programming, and they don’t tackle the challenges that we encounter when testing babies and young children. This knowledge is often handed down from Professors and Postdocs to PhD students, or it is acquired through hours and hours of despair. \nWe tried to summarize all the struggles that we encountered during our PhDs and the solutions we came up with. With this website, we aim to offer you a solution to overcome these problems, and we invite anyone to help us and contribute to this open science framework! \n\n\n\nIf you’re an expert on a specific topic and you want to change something or add something, if you don’t agree with our pipeline, or you spot an error in our code, please get in touch! \nYou can find us at:\n\n t.ghilardi@bbk.ac.uk\n\n francesco.poli@donders.ru.nl\n\n g.serino@bbk.ac.uk\n\n\n\n\n\nThis is a story of bullying and love started in 2018 at the Donders Institute in the Netherlands. Tommaso and Francesco were living their best PhD life when Giulia bumped into them, looking for data to write her master’s thesis.\nUnited by a strong passion for research and a creepy attraction to complex and noisy datasets, they immediately got along well. We haven’t stopped collaborating since then.\n\n\n\n\n\n\n\n\n\n\nTommaso Ghilardi is the programmer behind the scenes. He excels in Python and is known for his impeccably clean code. If you explore our website, you’re likely to come across his contributions. Tommaso’s skills aren’t limited to programming—he’s capable of setting up a lab in just an hour. He’s equally comfortable with research methods and a variety of programming languages. However, be cautious about imperfect code; Tommaso has a short fuse for anything less than perfect. He plays a significant role in shaping this website, making it what it is today. Thank you, Tommaso!\n\n\n\nPostdoc at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://tommasoghilardi.github.io\n t.ghilardi@bbk.ac.uk\n @TommasoGhi\n\n\n\n\n\n\n\n\n\n\n\n\nFrancesco Poli is the deep thinker among us. He’s not just any ordinary thinker; he can build computational models to predict behavior and decipher your thoughts in the blink of an eye. Francesco’s abilities extend to creating intricate mathematical theories to explain behavior patterns. He’s adept at juggling multiple projects, though he might occasionally forget to reply to emails amidst his busy schedule. With Francesco, expect the extraordinary.\n\n\n\nPostDoc at the Donders Centre for Cognition.\n https://www.ru.nl/en/people/poli-f\n francesco.poli@donders.ru.nl\n @FrancescPoli\n\n\n\n\n\n\n\n\n\n\n\n\nGiulia Serino brings imagination to the table. Introduced to programming by Francesco and Tommaso, she has surpassed them to become a skilled programmer herself. Her experiments are a blend of art and science, often resembling graphic masterpieces. However, Giulia tends to get lost in the minutiae of her work. She has a reputation for disappearing, yet her curiosity leads her to explore every corner of Google and various repositories, piecing together her unique theories on attention development.\n\n\n\nPhD student in Developmental Cognitive Neuroscience at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://cbcd.bbk.ac.uk/people/students/giulia-serino\n g.serino@bbk.ac.uk\n @GiSerino\n\n\n\n\nWe tried our best to offer the best and most accurate solutions. However, do always check the code and if the outputs make sense. Please get in touch if you spot any errors.\nWe apologize in advance for our poor coding skills. Our scripts are not perfect, and they don’t mean to be. But, as Francesco always says, they work! And we hope they will support you during your PhD.",
    "crumbs": [
      "Welcome to DevStart"
    ]
  },
  {
    "objectID": "index.html#why-did-we-create-it",
    "href": "index.html#why-did-we-create-it",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "There are many resources on the web to learn how to program, build experiments, and analyse data. However, they often assume basic skills in programming, and they don’t tackle the challenges that we encounter when testing babies and young children. This knowledge is often handed down from Professors and Postdocs to PhD students, or it is acquired through hours and hours of despair. \nWe tried to summarize all the struggles that we encountered during our PhDs and the solutions we came up with. With this website, we aim to offer you a solution to overcome these problems, and we invite anyone to help us and contribute to this open science framework!",
    "crumbs": [
      "Welcome to DevStart"
    ]
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "If you’re an expert on a specific topic and you want to change something or add something, if you don’t agree with our pipeline, or you spot an error in our code, please get in touch! \nYou can find us at:\n\n t.ghilardi@bbk.ac.uk\n\n francesco.poli@donders.ru.nl\n\n g.serino@bbk.ac.uk",
    "crumbs": [
      "Welcome to DevStart"
    ]
  },
  {
    "objectID": "index.html#who-are-we",
    "href": "index.html#who-are-we",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "This is a story of bullying and love started in 2018 at the Donders Institute in the Netherlands. Tommaso and Francesco were living their best PhD life when Giulia bumped into them, looking for data to write her master’s thesis.\nUnited by a strong passion for research and a creepy attraction to complex and noisy datasets, they immediately got along well. We haven’t stopped collaborating since then.\n\n\n\n\n\n\n\n\n\n\nTommaso Ghilardi is the programmer behind the scenes. He excels in Python and is known for his impeccably clean code. If you explore our website, you’re likely to come across his contributions. Tommaso’s skills aren’t limited to programming—he’s capable of setting up a lab in just an hour. He’s equally comfortable with research methods and a variety of programming languages. However, be cautious about imperfect code; Tommaso has a short fuse for anything less than perfect. He plays a significant role in shaping this website, making it what it is today. Thank you, Tommaso!\n\n\n\nPostdoc at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://tommasoghilardi.github.io\n t.ghilardi@bbk.ac.uk\n @TommasoGhi\n\n\n\n\n\n\n\n\n\n\n\n\nFrancesco Poli is the deep thinker among us. He’s not just any ordinary thinker; he can build computational models to predict behavior and decipher your thoughts in the blink of an eye. Francesco’s abilities extend to creating intricate mathematical theories to explain behavior patterns. He’s adept at juggling multiple projects, though he might occasionally forget to reply to emails amidst his busy schedule. With Francesco, expect the extraordinary.\n\n\n\nPostDoc at the Donders Centre for Cognition.\n https://www.ru.nl/en/people/poli-f\n francesco.poli@donders.ru.nl\n @FrancescPoli\n\n\n\n\n\n\n\n\n\n\n\n\nGiulia Serino brings imagination to the table. Introduced to programming by Francesco and Tommaso, she has surpassed them to become a skilled programmer herself. Her experiments are a blend of art and science, often resembling graphic masterpieces. However, Giulia tends to get lost in the minutiae of her work. She has a reputation for disappearing, yet her curiosity leads her to explore every corner of Google and various repositories, piecing together her unique theories on attention development.\n\n\n\nPhD student in Developmental Cognitive Neuroscience at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://cbcd.bbk.ac.uk/people/students/giulia-serino\n g.serino@bbk.ac.uk\n @GiSerino",
    "crumbs": [
      "Welcome to DevStart"
    ]
  },
  {
    "objectID": "index.html#warnings",
    "href": "index.html#warnings",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "We tried our best to offer the best and most accurate solutions. However, do always check the code and if the outputs make sense. Please get in touch if you spot any errors.\nWe apologize in advance for our poor coding skills. Our scripts are not perfect, and they don’t mean to be. But, as Francesco always says, they work! And we hope they will support you during your PhD.",
    "crumbs": [
      "Welcome to DevStart"
    ]
  },
  {
    "objectID": "CONTENT/Workshops/BCCCD2024.html",
    "href": "CONTENT/Workshops/BCCCD2024.html",
    "title": "BCCCD2024",
    "section": "",
    "text": "Hello hello!!! This page has been created to provide support and resources for the tutorial and we presented at BCCCD24."
  },
  {
    "objectID": "CONTENT/Workshops/BCCCD2024.html#software",
    "href": "CONTENT/Workshops/BCCCD2024.html#software",
    "title": "BCCCD2024",
    "section": "Software",
    "text": "Software\nIn this tutorial, our primary tool will be Python!! We recommend installing it via Miniconda. Our interaction with Python will primarily be through the Spyder IDE. You’re free to use any IDE of your choice, but if you’d like to follow along more smoothly, we suggest checking out our guide on how to install both Miniconda and Spyder: Getting started with Python.\nBesides Python, we’ll also be utilizing Psychopy! Psychopy is an awesome set of packages and functions designed for conducting psychological experiments. It’s available as a standalone software or can be installed as a Python package.\nBased on our experience, we find it more advantageous to use Psychopy as a Python package due to its increased flexibility. We provide this anaconda environment to easily create a virtual environment with both python, psychopy and all the libraries that we will need. You can find details on how to install a conda environment on this page: Getting started with Psychopy"
  },
  {
    "objectID": "CONTENT/Workshops/BCCCD2024.html#files",
    "href": "CONTENT/Workshops/BCCCD2024.html#files",
    "title": "BCCCD2024",
    "section": "Files",
    "text": "Files\nIn our workshop we will create a cool eye-tracking study, collect the data and analyze them. To make everything smoother we provide HERE the stimuli and the data that we will use for the workshop."
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html",
    "href": "CONTENT/Stats/LinearModels.html",
    "title": "Linear Models",
    "section": "",
    "text": "Welcome to the first tutorial on data analysis!!! Today we are going to talk about one of the most flexible statistical methods: Linear models.\nLet’s be clear, WE ARE NOT STATISTICIANS!!!!\nWe’ll be discussing linear models in a very accessible and practical manner. Our explanations might not align with the rigorous definitions statisticians are accustomed to, and for that, we apologize in advance! However, our aim is to provide a stepping stone for you to grasp the concept of linear models and similar analyses. Let’s get started!"
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html#import-data",
    "href": "CONTENT/Stats/LinearModels.html#import-data",
    "title": "Linear Models",
    "section": "Import data",
    "text": "Import data\nYou can download the data that we will use in this tutorial from here:\n LM_SimulatedData.csv \nOnce downloaded we need to import it in our R session. Here we read our csv and we print a small preview of it.\n\ndf = read.csv(\"..\\\\..\\\\resources\\\\Stats\\\\LM_SimulatedData.csv\")\nhead(df)\n\nYou can see that the data is really simple! We have 4 columns:\n\nsubject column that tell us form which participant the data was collected\ntime the passing time\nperformance the variable of interest, the one that we want to model\ntool which tool was being used\n\n\n\n\n\n\n\nLong format\n\n\n\nOne important information that we need to keep in mind is that to run lm() in R we need the data in a long format and not a wide format.\nIn long format, each row represents a single observation. Variables are organized in columns, with one column for the variable names and another for the values. This means that the column you want to model (in the example performance) has 1 row for observation but the other columns usually have repeated entries ( e.g. subject , time, tool)\nWide format, on the other hand, has each row representing a subject or group, with multiple columns for different variables or time points. While this can be visually appealing for humans, it’s not optimal for our linear modeling needs.\nIf your data is currently in wide format, don’t worry! R provides tools like the tidyr package with functions such as pivot_longer() to easily convert your data from wide to long format."
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html#formula",
    "href": "CONTENT/Stats/LinearModels.html#formula",
    "title": "Linear Models",
    "section": "Formula",
    "text": "Formula\nTo run models in R we usually use formulas! Sounds complex doesn’t it?!? Well it is not, let me guide you through it.\nIn R, model formulas follow a specific structure. On the left side of the formula, we place our dependent variable - the one we’re interested in studying. In this case, it’s the performance column. Next, we use the tilde symbol ~. This tilde tells R that we want to model the variable on the left using the variables on the right side of the formula. On the right side, we list the independent variables we believe may influence our dependent variable. To test whether time predicts performance, we can use the formula:\nperformance ~ time. This basic structure allows us to examine a single predictor.\nWe can extend this model by adding another variable, such as tool, to see if it also predicts performance:\nperformance ~ time + tool. This formulation tells the model to assess whether either time or tool predicts performance, treating them as independent predictors.\nTo examine the interaction between these variables, we use a slightly different syntax:\nperformance ~ time : tool. This instructs the model to evaluate whether the interaction between the two variables predicts performance.\nIt’s important to note that using : only tests the interaction, not the individual effects of each variable. To include both main effects and their interaction, we can use the formula:\nperformance ~ time + tool + time:tool.\nR offers a shorthand for this complete model using the * operator. The formula:\nperformance ~ time * tool is equivalent to the longer version above, testing both main effects and the interaction in a more concise format.\nThese formulas are for simple linear models. Different types of models add small and different pieces to this basic structure. We will see in the next tutorial how to handle these “add-ons”. Now that we have seen how to make a proper formula let’s use it in our model!!"
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html#run-the-model",
    "href": "CONTENT/Stats/LinearModels.html#run-the-model",
    "title": "Linear Models",
    "section": "Run the model",
    "text": "Run the model\nOK, now we have our amazing data! Let’s run this Linear model.\nIt’s extremely simple. We will use the function lm() and we will pass our data df and the formula we just made together!!\nAfter fitting the model we extract the summary of it. This is how we will get all the information we need.\n\nmod = lm(performance ~ time*tool, data = df)\nsummary(mod)\n\nestimate_means(mod, get_datagrid(mod, numeric=0, by = 'tool'))\n\nPerfect this was super simple!! We can use the output of the model to understand whether the variable are predicting the performance. What we need is the pvalue that is the last column of the Coefficients section. If the pvalue is below 0.05 it means we have our effect if it is above it means we don’t. YES EVEN IF IT IS 0.051!!!\nWhen looking at model outputs, people often zero in on the p-value. However, there’s much more to unpack in a model summary! For now, we’ll just skim the surface of model summary interpretation. It’s true that the model provides p-values, which serve as indicators of whether there’s evidence for an effect of our variables on the dependent variable. But it’s crucial to understand the full scope of the model output. That’s why we’ve created a separate tutorial to guide you through the intricacies of model interpretation. For now, let’s wrap up this current tutorial. Afterwards, we’ll dive together into the art of deciphering model results."
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html#model-checks",
    "href": "CONTENT/Stats/LinearModels.html#model-checks",
    "title": "Linear Models",
    "section": "Model checks",
    "text": "Model checks\nSo now we have run our model and seen the summary… That’s great but how can we know that our model actually is ok?? Linear models, like most statistical techniques require few data assumption to be run. These assumption need to be met otherwise even if our model could be showing amazing results it won’t be valid.\nWhat are these assumptions?? Well they depend a lot on the model you are running. We won’t go into much details as there are very good website that explain them1 ,2, in this simple linear mode they are:\n\nLinear relationship: There exists a linear relationship between the independent variable, x, and the dependent variable, y.\nIndependence: The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data.\nHomoscedasticity/Homogeneity of variance: The residuals have constant variance at every level of x.\nNormality: The residuals of the model are normally distributed.\n\nAgain this is not a theoretical tutorial. So we won’t go into details as which are the assumptions (please read some of the link provided tho!!) but we will show you how to actually check these assumptions.\nThere is a super easy and convenient way we usually check these assumptions. Using the easystats library.\n\n\n\n\n\n\nNote\n\n\n\nEasystats\nEasystats is a collection of R packages that includes tools dedicated to the post-processing of statistical models. It is made of all these packages: report, correlation, modelbased, bayestestR, effectsize, see, parameters, performance, insight, datawizard. We will extensively use all these package in our tutorials. The cool thing is that you can import all of them by just simply importing the collection Easystats.\nIn this tutorial here we will use the function from the package performance. This is a package to check model performance. However instead of importing performancewe will import Easystats that will import all of the packages mentioned above.\n\n\nSo now we import easystats and we use the function check_model() to indeed check the model assumptions.\n\nlibrary(easystats)\ncheck_model(mod)\n\nPerfect all done!! We have a plot of the model assumptions and we can check if they are met!! But what do these plot represent? Here below we created a table that mirrors each plot with it explanation in it. These are brief and simple explanations. If you want to understand more about the check_model() function we suggest you to read the documentation about it and also the very nice vignette that the package provides.\n\n\n\n\n\n\nTip\n\n\n\nOne of the awesome features of easystats is its broad support for various model types. What’s the big deal? Well, it means that the check_model() function adapts its checks based on the specific model you’re using! This flexibility makes it an incredibly powerful and user-friendly tool. Pretty much any model you’ve just run (or at least most of them) can be fed into the check_model() function, allowing you to easily verify if it meets the necessary assumptions.\nKeep in mind: Always be aware of which assumptions your model should satisfy. We’re not suggesting you use this function blindly! Instead, we’re showing you how to efficiently plot all relevant assumptions in one go. It’s simpler and quicker!!\n\n\nStatistical tests\nYou’ve probably noticed that we’ve been relying on visual checks so far. In our view, this is often the best approach, as statistical tests for model assumptions can sometimes be overly stringent. However, there may be situations where you need to provide statistical evidence to support your model assumptions. This often happens when a reviewer (let’s call them Reviewer 2, shall we?) insists on seeing numerical proof. Fortunately, easystats has got your back.\nHere are some examples of what you can use:\n\ncheck_normality(mod)\n\nTo check the normality of our residuals and:\n\ncheck_homogeneity(mod)\n\nto check homoscedasticity/homogeneity of variance. Again you can find all the function in the performance package (part fo the Easystats collection)"
  },
  {
    "objectID": "CONTENT/Stats/LinearModels.html#interpret-results",
    "href": "CONTENT/Stats/LinearModels.html#interpret-results",
    "title": "Linear Models",
    "section": "Interpret Results",
    "text": "Interpret Results\nBut how to interpret these results?\n\nlibrary(ggplot2)\n\nmodel_p = parameters(mod)\n\nggplot()+\n  # Intercept\n  geom_point(aes(x = 0, y = model_p[1,2]), size = pi * model_p[1,3]^2, alpha = 0.3, color = 'darkred')+\n  geom_point(aes(x = 0, y = model_p[1,2]), color = 'darkred')+\n\n  coord_cartesian(xlim = c(-5,5), ylim = c(-5,5))"
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPython.html",
    "href": "CONTENT/GettingStarted/GettingStartedWithPython.html",
    "title": "Starting with Python",
    "section": "",
    "text": "Python is one of the most popular programming languages in general. In data science, it competes with Matlab and R for first place on the podium.\nIn our everyday we often use python to pre-process and analyze the data. In this tutorial we will explain our preferred way of installing python and managing its libraries. There are several ways to install python this is the one we recommend for its simplicity and flexibility",
    "crumbs": [
      "Getting started:",
      "Starting with Python"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPython.html#anaconda",
    "href": "CONTENT/GettingStarted/GettingStartedWithPython.html#anaconda",
    "title": "Starting with Python",
    "section": "Anaconda",
    "text": "Anaconda\nAnaconda is an open-source distribution for python. It is used for data science, machine learning, deep learning, etc. It makes simple to download and organize your python environment.\n\n\n\n\n\n\nCaution\n\n\n\nWhile Anaconda is a convenient way to obtain Python, it’s not our preferred method. That said, we’ve included it here because it’s one of the simplest options. For our recommended approach, just keep scrolling a bit further.\n\n\nTo install Anaconda simply follow this link. Choose a version suitable for you and click on download. Once you complete the download, open the setup.\n\n\n\n\n\nFollow the instructions in the setup.\nAfter the installation is complete, launch the Anaconda navigator. The Anaconda Navigator is a desktop GUI that comes with the anaconda distribution. Navigator allows you to launch common Python programs and easily manage conda packages, environments, and channels without using command-line commands. Navigator can search for packages on Anaconda Cloud or in a local Anaconda Repository.",
    "crumbs": [
      "Getting started:",
      "Starting with Python"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPython.html#minicondaminiforge",
    "href": "CONTENT/GettingStarted/GettingStartedWithPython.html#minicondaminiforge",
    "title": "Starting with Python",
    "section": "Miniconda/Miniforge",
    "text": "Miniconda/Miniforge\nWhile Anaconda is one of the simplest ways to install Python, it can be quite heavy, as it comes with many programs and libraries that are often unnecessary. For this reason, we recommend using either Miniconda or Miniforge.\nBoth Miniconda and Miniforge are lightweight installers for Conda. They provide a minimal setup, including only Conda, Python, and a few essential dependencies, giving you more control over what gets installed on your system and keeping your setup lean and efficient. Miniconda includes the default Conda package manager and channels, focusing on the official Conda repository. Miniforge, on the other hand, uses Conda-Forge as its default channel and comes pre-installed with Mamba, a faster and more efficient alternative to Conda for package management.\nWhich one should you use? To simplify, if everything mentioned earlier sounds like jargon, Miniconda is a safe choice. However, if you want the fastest and typically best option, go with Miniforge.\n\nMinicondaMiniforge\n\n\nTo use Miniconda download the installer from Miniconda (remember to scroll down under Anaconda)\nThe installation process is similar to that of Anaconda. Once the installation is complete, you will find the Anaconda Prompt among your programs. This prompt serves as your interface for installing Python packages and creating environments (we will see below how to do so).\n\n\n\n\n\n\n\nTo use Miniforge lead to Miniforge and download the version right for your system. And start the installation process.\n\n\n\n\n\n\nCaution\n\n\n\nThere are multiple version you can download. The one you should download is at the end of the page: Miniforge3\n\n\nThe installation process is similar to that of Anaconda. Once the installation is complete, you will find the Anaconda Prompt among your programs. This prompt serves as your interface for installing Python packages and creating environments (we will see below how to do so).\n\n\n\n\n\nOne important thing about Miniforge is that you should use mamba instead of conda for faster performance. The commands remain exactly the same—just replace conda with mamba.",
    "crumbs": [
      "Getting started:",
      "Starting with Python"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html",
    "title": "Create your first paradigm",
    "section": "",
    "text": "We will create a very simple and basic experiment that will be the stepping stone for some of the future tutorials. In the future tutorials we will show you how to extend and make this tutorial in a real experiment.\n\n\n\n\n\n\nStimuli!\n\n\n\nYou can download from here the stimuli that we will use in this example. They are very simple and basic stimuli:\n\na fixation cross\ntwo cues (a circle and a square)\na reward (a cute medal)\na non-reward (a cartoon of an exploding empty cloud)\na sound of winning at an arcade game\na sound of losing at an arcade game\n\n\n\nIn this tutorial, we will create an experiment in which, after the fixation cross, one of the two cues is presented. The cues will indicate whether we will receive a reward or not and where this will appear. After the circle is presented as a cue, the medal will be presented on the right. After the square is presented as a cue, the empty cloud will be presented on the left. Thus, if you follow the cued indication you will be able to predict the location of the next stimuli and whether or not it will be rewarding. Here below you can find a graphic representation of the design:\n\n\n\nFirst things first, let’s import the relevant libraries and define the path to where our stimuli are. PsychoPy has a lot of different modules that allow us to interface with different types of stimuli and systems. For this tutorial we need the following:\n\n# Import some libraries from PsychoPy and others\nimport os\n\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound\n\n\n\n\nThe next step is to create the window. The window is what we will show the stimuli in; it is the canvas on which to draw objects. For now, we will create a small window of 960*540 pixels. In this way, we will able to see the stimuli and still interact with the rest of our PC interface. In a real experiment, we would probably set the window dimension to the entirety of the display (fullscr=True) and maybe on a secondary screen (screen = 1).\n\n# Winsize\nwinsize = (960, 540)\n\n# create a window\nwin = visual.Window(size = winsize, fullscr=False, units=\"pix\", pos =(0,30), screen=0)\n\nNow let’s import the stimuli that we will present in this tutorial. We have 5 stimuli:\n\na fixation cross that we will use to catch the attention of our participants\na circle that will be our cue that signals a rewarding trial\na square that will be our cue that signals a non-rewarding trial\na cartoon of a medal that will be our reward\na cartoon of an empty cloud that will be our non-reward\n\nOn top of these visual stimuli, we will also import two sounds that will help us signal the type of trials. So:\n\na tada! winning sound\na papapaaa! losing sound\n\n\n\n\n\n\n\nTip\n\n\n\nWhen importing a visual stimulus we need to pass to the importing function in which window it will be displayed. In our case, we will pass all of them the “win” window that we just created.\n\n\n\n\n\n\n\n\nPATHS\n\n\n\nWhen working with file paths in Python, it’s important to remember that Windows and macOS/Linux use different conventions for their file paths:\n\nWindows Paths:\n\nWindows file paths typically use backslashes (\\). However, in Python, a backslash is used as an escape character. To handle this, you have two options:\n\nUse double backslashes to avoid Python interpreting the backslash as an escape character:\n'C:\\\\Users\\\\tomma\\\\Desktop'\nAlternatively, use a raw string by prefixing the path with r, which tells Python to treat backslashes as literal characters:\nr'C:\\Users\\tomma\\Desktop'\n\n\nmacOS/Linux Paths:\n\nmacOS and Linux use forward slashes (/) for their file paths, which are also compatible with Python’s string handling. You can use the path directly. There’s no need for double slashes or raw strings in macOS/Linux paths.\n'/Users/tomma/Desktop'\n\n\nAlways use the appropriate path format for your operating system to ensure your Python scripts work smoothly across different platforms. We’ll follow this rule in all our scripts, so keep it in mind – it’s a key detail for error-free scripting!\n\n\n\n#%% Load and prepare stimuli\n\n# Setting the directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n\n# Load images\nfixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\ncircle   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\circle.png', size = (200, 200))\nsquare   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\square.png', size = (200, 200))\nwinning   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\winning.png', size = (200, 200), pos=(250,0))\nlosing  = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\loosing.png', size = (200, 200), pos=(-250,0))\n\n# Load sound\nwinning_sound = sound.Sound('EXP\\\\Stimuli\\\\winning.wav')\nlosing_sound = sound.Sound('EXP\\\\Stimuli\\\\loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create a list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\nNote that in this simple experiment, we will present the reward always on the right and the non-rewards always on the left that’s why when we import the two rewards we set their pos to (250,0) and (-250,0). The first value indicates the number of pixels on the x-axis and the second is the number of pixels on the y-axis.\n\n\n\nNow we want to show a stimulus in the center of our window. To do so, we will have to use the function “draw”. As the name suggests this function draws the stimulus that we want on the window that we have created.\nLet’s start with displaying the fixation cross in the center.\n\n# Draw the fixation\nfixation.draw()\n\nDo you see the fixation cross?????? Probably not!! This is because we have drawn the fixation cross but we have not refreshed the window. Psychopy allows you to draw as many stimuli as you want on a window but the changes are only shown when you “refresh” the window. To do so we need to use the “flip” function.\n\nwin.flip()\n\nPerfect!!!! The fixation cross is there. Before each flip, we need to draw our objects. Otherwise, we will only see the basic window with nothing in it. Let’s try!!! flip the window now.\n\n# Flipping the window (refreshing)\nwin.flip()\n\nThe fixation is gone again! Exactly as predicted. Flipping the window allows us to draw and show something new in each frame. This means that the speed limit of our presentation is the actual frame rate of our display. If we have a 60Hz display we can present an image 60 times in a second.\nSo if we want to present our fixation for an entire second we would have to draw and flip it 60 times (our display has a refresh rate of 60Hz)! Let’s try:\n\nfor _ in range(60):\n    fixation.draw()\n    win.flip()\nwin.flip() # we re-flip at the end to clean the window\n\nNow we have shown the fixation for 1 second and then it disappeared. Nice!! However, you probably have already figured out that what we have done was unnecessary. If we want to present a static stimulus for 1s we could have just drawn it, flipped the window, and then waited for 1s. But now you have an idea of how to show animated stimuli or even videos!!! AMAZING!!!.\nNow let’s try to show the fixation for 1s by just waiting.\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n\n\n\nWe have seen how to show a stimulus let’s now play the sounds that we have imported. This is extremely simple, we can just play() them:\n\nwinning_sound.play()\ncore.wait(2)\nlosing_sound.play()\n\nGreat now we have played our two sounds!!\n\n\n\n\n\n\nWarning\n\n\n\nWhen playing a sound the script will continue and will not wait for the sound to have finished playing. So if you play two sounds one after without waiting the two sounds will play overlapping. That’s why we have used core.wait(2), this tells PsychoPy to wait 2 seconds after starting to play the sound.\n\n\n\n\n\nNow let’s try to put everything we have learned in one place and present one rewarding and one non-rewarding trial:\n\nwe present the fixation for 1s\nwe present one of the two cues for 3s\nwe wait 750ms of blank screen\nwe present the reward or the non-reward depending on the cue for 2s.\n\nIn the end, we also close the window.\n\n###### 1st Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the winning cue\ncircle.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nwinning.draw()\nwin.flip()\nwinning_sound.play()\ncore.wait(2)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n###### 2nd Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the non-rewarding cue\nsquare.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nlosing.draw()\nwin.flip()\nlosing_sound.play()\ncore.wait(2)  # wait for 2 second\nwin.flip()    # we re-flip at the end to clean the window\n\n\nwin.close()  # let's close the window at the end of the trial\n\n\n\n\nAmazing, we’ve got two trials! But, these trials are back-to-back, which isn’t typically what we want. More often, we prefer a brief gap between trials, known as the Inter Stimulus Interval (ISI). We could introduce this interval by simply adding a core.wait(1), which would pause the script for a second. However, I’d like to introduce you to an alternative method to wait for this second, which will be useful in future tutorials.\nTo implement this ISI, we’ll create a psychopy core.Clock(). Once initiated, this clock begins to keep track of time, allowing us to check how much time has elapsed since the clock started at any given moment. We’ll then use a while loop to monitor the elapsed time and break this loop once a second has passed.\nThe while loop is a loop that will keep doing the same thing over and over again as long as a certain condition is true, here the passing of 1s.\n\n### ISI\nclock = core.Clock() # start clock\nwhile clock.getTime() &lt; 1:\n    pass\n\nPerfect we can now add this at the end of our trials!!\n\n\n\nFantastic, we’ve nearly have our study! However, studies often don’t run to completion, especially when we’re working with infants and children. More often than not, we need to halt the study prematurely. This could be due to the participant becoming fatigued or distracted, or perhaps we need to tweak some settings.\nHow can we accomplish this? Of course, we could just shut down Python and let the experiment crash… but surely, there’s a more elegant solution… And indeed, there is! In fact, there are numerous methods to achieve this, and we’re going to demonstrate one of the most straightforward and flexible ones to you.\nWe can use the event.getKeys() function to ask Psychopy to report any key that has been pressed during our trial. In our case, we will check if the ESC key has been pressed and if it has, we will simply close the window and stop the study.\n\n### Check for closing experiment\nkeys = event.getKeys() # collect list of pressed keys\nif 'escape' in keys:\n    win.close()  # close window\n    core.quit()  # stop study\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can add this check for closing the study at any point during the study. However, we recommend placing it at the end of each trial. This ensures that even if you stop the experiment, you will have a complete trial, making it easier to analyze data since you won’t have any incomplete sections of the study.\nAlso, you can use this same method to pause the study or interact with its progress in general.\n\n\n\n\n\nIn an experiment, we want more than 1 trial. Let’s then create an experiment with 10 trials. We just need to repeat what we have done above multiple times. However, we need to randomize the type of trials, otherwise, it would be too easy to learn. To do so, we will create a list of 0 and 1. where 0 would identify a rewarding trial and 1 would index a non-rewarding trial.\nTo properly utilize this list of 0 and 1, we will need to create other lists of our stimuli. This will make it easier to call the right stimuli depending on the trial. We can do so by:\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\nPerfect!! Now we can put all the pieces together and run our experiment.\n\n\n\n\n\n\nCaution\n\n\n\nIn this final script, we will change the dimension of the window we will use. Since in most of the experiments, we will want to use the entire screen to our disposal, we will set fullscr = True when defining the window. In addition, we will also change the position of the rewarding and non-rewarding stimulus since now the window is bigger.\nIf you are testing this script on your laptop and do not want to lose the ability to interact with it until the experiment is finished, keep the same window size and position as the previous lines of code.\n\n\n\n# Import some libraries from PsychoPy and others\nimport os\n\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound\n\n#%% Load and prepare stimuli\n\n# Setting directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n# Winsize\nwinsize = (1920, 1080)\n\n# create a window\nwin = visual.Window(size = winsize, fullscr=False, units=\"pix\", pos =(0,30), screen=1)\n\n# Load images\nfixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\ncircle   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\circle.png', size = (200, 200))\nsquare   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\square.png', size = (200, 200))\nwinning  = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\winning.png', size = (200, 200), pos=(560,0))\nlosing  = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\loosing.png', size = (200, 200), pos=(-560,0))\n\n# Load sound\nwinning_sound = sound.Sound('EXP\\\\Stimuli\\\\winning.wav')\nlosing_sound = sound.Sound('EXP\\\\Stimuli\\\\loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\n\n#%% Trials\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n    fixation.draw()\n    win.flip()\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    core.wait(3)  # wait for 3 seconds\n\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n    sounds[trial].play()\n    core.wait(2)  # wait for 1 second\n    win.flip()    # we re-flip at the end to clean the window\n\n    ### ISI\n    clock = core.Clock() # start clock\n    while clock.getTime() &lt; 1:\n        pass\n      \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        core.quit()  # stop study\n        \nwin.close()\ncore.quit()",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#preparation",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#preparation",
    "title": "Create your first paradigm",
    "section": "",
    "text": "First things first, let’s import the relevant libraries and define the path to where our stimuli are. PsychoPy has a lot of different modules that allow us to interface with different types of stimuli and systems. For this tutorial we need the following:\n\n# Import some libraries from PsychoPy and others\nimport os\n\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#stimuli-1",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#stimuli-1",
    "title": "Create your first paradigm",
    "section": "",
    "text": "The next step is to create the window. The window is what we will show the stimuli in; it is the canvas on which to draw objects. For now, we will create a small window of 960*540 pixels. In this way, we will able to see the stimuli and still interact with the rest of our PC interface. In a real experiment, we would probably set the window dimension to the entirety of the display (fullscr=True) and maybe on a secondary screen (screen = 1).\n\n# Winsize\nwinsize = (960, 540)\n\n# create a window\nwin = visual.Window(size = winsize, fullscr=False, units=\"pix\", pos =(0,30), screen=0)\n\nNow let’s import the stimuli that we will present in this tutorial. We have 5 stimuli:\n\na fixation cross that we will use to catch the attention of our participants\na circle that will be our cue that signals a rewarding trial\na square that will be our cue that signals a non-rewarding trial\na cartoon of a medal that will be our reward\na cartoon of an empty cloud that will be our non-reward\n\nOn top of these visual stimuli, we will also import two sounds that will help us signal the type of trials. So:\n\na tada! winning sound\na papapaaa! losing sound\n\n\n\n\n\n\n\nTip\n\n\n\nWhen importing a visual stimulus we need to pass to the importing function in which window it will be displayed. In our case, we will pass all of them the “win” window that we just created.\n\n\n\n\n\n\n\n\nPATHS\n\n\n\nWhen working with file paths in Python, it’s important to remember that Windows and macOS/Linux use different conventions for their file paths:\n\nWindows Paths:\n\nWindows file paths typically use backslashes (\\). However, in Python, a backslash is used as an escape character. To handle this, you have two options:\n\nUse double backslashes to avoid Python interpreting the backslash as an escape character:\n'C:\\\\Users\\\\tomma\\\\Desktop'\nAlternatively, use a raw string by prefixing the path with r, which tells Python to treat backslashes as literal characters:\nr'C:\\Users\\tomma\\Desktop'\n\n\nmacOS/Linux Paths:\n\nmacOS and Linux use forward slashes (/) for their file paths, which are also compatible with Python’s string handling. You can use the path directly. There’s no need for double slashes or raw strings in macOS/Linux paths.\n'/Users/tomma/Desktop'\n\n\nAlways use the appropriate path format for your operating system to ensure your Python scripts work smoothly across different platforms. We’ll follow this rule in all our scripts, so keep it in mind – it’s a key detail for error-free scripting!\n\n\n\n#%% Load and prepare stimuli\n\n# Setting the directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n\n# Load images\nfixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\ncircle   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\circle.png', size = (200, 200))\nsquare   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\square.png', size = (200, 200))\nwinning   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\winning.png', size = (200, 200), pos=(250,0))\nlosing  = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\loosing.png', size = (200, 200), pos=(-250,0))\n\n# Load sound\nwinning_sound = sound.Sound('EXP\\\\Stimuli\\\\winning.wav')\nlosing_sound = sound.Sound('EXP\\\\Stimuli\\\\loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create a list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\nNote that in this simple experiment, we will present the reward always on the right and the non-rewards always on the left that’s why when we import the two rewards we set their pos to (250,0) and (-250,0). The first value indicates the number of pixels on the x-axis and the second is the number of pixels on the y-axis.",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#show-a-visual-stimulus",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#show-a-visual-stimulus",
    "title": "Create your first paradigm",
    "section": "",
    "text": "Now we want to show a stimulus in the center of our window. To do so, we will have to use the function “draw”. As the name suggests this function draws the stimulus that we want on the window that we have created.\nLet’s start with displaying the fixation cross in the center.\n\n# Draw the fixation\nfixation.draw()\n\nDo you see the fixation cross?????? Probably not!! This is because we have drawn the fixation cross but we have not refreshed the window. Psychopy allows you to draw as many stimuli as you want on a window but the changes are only shown when you “refresh” the window. To do so we need to use the “flip” function.\n\nwin.flip()\n\nPerfect!!!! The fixation cross is there. Before each flip, we need to draw our objects. Otherwise, we will only see the basic window with nothing in it. Let’s try!!! flip the window now.\n\n# Flipping the window (refreshing)\nwin.flip()\n\nThe fixation is gone again! Exactly as predicted. Flipping the window allows us to draw and show something new in each frame. This means that the speed limit of our presentation is the actual frame rate of our display. If we have a 60Hz display we can present an image 60 times in a second.\nSo if we want to present our fixation for an entire second we would have to draw and flip it 60 times (our display has a refresh rate of 60Hz)! Let’s try:\n\nfor _ in range(60):\n    fixation.draw()\n    win.flip()\nwin.flip() # we re-flip at the end to clean the window\n\nNow we have shown the fixation for 1 second and then it disappeared. Nice!! However, you probably have already figured out that what we have done was unnecessary. If we want to present a static stimulus for 1s we could have just drawn it, flipped the window, and then waited for 1s. But now you have an idea of how to show animated stimuli or even videos!!! AMAZING!!!.\nNow let’s try to show the fixation for 1s by just waiting.\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#play-a-sound",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#play-a-sound",
    "title": "Create your first paradigm",
    "section": "",
    "text": "We have seen how to show a stimulus let’s now play the sounds that we have imported. This is extremely simple, we can just play() them:\n\nwinning_sound.play()\ncore.wait(2)\nlosing_sound.play()\n\nGreat now we have played our two sounds!!\n\n\n\n\n\n\nWarning\n\n\n\nWhen playing a sound the script will continue and will not wait for the sound to have finished playing. So if you play two sounds one after without waiting the two sounds will play overlapping. That’s why we have used core.wait(2), this tells PsychoPy to wait 2 seconds after starting to play the sound.",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#create-a-trial",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#create-a-trial",
    "title": "Create your first paradigm",
    "section": "",
    "text": "Now let’s try to put everything we have learned in one place and present one rewarding and one non-rewarding trial:\n\nwe present the fixation for 1s\nwe present one of the two cues for 3s\nwe wait 750ms of blank screen\nwe present the reward or the non-reward depending on the cue for 2s.\n\nIn the end, we also close the window.\n\n###### 1st Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the winning cue\ncircle.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nwinning.draw()\nwin.flip()\nwinning_sound.play()\ncore.wait(2)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n###### 2nd Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the non-rewarding cue\nsquare.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nlosing.draw()\nwin.flip()\nlosing_sound.play()\ncore.wait(2)  # wait for 2 second\nwin.flip()    # we re-flip at the end to clean the window\n\n\nwin.close()  # let's close the window at the end of the trial",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#isi",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#isi",
    "title": "Create your first paradigm",
    "section": "",
    "text": "Amazing, we’ve got two trials! But, these trials are back-to-back, which isn’t typically what we want. More often, we prefer a brief gap between trials, known as the Inter Stimulus Interval (ISI). We could introduce this interval by simply adding a core.wait(1), which would pause the script for a second. However, I’d like to introduce you to an alternative method to wait for this second, which will be useful in future tutorials.\nTo implement this ISI, we’ll create a psychopy core.Clock(). Once initiated, this clock begins to keep track of time, allowing us to check how much time has elapsed since the clock started at any given moment. We’ll then use a while loop to monitor the elapsed time and break this loop once a second has passed.\nThe while loop is a loop that will keep doing the same thing over and over again as long as a certain condition is true, here the passing of 1s.\n\n### ISI\nclock = core.Clock() # start clock\nwhile clock.getTime() &lt; 1:\n    pass\n\nPerfect we can now add this at the end of our trials!!",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#stop-the-experiment",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#stop-the-experiment",
    "title": "Create your first paradigm",
    "section": "",
    "text": "Fantastic, we’ve nearly have our study! However, studies often don’t run to completion, especially when we’re working with infants and children. More often than not, we need to halt the study prematurely. This could be due to the participant becoming fatigued or distracted, or perhaps we need to tweak some settings.\nHow can we accomplish this? Of course, we could just shut down Python and let the experiment crash… but surely, there’s a more elegant solution… And indeed, there is! In fact, there are numerous methods to achieve this, and we’re going to demonstrate one of the most straightforward and flexible ones to you.\nWe can use the event.getKeys() function to ask Psychopy to report any key that has been pressed during our trial. In our case, we will check if the ESC key has been pressed and if it has, we will simply close the window and stop the study.\n\n### Check for closing experiment\nkeys = event.getKeys() # collect list of pressed keys\nif 'escape' in keys:\n    win.close()  # close window\n    core.quit()  # stop study\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can add this check for closing the study at any point during the study. However, we recommend placing it at the end of each trial. This ensures that even if you stop the experiment, you will have a complete trial, making it easier to analyze data since you won’t have any incomplete sections of the study.\nAlso, you can use this same method to pause the study or interact with its progress in general.",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#create-an-entire-experiment",
    "href": "CONTENT/GettingStarted/CreateYourFirstParadigm.html#create-an-entire-experiment",
    "title": "Create your first paradigm",
    "section": "",
    "text": "In an experiment, we want more than 1 trial. Let’s then create an experiment with 10 trials. We just need to repeat what we have done above multiple times. However, we need to randomize the type of trials, otherwise, it would be too easy to learn. To do so, we will create a list of 0 and 1. where 0 would identify a rewarding trial and 1 would index a non-rewarding trial.\nTo properly utilize this list of 0 and 1, we will need to create other lists of our stimuli. This will make it easier to call the right stimuli depending on the trial. We can do so by:\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\nPerfect!! Now we can put all the pieces together and run our experiment.\n\n\n\n\n\n\nCaution\n\n\n\nIn this final script, we will change the dimension of the window we will use. Since in most of the experiments, we will want to use the entire screen to our disposal, we will set fullscr = True when defining the window. In addition, we will also change the position of the rewarding and non-rewarding stimulus since now the window is bigger.\nIf you are testing this script on your laptop and do not want to lose the ability to interact with it until the experiment is finished, keep the same window size and position as the previous lines of code.\n\n\n\n# Import some libraries from PsychoPy and others\nimport os\n\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound\n\n#%% Load and prepare stimuli\n\n# Setting directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n# Winsize\nwinsize = (1920, 1080)\n\n# create a window\nwin = visual.Window(size = winsize, fullscr=False, units=\"pix\", pos =(0,30), screen=1)\n\n# Load images\nfixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\ncircle   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\circle.png', size = (200, 200))\nsquare   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\square.png', size = (200, 200))\nwinning  = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\winning.png', size = (200, 200), pos=(560,0))\nlosing  = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\loosing.png', size = (200, 200), pos=(-560,0))\n\n# Load sound\nwinning_sound = sound.Sound('EXP\\\\Stimuli\\\\winning.wav')\nlosing_sound = sound.Sound('EXP\\\\Stimuli\\\\loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, losing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\n\n#%% Trials\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n    fixation.draw()\n    win.flip()\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    core.wait(3)  # wait for 3 seconds\n\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n    sounds[trial].play()\n    core.wait(2)  # wait for 1 second\n    win.flip()    # we re-flip at the end to clean the window\n\n    ### ISI\n    clock = core.Clock() # start clock\n    while clock.getTime() &lt; 1:\n        pass\n      \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        core.quit()  # stop study\n        \nwin.close()\ncore.quit()",
    "crumbs": [
      "Creating an experiment:"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilDataAnalysis.html",
    "href": "CONTENT/EyeTracking/PupilDataAnalysis.html",
    "title": "Pupil Data Analysis",
    "section": "",
    "text": "If you have collected and pre-processed your pupil data, the long awaited moment arrived: It’s finally time to analyse your data and get your results!!!\nIn this tutorial we will do two types of analysis. The first one is more simple, and the second is more advanced. For some research questions, simple analyses are enough: they are intuitive and easy to understand. However, pupil data is actually very rich and complex, and more sophisticated analyses can sometimes help to really get the most out of your data and let them shine!\nBefore starting any type of analysis, let’s import the data and take a quick look at it.",
    "crumbs": [
      "Eye-tracking",
      "Pupil Data Analysis"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilDataAnalysis.html#import-data",
    "href": "CONTENT/EyeTracking/PupilDataAnalysis.html#import-data",
    "title": "Pupil Data Analysis",
    "section": "Import data",
    "text": "Import data\nThe data used in this tutorial comes from the pre-processing tutorial of pupil dilation. If you haven’t run that tutorial yet, it’s a good idea to check it out first to ensure your data is prepared and ready for analysis: Pre-processing pupil data. In case you did not save the result of the pre-processing you can download them from here :\n Peocessed_PupilData.csv \nNow that you have the data, let’s import it along with the necessary libraries. We’ll also ensure that the Event and Subjects columns are properly set as factors (categorical for easier analysis. Here’s how:\n\nlibrary(tidyverse)  # Data manipulation and visualization\nlibrary(easystats)  # Easy statistical modeling\n\ndata = read.csv(\"..\\\\..\\\\resources\\\\Pupillometry\\\\Processed\\\\Processed_PupilData.csv\")\n\n# Make sure Event and Subject are factors\ndata$Event = as.factor(data$Event)\ndata$Subject = as.factor(data$Subject)\n\nhead(data)\n\n  X   Subject  Event TrialN mean_pupil  time\n1 1 Subject_1 Square      2 0.02437347  0.00\n2 2 Subject_1 Square      2 0.05996201  3.30\n3 3 Subject_1 Square      2 0.05996201  6.65\n4 4 Subject_1 Square      2 0.06092957 10.00\n5 5 Subject_1 Square      2 0.06092957 13.30\n6 6 Subject_1 Square      2 0.06457876 16.65\n\n\nFor a detailed description of the data, you can have a look at the tutorial on preprocessing pupil data. The key variables to focus on here are the following:\n\nmean_pupil indicates what the pupil size was at every moment in time (every 50 milliseconds, 20Hz). This is our dependent variable.\ntime indicates the specific moment in time within each trial\nTrialN indicates the trial number\nEvent indicates whether the trial contained a circle (followed by a reward) or a square (not followed by a reward). This variable is not numerical, but categorical. We thus set it to factor with as.factor().\nSubject contains a different ID number for each subject. This is also a categorical variable.",
    "crumbs": [
      "Eye-tracking",
      "Pupil Data Analysis"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilDataAnalysis.html#comparing-means",
    "href": "CONTENT/EyeTracking/PupilDataAnalysis.html#comparing-means",
    "title": "Pupil Data Analysis",
    "section": "Comparing means",
    "text": "Comparing means\nIn many paradigms, you have two or more conditions and want to test whether your dependent variable (pupil size in this case!) is significantly different across conditions. In our example paradigm, we may want to test whether, on average, pupil size while looking at the rewarding cue (the circle) is greater than pupil size while looking at the non-rewarding cue (the square). This would mean that even before the reward is presented, infants have learned that a reward will be coming and dilate their pupils in response to it! Pretty cool, uh?\nIf we want to test multiple groups, we can use a t-test, an ANOVA or… A linear model! Here, we’ll be using a special type of linear model, a mixed-effect model - which is infinitely better for many many reasons [add link].\nAdapt the data\nWe want to compare the means across conditions but… We don’t have means yet! We have a much richer dataset, that contains hundreds of datapoints with milliseconds precision. For this first simple analysis, we just want one average measure of pupil dilation for each trial instead. We can compute this using the tidyverse library (that is container of multiple packages) a powerful collection of packages for wrangling and visualuzating dataframes in R.\nHere, we group the data by Subject, Event, and TrialN, then summarize it within these groups by calculating the mean values.\n\naveraged_data = data %&gt;%\n  group_by(Subject, Event, TrialN) %&gt;%\n  summarise(mean_pupil = mean(mean_pupil, na.rm = TRUE))\n\nhead(averaged_data)\n\n# A tibble: 6 × 4\n# Groups:   Subject, Event [2]\n  Subject   Event  TrialN mean_pupil\n  &lt;fct&gt;     &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 Subject_1 Circle      3     0.0437\n2 Subject_1 Circle      4     0.302 \n3 Subject_1 Circle      6     0.0512\n4 Subject_1 Circle      9    -0.0411\n5 Subject_1 Square      2    -0.109 \n6 Subject_1 Square      5     0.0489\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this step, we used the average to calculate an index for each trial, meaning we averaged the pupil dilation over the trial duration. However, this is not the only option. Other approaches include extracting the peak value (max(mean_pupil, na.rm = TRUE)) or calculating the sum of the signal (sum(mean_pupil, na.rm = TRUE)), which can also represent the area under the curve (AUC) for the trial.\n\n\nLinear mixed-effect model\nWith a single value for each participant, condition, and trial (averaged across time points), we are now ready to proceed with our analysis. Even if the word “Linear mixed-effect model” might sound scary, the model is actually very simple. We take our experimental conditions (Event) and check whether they affect pupil size (mean_pupil). To account for individual differences in pupil response intensity, we include participants as a random intercept.\nLet’s give it a go!!!\n\nlibrary(lmerTest)   # Mixed-effect models library\n\n# The actual model\nmodel_avg = lmer(mean_pupil ~ Event + (1|Subject), data = averaged_data)\n\nsummary(model_avg) # summary of the model\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ Event + (1 | Subject)\n   Data: averaged_data\n\nREML criterion at convergence: 20\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.90027 -0.68329 -0.03241  0.65714  2.00903 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n Subject  (Intercept) 0.0008169 0.02858 \n Residual             0.0747253 0.27336 \nNumber of obs: 55, groups:  Subject, 6\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)   \n(Intercept)  0.10945    0.05392 17.96376   2.030  0.05746 . \nEventSquare -0.20742    0.07375 48.99682  -2.812  0.00705 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nEventSquare -0.697\n\n\nWe won’t dive into the detailed interpretation of the results here—this isn’t the right place for that. However, if you’re curious about how to interpret mixed-effects model outputs, you can check out this excellent guide: Linear mixed-effects models. It provides a great foundation. And keep an eye out for our upcoming page dedicated to mixed-effects modeling, where we’ll break everything down step by step!\nThe key takeaway here is that there’s a significatn difference between the Event. Specifically, the Square cue appears to result in smaller pupil dilation compared to the Circle event (which serves as the reference level for the intercept). COOL!\nLet’s visualize the effect!!\n\nCode# Create a data grid for Event and time\ndatagrid = get_datagrid(model_avg, by = c('Event'))\n\n# Compute model-based expected values for each level of Event\npred = estimate_expectation(datagrid)\n\n# 'pred' now contains predicted values and confidence intervals for each event condition.\n# We can visualize these predictions and overlay them on the observed data.\n\nggplot() +\n  # Observed data (jittered points to show distribution)\n  geom_jitter(data = averaged_data, aes(x=Event, y=mean_pupil, color=Event), width=0.1, alpha=0.5, size = 5) +\n  \n  # Model-based predictions: points for Predicted values\n  geom_point(data=pred, aes(x=Event, y=Predicted, fill=Event), \n             shape=21, size=10) +\n  \n  # Error bars for the confidence intervals\n  geom_errorbar(data=pred, aes(x=Event, ymin=Predicted - SE, ymax=Predicted + SE, color=Event), \n                width=0.2, lwd=1.5) +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'none') +\n  labs(title=\"Predicted Means vs. Observed Data\",\n       x=\"Condition\",\n       y=\"Baseline-Corrected Pupil Size\")",
    "crumbs": [
      "Eye-tracking",
      "Pupil Data Analysis"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilDataAnalysis.html#analysing-the-time-course-of-pupil-size",
    "href": "CONTENT/EyeTracking/PupilDataAnalysis.html#analysing-the-time-course-of-pupil-size",
    "title": "Pupil Data Analysis",
    "section": "Analysing the time course of pupil size",
    "text": "Analysing the time course of pupil size\nAlthough we have seen how to compare mean values of pupil size, our original data was much richer. By taking averages, we made it simpler but we also lost precious information. Usually, it is better to keep the data as rich as possible, even if that might require more complex analyses. Here we’ll show you one example of a more complex analysis: generalised additive models. Fear not though!!! As usual, we will try to break it down in small digestible bites, and you might realise it’s not actually that complicated after all.\nThe key aspect here is that we will stop taking averages, and analyse the time course of pupil dilation instead. We will analyse how it changes over time with precision in the order of milliseconds!! This is exciting!!!\nThis is something that we cannot do with linear models. For example, in this case linear models would assume that, over the course of a trial, pupil size will only increase linearly over time. The model would be something like this:\n\nlinear_model = lmer(mean_pupil ~ Event * time + (1|Subject), data = data) \n\nNote that, compared to the previous model, we have made two changes: First, we have changed the data. While before we were using averages, now we use the richer data set; Second, we added time as a predictor. We are saying that mean_pupil might be changing linearly across time… But this is very silly!!! To understand how silly it is, let’s have a look at the data over time.\n\nCode# Let's first compute average pupil size at each time point by condition\ndata_avg_time = data %&gt;%\n  group_by(Event, time) %&gt;%\n  summarise(mean_pupil = mean(mean_pupil, na.rm=TRUE))\n\n# Now let's plot these averages over time\nggplot(data_avg_time, aes(x=time, y=mean_pupil, color=Event)) +\n  geom_line(lwd=1.5) +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'bottom',\n        legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) +\n\n  labs(x = \"time (ms)\",\n       y = \"Baseline-Corrected Pupil Size\") \n\n\n\n\n\n\n\nHere’s the data averaged by condition at each time point. As you can clearly see, pupil dilation doesn’t follow a simple linear increase or decrease; the pattern is much more complex. Let’s see how poorly a simple linear model fits this intricate pattern!\n\nCode# Create a data grid for Event and time\ndatagrid = get_datagrid(linear_model, by = c('Event','time'))\n\n# Estimate expectation and uncertainty (Predicted and SE)\nEst = estimate_expectation(linear_model, datagrid)\n\n# Plot predictions with confidence intervals and the observed data\nggplot() +\n  # Real data line\n  geom_line(data = data_avg_time, aes(x=time, y=mean_pupil, color=Event), lwd=1.5) +\n  \n  # Predicted ribbons\n  geom_ribbon(data = Est, aes(x=time, ymin = Predicted - SE, ymax = Predicted + SE,\n                              fill = Event), alpha = 0.2) +\n  \n  # Predicted lines\n  geom_line(data = Est, aes(x=time, y=Predicted, color=Event), lwd=1.8,linetype = \"dashed\") +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'bottom',\n        legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) +\n  labs(title = \"Linear Model Predictions vs. Data\",\n       x = \"time (ms)\",\n       y = \"Baseline-corrected Pupil Size\")\n\n\n\n\n\n\n\nThe estimates from our model don’t really resemble the actual data! To capture all those non-linear, smooth changes over time, we need a more sophisticated approach. Enter Generalized Additive Models (GAMs)—the perfect tool to save the day!\nGeneralized additive model\nHere, we will not get into all the details of generalized additive models (from now on, GAMs). We will just show one example of how they can be used to model pupil size. To do this, we have to abandon linear models and download a new package instead, mgcv (install.packages(\"mgcv\")). This package is similar to the one we used before for linear models but offers greater flexibility, particularly for modeling time-series data and capturing non-linear relationships.\nWhat are GAMs\nOk, cool! GAMs sound awesome… but you might still be wondering what they actually do. Let me show you an example with some figures—that always helps make things clearer!\n\nCodelibrary(patchwork)\n\n# Parameters\namp &lt;- 1; freq &lt;- 1; phase &lt;- 0; rate &lt;- 100; dur &lt;- 2\ntime &lt;- seq(0, dur, by = 1 / rate)\n\n# Sinusoidal wave with noise\nwave &lt;- amp * sin(2 * pi * freq * time + phase) + rnorm(length(time), mean = 0, sd = 0.2)\n\n\n# Plot\none = ggplot()+\n  geom_point(aes(y=wave, x= time),size=3)+\n  theme_bw(base_size = 45)+\n  labs(y='Data')\n\n\ntwo = ggplot()+\n  geom_point(aes(y=wave, x= time),size=3)+\n  geom_smooth(aes(y=wave, x= time), method = 'lm', color='black', lwd=1.5)+\n  theme_bw(base_size = 45)+\n   theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\ntree= ggplot()+\n  geom_point(aes(y=wave, x= time),size=3)+\n  geom_smooth(aes(y=wave, x= time), method = 'gam', color='black', lwd=1.5)+\n  theme_bw(base_size = 45)+\n   theme(\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank()\n  )\n\none + two + tree\n\n\n\n\n\n\n\nWhen modeling data with many fluctuations, a simple linear model often falls short. In the left plot, we see the raw data with its complex, non-linear pattern. The middle plot illustrates a linear model’s attempt to capture these fluctuations, but it oversimplifies the relationships and fails to reflect the true data structure. Finally, the right plot showcases an additive model, which adapts to the data’s variability by following its fluctuations and accurately capturing the underlying pattern. This demonstrates the strength of additive models in modeling non-linear, smooth changes.\nWell….. This sounds like the same problem we have in our pupil data!!! Let’s go figure\n\n\n\n\n\n\nNote\n\n\n\nLinear models can be extended to capture fluctuations using polynomial terms, but this approach has limitations. Higher-order polynomials can overfit the data, capturing noise instead of meaningful patterns. Additive models, however, use smooth functions like splines to flexibly adapt to data fluctuations without the instability of polynomials, making them a more robust and interpretable choice.\n\n\nRun our GAM\nTo run a GAM, the syntax is relatively similar to what we used in the linear model section.\n\nlibrary(\"mgcv\")\n\n# Additive model\nadditive_model = bam(mean_pupil ~ Event\n                     + s(time, by=Event, k=20)\n                     + s(time, Subject, bs='fs', m=1),\n                     data=data)\n\nLet’s break the formula down:\n\nmean_pupil ~ Event: Here, I treat Condition as a main effect, just like we did before.\ns(time, by=Event, k=20): This is where the magic happens. By wrapping time in s(), we are telling the model: “Don’t assume that changes in pupil size over time are linear. Instead, estimate a smooth, wiggly function.” The argument by=Event means: “Do this separately for each condition, so that each condition gets its own smooth curve over time.” Finally, k=20 controls how wiggly the curve can be (technically, how many ‘knots’ or flexibility points the smoothing function is allowed to have). In practice, we are allowing the model to capture complex, non-linear patterns of pupil size changes over time for each condition.\ns(time, Subject, bs='fs', m=1): Here, we go one step further and acknowledge that each participant might have their own unique shape of the time course. By using bs='fs', I am specifying a ‘factor smooth’, which means: “For each subject, estimate their own smooth function over time.” Setting m=1 is a specific parameter choice that defines how we penalize wiggliness. Essentially, this term is allowing us to capture individual differences in how pupil size changes over time, over and above the general pattern captured by the main smooth. It’s something like the random effect we have seen before in the linear mixed-effect model.\n\nNow that we have run our first GAM, we can see how well it predicts the data!\n\nCode# Data grid\ndatagrid = get_datagrid(additive_model, length = 100, include_random = T)\n\n# Estimate expectation and uncertainty (Predicted and SE)\nEst = estimate_expectation(additive_model, datagrid, exclude=c(\"s(time,Subject)\"))\n\n\n# Plot predictions with confidence intervals and the observed data\nggplot() +\n  # Real data line\n  geom_line(data = data_avg_time, aes(x=time, y=mean_pupil, color=Event), size=1.5) +\n  \n  # Predicted ribbons\n  geom_ribbon(data = Est, aes(x=time, ymin = CI_low, ymax = CI_high,\n                              fill = Event), alpha = 0.2) +\n  \n  # Predicted lines\n  geom_line(data = Est, aes(x=time, y=Predicted, color=Event), size=1.8, linetype = \"dashed\") +\n  \n  theme_bw(base_size = 45)+\n  theme(legend.position = 'bottom',\n        legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) +\n  labs(title = \"Additive model Predictions vs. Data\",\n       x = \"time (ms)\",\n       y = \"Baseline-corrected Pupil Size\")\n\n\n\n\n\n\n\nThis looks so much better!!! The line fit so much better to the data!! We can also have a look at whether the effect of our experimental condition is significant:\n\nsummary(additive_model) \n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nmean_pupil ~ Event + s(time, by = Event, k = 20) + s(time, Subject, \n    bs = \"fs\", m = 1)\n\nParametric coefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.106962   0.036428   2.936  0.00332 ** \nEventSquare -0.202138   0.003477 -58.130  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                       edf Ref.df      F p-value    \ns(time):EventCircle  7.307  9.063 12.870 &lt; 2e-16 ***\ns(time):EventSquare  2.877  3.267  4.069 0.00525 ** \ns(time,Subject)     40.547 53.000 73.299 &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.222   Deviance explained = 22.3%\nfREML = 8767.1  Scale est. = 0.099062  n = 33000\n\n\nThe fixed effects (Parametric coefficients) show a strong negative effect for Eventquare, indicating that pupil size for Square is significantly lower than for Circle. This suggests that pupil size is greater when expecting a rewarding stimulus compared to a non-rewarding one.\nThe smooth terms indicate whether the non-linear relationships modeled by s() explain significant variance in the data. A significant smooth term confirms that the function captures meaningful, non-linear patterns beyond random noise or simpler terms. While fixed effects are typically more important for hypothesis testing, it’s crucial to ensure the model specification captures the data’s fluctuations accurately.\nYou did it!!! You started from a simpler model and little by little you built a very complex Generalized Additive Model!! Amazing work!!!\n\n\n\n\n\n\nWarning\n\n\n\nThis is just a very basic tutorial!\nThere are additional checks and considerations to keep in mind when using additive models to model pupil dilation data. We plan to extend this tutorial over time to include more details.\nLuckily, there are researchers who have already explored and explained these steps thoroughly. This paper, in particular, has greatly informed our approach. It dives deeper into the use of GAMs (still with the mgcv package), reviewing techniques for fitting models, addressing auto-correlations, and ensuring the accuracy and robustness of your GAMs. We highly recommend reading this paper to deepen your understanding!",
    "crumbs": [
      "Eye-tracking",
      "Pupil Data Analysis"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html",
    "title": "Using I2MC for robust fixation extraction",
    "section": "",
    "text": "When it comes to eye-tracking data, a fundamental role is played by fixations. A fixation indicates that a person’s eyes are looking at a particular point of interest for a given amount of time. More specifically, a fixation is a cluster of consecutive data points in an eye-tracking dataset for which a person’s gaze remains relatively still and focused on a particular area or object.\nTypically, eye-tracking programs come with their own fixation detection algorithms that give us a rough idea of what the person was looking at. But here’s the problem: these tools aren’t always very good when it comes to data from infants and children. Why? Because infants and children can be all over the place! They move their heads, put their hands (or even feet) in front of their faces, close their eyes, or just look away. All of this makes the data a big mess that’s hard to make sense of with regular fixation detection algorithms. Because the data is so messy, it is difficult to tell which data points are part of the same fixation or different fixations.\nBut don’t worry! We’ve got a solution: I2MC.\nI2MC stands for “Identification by Two-Means Clustering”, and it was designed specifically for this kind of problem. It’s designed to deal with all kinds of noise, and even periods of data loss. In this tutorial, we’ll show you how to use I2MC to find fixations. We won’t get into the nerdy stuff about how it works - this is all about the practical side. If you’re curious about the science, you can read the original article.\nNow that we’ve introduced I2MC, let’s get our hands dirty and see how to use it!",
    "crumbs": [
      "Eye-tracking",
      "Using I2MC for robust fixation extraction"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html#import-data",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html#import-data",
    "title": "Using I2MC for robust fixation extraction",
    "section": "Import data",
    "text": "Import data\nNow we will write a simple function to import our data. This step unfortunately will have to be adapted depending on the system you used to collect the data and the data structure you will have in the end. For this tutorial, you can use your data-set (probably you will have to adapt the importing function) or use our data that you can download from here.\nLet’s create step by step our function to import the data\n\n# Load data\nraw_df = pd.read_csv(PATH_TO_DATA, delimiter=',')\n\nAfter reading the data we will create a new data-frame that we will fill with the information needed from our raw_df. this is the point that would change depending on you eye-tracked and data format. you will probably have to change the columns names to be sure to have the 5 relevant ones.\n\n# Create empty dataframe\ndf = pd.DataFrame()\n    \n# Extract required data\ndf['time'] = raw_df['time']\ndf['L_X'] = raw_df['L_X']\ndf['L_Y'] = raw_df['L_Y']\ndf['R_X'] = raw_df['R_X']\ndf['R_Y'] = raw_df['R_Y']\n\nAfter selecting the relevant data we will perform some very basic processing. Sometimes there could be weird peaks where one sample is (very) far outside the monitor. Here, we will count as missing any data that is more than one monitor distance outside the monitor. Tobii gives us the validity index of the measured eye, here when the validity is too low (&gt;1) we will consider the sample as missing\n\n###\n# Sometimes we have weird peaks where one sample is (very) far outside the\n# monitor. Here, count as missing any data that is more than one monitor\n# distance outside the monitor.\n\n# Left eye\nlMiss1 = (df['L_X'] &lt; -res[0]) | (df['L_X']&gt;2*res[0])\nlMiss2 = (df['L_Y'] &lt; -res[1]) | (df['L_Y']&gt;2*res[1])\nlMiss  = lMiss1 | lMiss2 | (raw_df['L_V'] == False)\ndf.loc[lMiss,'L_X'] = np.nan\ndf.loc[lMiss,'L_Y'] = np.nan\n\n# Right eye\nrMiss1 = (df['R_X'] &lt; -res[0]) | (df['R_X']&gt;2*res[0])\nrMiss2 = (df['R_Y'] &lt; -res[1]) | (df['R_Y']&gt;2*res[1])\nrMiss  = rMiss1 | rMiss2 | (raw_df['R_V'] == False)\ndf.loc[rMiss,'R_X'] = np.nan\ndf.loc[rMiss,'R_Y'] = np.nan\n\nPerfect!!!\n\nEverything into a function\nWe have read the data, extracted the relevant information and done some extremely basic processing rejecting data that had to be considered non valid. Now we will wrap this code in a function to make it easier to use with I2MC:\n\n# ===================================================================\n# Import data from Tobii TX300\n# ===================================================================\n\ndef tobii_TX300(fname, res=[1920,1080]):\n\n    # Load all data\n    raw_df = pd.read_csv(fname, delimiter=',')\n    df = pd.DataFrame()\n    \n    # Extract required data\n    df['time'] = raw_df['time']\n    df['L_X'] = raw_df['L_X']\n    df['L_Y'] = raw_df['L_Y']\n    df['R_X'] = raw_df['R_X']\n    df['R_Y'] = raw_df['R_Y']\n    \n    \n    ###\n    # Sometimes we have weird peaks where one sample is (very) far outside the\n    # monitor. Here, count as missing any data that is more than one monitor\n    # distance outside the monitor.\n    \n    # Left eye\n    lMiss1 = (df['L_X'] &lt; -res[0]) | (df['L_X']&gt;2*res[0])\n    lMiss2 = (df['L_Y'] &lt; -res[1]) | (df['L_Y']&gt;2*res[1])\n    lMiss  = lMiss1 | lMiss2 | (raw_df['L_V'] == False)\n    df.loc[lMiss,'L_X'] = np.nan\n    df.loc[lMiss,'L_Y'] = np.nan\n    \n    # Right eye\n    rMiss1 = (df['R_X'] &lt; -res[0]) | (df['R_X']&gt;2*res[0])\n    rMiss2 = (df['R_Y'] &lt; -res[1]) | (df['R_Y']&gt;2*res[1])\n    rMiss  = rMiss1 | rMiss2 | (raw_df['R_V'] == False)\n    df.loc[rMiss,'R_X'] = np.nan\n    df.loc[rMiss,'R_Y'] = np.nan\n    \n    return(df)\n\n\n\nFind our data\nNice!! we have our import function that we will use to read our data. Now, let’s find our data! To do this, we will use the glob library, which is a handy tool for finding files in Python. Before that let’s set our working directory. The working directory is the folder where we have all our scripts and data. We can set it using the os library:\n\nimport os\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD\\Workshop')\n\nThis is my directory, you will have something different, you need to change it to where your data are. Once you are done with that we can use glob to find our data files. In the code below, we are telling Python to look for files with a .csv extension in a specific folder on our computer. Let’s import glob and then let’s find the files:\n\nimport glob\ndata_files = glob.glob('DATA\\\\RAW\\\\**\\\\*.csv', recursive = True)\n\n\nDATA\\\\RAW\\\\: This is the path where we want to start our search.\n\\\\*: This special symbol tells Python to search in all the subfolders (folders within folders) under our starting path.\n\\\\*.csv: We’re asking Python to look for files with names ending in “.csv”.\nrecursive=True: This option means that Python should search for files not just in the immediate subfolders but in all the subfolders within subfolders, and so on.\n\nSo, when we run this code, Python will find and give us a list of all the “.csv” files located in any subfolder within our specified path. This makes it really convenient to find and work with lots of files at once.\n\n\nDefine the output folder\nBefore doing anything else I would suggest creating a folder where we will save the output of I2MC. We will create a folder called i2mc_output. Using the os library we make sure the output folder doesn’t exist (os.path.isdir(output_fodler)) and then we create it (os.mkdir(output_fodler))\n\nimport os\n\n# define the output folder\noutput_fodler = 'DATA\\\\i2mc_output' # define folder path\\name\n\n# Create the folder\nif not os.path.isdir(output_fodler):\n   os.mkdir(output_fodler)\n\n\n\nI2MC settings\nNow that we’ve got our data, know how to import it using glob and we have out output folder, we’re all set to run I2MC. But wait, before we dive in, we need to set up a few things. These settings are like the instructions we give to I2MC before it does its magic. The default settings usually work just fine for most situations. You can keep them as they are and proceed. If you’re curious about what each of these settings does, you can explore the original I2MC article for a detailed understanding. Here I’ve added a general explanation about what each setting does. Once you’ve read through the instructions and have a clear understanding, you can customize the settings to match your specific requirements.\nLet’s define these settings:\n\n# =============================================================================\n# NECESSARY VARIABLES\n\nopt = {}\n# General variables for eye-tracking data\nopt['xres']         = 1920.0                # maximum value of horizontal resolution in pixels\nopt['yres']         = 1080.0                # maximum value of vertical resolution in pixels\nopt['missingx']     = np.nan          # missing value for horizontal position in eye-tracking data (example data uses -xres). used throughout the algorithm as signal for data loss\nopt['missingy']     = np.nan          # missing value for vertical position in eye-tracking data (example data uses -yres). used throughout algorithm as signal for data loss\nopt['freq']         = 300.0                 # sampling frequency of data (check that this value matches with values actually obtained from measurement!)\n\n# Variables for the calculation of visual angle\n# These values are used to calculate noise measures (RMS and BCEA) of\n# fixations. The may be left as is, but don't use the noise measures then.\n# If either or both are empty, the noise measures are provided in pixels\n# instead of degrees.\nopt['scrSz']        = [50.9174, 28.6411]    # screen size in cm\nopt['disttoscreen'] = 65.0                  # distance to screen in cm.\n\n# Options of example script\ndo_plot_data = True # if set to True, plot of fixation detection for each trial will be saved as a png file in the output folder.\n# the figures works best for short trials (up to around 20 seconds)\n\n# =============================================================================\n# OPTIONAL VARIABLES\n# The settings below may be used to adopt the default settings of the\n# algorithm. Do this only if you know what you're doing.\n\n# # STEFFEN INTERPOLATION\nopt['windowtimeInterp']     = 0.1                           # max duration (s) of missing values for interpolation to occur\nopt['edgeSampInterp']       = 2                             # amount of data (number of samples) at edges needed for interpolation\nopt['maxdisp']              = opt['xres']*0.2*np.sqrt(2)    # maximum displacement during missing for interpolation to be possible\n\n# # K-MEANS CLUSTERING\nopt['windowtime']           = 0.2                           # time window (s) over which to calculate 2-means clustering (choose value so that max. 1 saccade can occur)\nopt['steptime']             = 0.02                          # time window shift (s) for each iteration. Use zero for sample by sample processing\nopt['maxerrors']            = 100                           # maximum number of errors allowed in k-means clustering procedure before proceeding to next file\nopt['downsamples']          = [2, 5, 10]\nopt['downsampFilter']       = False                         # use chebychev filter when downsampling? Its what matlab's downsampling functions do, but could cause trouble (ringing) with the hard edges in eye-movement data\n\n# # FIXATION DETERMINATION\nopt['cutoffstd']            = 2.0                           # number of standard deviations above mean k-means weights will be used as fixation cutoff\nopt['onoffsetThresh']       = 3.0                           # number of MAD away from median fixation duration. Will be used to walk forward at fixation starts and backward at fixation ends to refine their placement and stop the algorithm from eating into saccades\nopt['maxMergeDist']         = 30.0                          # maximum Euclidean distance in pixels between fixations for merging\nopt['maxMergeTime']         = 30.0                          # maximum time in ms between fixations for merging\nopt['minFixDur']            = 40.0                          # minimum fixation duration after merging, fixations with shorter duration are removed from output\n\n\n\nRun I2MC\nNow we can finally run I2MC on all our files. To do so we will make for loop that will iterate between all the files and: - create a folder for each subject - import the data with the function created before - run I2MC on the file - save the output file and the plot.\n\n#%% Run I2MC\n\nfor file_idx, file in enumerate(data_files):\n    print('Processing file {} of {}'.format(file_idx + 1, len(data_files)))\n\n    # Extract the name form the file path\n    name = os.path.splitext(os.path.basename(file))[0]\n    \n    # Create the folder the specific subject\n    subj_folder = os.path.join(output_fodler, name)\n    if not os.path.isdir(subj_folder):\n       os.mkdir(subj_folder)\n       \n    # Import data\n    data = tobii_TX300(file, [opt['xres'], opt['yres']])\n\n    # Run I2MC on the data\n    fix,_,_ = I2MC.I2MC(data,opt)\n\n    ## Create a plot of the result and save them\n    if do_plot_data:\n        # pre-allocate name for saving file\n        save_plot = os.path.join(subj_folder, name+'.png')\n        f = I2MC.plot.data_and_fixations(data, fix, fix_as_line=True, res=[opt['xres'], opt['yres']])\n        # save figure and close\n        f.savefig(save_plot)\n        plt.close(f)\n\n    # Write data to file after make it a dataframe\n    fix['participant'] = name\n    fix_df = pd.DataFrame(fix)\n    save_file = os.path.join(subj_folder, name+'.csv')\n    fix_df.to_csv(save_file)",
    "crumbs": [
      "Eye-tracking",
      "Using I2MC for robust fixation extraction"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html#we-are-done",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html#we-are-done",
    "title": "Using I2MC for robust fixation extraction",
    "section": "WE ARE DONE!!!!!",
    "text": "WE ARE DONE!!!!!\nCongratulations on reaching this point! By now, you should have new files containing valuable information from I2MC.\nBut what exactly does I2MC tell us?\nI2MC provides us with a data frame that contains various pieces of information:\nWhat I2MC Returns:\n\ncutoff: A number representing the cutoff used for fixation detection.\nstart: An array holding the indices where fixations start.\nend: An array holding the indices where fixations end.\nstartT: An array containing the times when fixations start.\nendT: An array containing the times when fixations end.\ndur: An array storing the durations of fixations.\nxpos: An array representing the median horizontal position for each fixation in the trial.\nypos: An array representing the median vertical position for each fixation in the trial.\nflankdataloss: A boolean value (1 or 0) indicating whether a fixation is flanked by data loss (1) or not (0).\nfracinterped: A fraction that tells us the amount of data loss or interpolated data in the fixation data.\n\nIn simple terms, I2MC helps us understand where and for how long a person’s gaze remains fixed during an eye-tracking experiment. This is just the first step. Now that we have our fixations, we’ll need to use them to extract the information we’re interested in. Typically, this involves using the raw data to understand what was happening at each specific time point and using the data from I2MC to determine where the participant was looking at that time. This will be covered in a new tutorial. For now, you’ve successfully completed the preprocessing of your eye-tracking data, extracting a robust estimation of participants’ fixations!!\n\n\n\n\n\n\nWarning\n\n\n\nCaution: This tutorial is simplified and assumes the following:\n\nEach participant has only one file (1 trial).\nAll files contain data.\nThe data is relatively clean (I2MC won’t throw any errors).\nAnd so on.\n\nIf your data doesn’t match these assumptions, you may need to modify the script to handle any discrepancies.\nFor a more comprehensive example and in-depth usage, check out the I2MC repository. It provides a more complete example with data checks for missing data and potential errors. Now that you’ve understood the basics here, interpreting that example should be much easier. If you encounter any issues while running the script, you can give that example a try or reach out to us via email!!!",
    "crumbs": [
      "Eye-tracking",
      "Using I2MC for robust fixation extraction"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/I2MC_tutorial.html#entire-script",
    "href": "CONTENT/EyeTracking/I2MC_tutorial.html#entire-script",
    "title": "Using I2MC for robust fixation extraction",
    "section": "Entire script",
    "text": "Entire script\nTo make it simple here is the entire script that we wrote together!!!\n\nimport os\nimport glob\nimport I2MC\nimport pandas as pd\n\n\n# =============================================================================\n# Import data from Tobii TX300\n# =============================================================================\n\ndef tobii_TX300(fname, res=[1920,1080]):\n\n    # Load all data\n    raw_df = pd.read_csv(fname, delimiter=',')\n    df = pd.DataFrame()\n    \n    # Extract required data\n    df['time'] = raw_df['time']\n    df['L_X'] = raw_df['L_X']\n    df['L_Y'] = raw_df['L_Y']\n    df['R_X'] = raw_df['R_X']\n    df['R_Y'] = raw_df['R_Y']\n    \n    \n    ###\n    # Sometimes we have weird peaks where one sample is (very) far outside the\n    # monitor. Here, count as missing any data that is more than one monitor\n    # distance outside the monitor.\n    \n    # Left eye\n    lMiss1 = (df['L_X'] &lt; -res[0]) | (df['L_X']&gt;2*res[0])\n    lMiss2 = (df['L_Y'] &lt; -res[1]) | (df['L_Y']&gt;2*res[1])\n    lMiss  = lMiss1 | lMiss2 | (raw_df['L_V'] == False)\n    df.loc[lMiss,'L_X'] = np.nan\n    df.loc[lMiss,'L_Y'] = np.nan\n    \n    # Right eye\n    rMiss1 = (df['R_X'] &lt; -res[0]) | (df['R_X']&gt;2*res[0])\n    rMiss2 = (df['R_Y'] &lt; -res[1]) | (df['R_Y']&gt;2*res[1])\n    rMiss  = rMiss1 | rMiss2 | (raw_df['R_V'] == False)\n    df.loc[rMiss,'R_X'] = np.nan\n    df.loc[rMiss,'R_Y'] = np.nan\n\n    return(df)\n\n\n\n#%% Preparation\n\n# Settign the working directory\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD\\Workshop')\n\n# Find the files\ndata_files = glob.glob('DATA\\\\RAW\\\\**\\\\*.csv', recursive = True) # find all the files\n\n# define the output folder\noutput_fodler = 'DATA\\\\i2mc_output' # define folder path\\name\n\n# Create the outputfolder\nif not os.path.isdir(output_fodler):\n   os.mkdir(output_fodler)\n\n\n# =============================================================================\n# NECESSARY VARIABLES\n\nopt = {}\n# General variables for eye-tracking data\nopt['xres']         = 1920.0                # maximum value of horizontal resolution in pixels\nopt['yres']         = 1080.0                # maximum value of vertical resolution in pixels\nopt['missingx']     = np.nan                # missing value for horizontal position in eye-tracking data (example data uses -xres). used throughout the algorithm as signal for data loss\nopt['missingy']     = np.nan                # missing value for vertical position in eye-tracking data (example data uses -yres). used throughout algorithm as signal for data loss\nopt['freq']         = 300.0                 # sampling frequency of data (check that this value matches with values actually obtained from measurement!)\n\n# Variables for the calculation of visual angle\n# These values are used to calculate noise measures (RMS and BCEA) of\n# fixations. The may be left as is, but don't use the noise measures then.\n# If either or both are empty, the noise measures are provided in pixels\n# instead of degrees.\nopt['scrSz']        = [50.9174, 28.6411]    # screen size in cm\nopt['disttoscreen'] = 65.0                  # distance to screen in cm.\n\n# Options of example script\ndo_plot_data = True # if set to True, plot of fixation detection for each trial will be saved as png-file in output folder.\n# the figures works best for short trials (up to around 20 seconds)\n\n# =============================================================================\n# OPTIONAL VARIABLES\n# The settings below may be used to adopt the default settings of the\n# algorithm. Do this only if you know what you're doing.\n\n# # STEFFEN INTERPOLATION\nopt['windowtimeInterp']     = 0.1                           # max duration (s) of missing values for interpolation to occur\nopt['edgeSampInterp']       = 2                             # amount of data (number of samples) at edges needed for interpolation\nopt['maxdisp']              = opt['xres']*0.2*np.sqrt(2)    # maximum displacement during missing for interpolation to be possible\n\n# # K-MEANS CLUSTERING\nopt['windowtime']           = 0.2                           # time window (s) over which to calculate 2-means clustering (choose value so that max. 1 saccade can occur)\nopt['steptime']             = 0.02                          # time window shift (s) for each iteration. Use zero for sample by sample processing\nopt['maxerrors']            = 100                           # maximum number of errors allowed in k-means clustering procedure before proceeding to next file\nopt['downsamples']          = [2, 5, 10]\nopt['downsampFilter']       = False                         # use chebychev filter when downsampling? Its what matlab's downsampling functions do, but could cause trouble (ringing) with the hard edges in eye-movement data\n\n# # FIXATION DETERMINATION\nopt['cutoffstd']            = 2.0                           # number of standard deviations above mean k-means weights will be used as fixation cutoff\nopt['onoffsetThresh']       = 3.0                           # number of MAD away from median fixation duration. Will be used to walk forward at fixation starts and backward at fixation ends to refine their placement and stop algorithm from eating into saccades\nopt['maxMergeDist']         = 30.0                          # maximum Euclidean distance in pixels between fixations for merging\nopt['maxMergeTime']         = 30.0                          # maximum time in ms between fixations for merging\nopt['minFixDur']            = 40.0                          # minimum fixation duration after merging, fixations with shorter duration are removed from output\n\n\n\n#%% Run I2MC\n\nfor file_idx, file in enumerate(data_files):\n    print('Processing file {} of {}'.format(file_idx + 1, len(data_files)))\n\n    # Extract the name form the file path\n    name = os.path.splitext(os.path.basename(file))[0]\n    \n    # Create the folder the specific subject\n    subj_folder = os.path.join(output_fodler, name)\n    if not os.path.isdir(subj_folder):\n       os.mkdir(subj_folder)\n       \n    # Import data\n    data = tobii_TX300(file, [opt['xres'], opt['yres']])\n\n    # Run I2MC on the data\n    fix,_,_ = I2MC.I2MC(data,opt)\n\n    ## Create a plot of the result and save them\n    if do_plot_data:\n        # pre-allocate name for saving file\n        save_plot = os.path.join(subj_folder, name+'.png')\n        f = I2MC.plot.data_and_fixations(data, fix, fix_as_line=True, res=[opt['xres'], opt['yres']])\n        # save figure and close\n        f.savefig(save_plot)\n        plt.close(f)\n\n    # Write data to file after make it a dataframe\n    fix['participant'] = name\n    fix_df = pd.DataFrame(fix)\n    save_file = os.path.join(subj_folder, name+'.csv')\n    fix_df.to_csv(save_file)",
    "crumbs": [
      "Eye-tracking",
      "Using I2MC for robust fixation extraction"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/EyetrackingCalibration.html",
    "href": "CONTENT/EyeTracking/EyetrackingCalibration.html",
    "title": "Calibrating eye-tracking",
    "section": "",
    "text": "Before running an eye-tracking study, we usually need to do a calibration. What is a calibration you ask??\nWell, an eye-tracker works by using illuminators to emit near-infrared light towards the eye. This light reflects off the cornea, creating a visible glint or reflection that the eye-tracker’s sensors can detect. By capturing this reflection, the eye-tracker can determine the position of the eye relative to a screen, allowing it to pinpoint where the user is looking, known as the gaze point.\n\n\n\n\n\n\n\nImage from the tobii website\n\n\nThere are different ways to run a calibration using tobii eye-trackers. Here below the ones we have used:\n\n\nTobii offers a nice software called Tobii Pro, which allows you to run calibrations with considerable flexibility. However, Tobii Pro requires a paid license, which may not always be available to you. If you’re fortunate enough to have a paid license for Tobii Pro, by all means, take advantage of it. But if you don’t, don’t worry! Keep reading, as we have alternative solutions to share with you.\n\n\n\nAnother possibility is to use Tobii pro eye-tracker manager, an alternative software from Tobii. Unlike Tobii Pro, this application is free to use, though its features are more limited. If you’re looking to use Tobii software and don’t require advanced calibration options, this could be a suitable choice.\n\n\n\nFinally, if you are broke (like us!) and/or you want flexibility in running your calibration, one of the best option is to use the Tobii SDK that allows us to interact with the eyetrackers using Python. However, running a calibration using the SDK is pretty complex. We are trying to write a nice script to make it easier but in the meantime we have another solution…. We can use !! Psychopy tobii infants is a nice python code that wraps around the Tobii SDK making it easy to run a calibration, especailly an infant friendly one!! This code collection allows us to use the Tobii SDK with the Psychopy library.\nFinally, if you are broke and you want flexibility in running your calibration, one of the best options is to use the Tobii SDK. How we explained in previous tutorials (Create an eye-tracking experiment), this SDK allows the interaction with eye-trackers using Python. However, implementing a calibration using the SDK can be quite challenging……. We’re are trying to write an easy to explain and run code to perform calibrations, but in the meantime, we have an alternative solution!!\nEnter Psychopy_tobii_infant! This is a series of python functions that effectively wraps around the Tobii SDK, making it much easier to run calibrations, particularly those designed for infants. This code collection enables us to use the Tobii SDK in conjunction with the PsychoPy library, offering a more accessible approach to eye-tracking calibration!!\nIn the following paragraph we will see together how to prepare and use Psychopy tobii infant to run a nice and infant friendly calibration.\n\n\n\n\n\n\nNote\n\n\n\nUse Psychopy tobii infant to run studies\nOne question that you could ask yourself is… can I use Psychopy tobii infants to run my eye-tracking studies?? Well yes!! We have actually used it for few of our studies. Psychopy tobii infants provides an easy intarface between tobii eye-trackers and psychopy.\nOver the years we have personally move away from it but we often come back to it for some of its handy functionalities (e.g. the calibration).\n\n\n\n\nOk, let’s get started!! First thing first we need to download the codes of Psychopy tobii infant. You can find them here on GitHub: https://github.com/yh-luo/psychopy_tobii_infant.\nOn this page, you can click on &lt;&gt;Code and then on Download ZIP as shown below:\n\n\n\n\n\nPerfect!! Now that we have downloaded the code as a zip file we need to:\n\nextract the file\nidentify the folder “psychopy_tobii_infant”\ncopy this folder in the same location of your eye-tracking experiment script\n\nYou should end up like with something like this:\n\n\n\n\n\nNow we are all set and ready to go !!!!!!!!!!\n\n\n\nWe’ll now import the required libraries and custom functions. First, we’ll set the working directory to where our stimuli and the psychopy_tobii_infant folder are located. This allows Python to access these resources. Once set, we can easily import the necessary libraries and the custom functions from psychopy_tobii_infant for our eye-tracking experiment.\n\nimport os\nfrom psychopy import visual, sound\n\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\n\n# import Psychopy tobii infant\nfrom psychopy_tobii_infant import TobiiInfantController\n\nDone!! Now we can use its function to run our code.\n\n\n\nBefore running our calibration we need to prepare a few things!\nFirst, we create a nice window. As we explained before, this is the canvas where we will draw our stimuli.\n\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\nSecond we will import a few stimuli. I will explain later what we need them for, just trust me for now. If you want to follow along these are the stimuli we have used. Just download them and unzip them.\nOnce you have downloaded the stimuli we can use glob to find all the .png in the folder, read the video file and the audio file.\nThe .pngs are images of cute cartoon animals that we will use in the calibration.\n\n\n\n\n\nLet’s find them and put the in a list.\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\nPerfect we have found all the stimuli that we need.\nNow we will use the TobiiInfantController that we have imported before to connect to the eye-tracker. It is extremely simple:\n\nEyeTracker = TobiiInfantController(win)\n\nPerfect we now even have our eye-tracker!!\n\n\n\nThe first step for a proper calibration involves centering the participant’s eyes on the screen and ensuring their gaze is perpendicular to it. This positioning is crucial for accurate data collection.\nYou can see here two classic scenarios of infants in front of a eye-tracker. Again, the important thing is that the head and gaze of the child is perpendicular to the screen\n\n\n\n\n\nBut how to do it?? We can eyeball it but it’s not going to be easy. Luckily we can get some help!!\nUsing EyeTracker.show_status() we can see the eyes of the participant in relation to the screen. Therefore we can move either the eye-tracker or the infant to make sure the gaze is parallel to the screen. Sounds good doesn’t it??!!?? However, most of the times infants won’t look at the screen if nothing interesting is presented on it. Thus we need to capture their attention!!!!!\nWe can use the video we imported before. We set it to autodraw (each time the window is flipped a new frame will be drawn automatically) and the we start it by calling play().\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\nThis is what we will see:\n\n\n\n\n\nIn the center of the screen you will see the video we imported. In this case, a video of funny dancing fruits. But more importantly, at the top we can see the eyes of our participant. We need to make sure that we actually see the eyes and that they are centered in the screen. In addition is important to notice that under the eye position there is a green bar. The black line on it represents the distance of the participant head from the eye-tracker (the more on the right, the further from the screen; the more on the left, the closer to it). Ideally the black line should be in the center (overlapping the white thick line). This would indicate that the head of the participant is at an ideal distance (usually 65cm).\nOnce we are satisfied with the position of the eye-tracker and the infant we can press spacebar to proceed to the next step!! Before doing that we will stop the presentation of\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n\n\nOk, now that we are happy with the setup let’s actually run the calibration. It only takes two steps.\nFirst, we need to define the amount and the position of the points where we will present our stimuli. In infant studies we usually rely on 5 points. Four on the corners of the screen and one in the center.\nThe easiest way to define them is by using Psychopy’s normalized unit.\n\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\n\nAs we usually like to work in pixel units, we could define them in pixels or just transform our CALINORMP using the dimension of our window.\n\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nPerfect! Now we run our calibration by running\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound)\n\nOk, but what is going to happen??? After running this line we will start our calibration. Using the numbers on our keyboard (use 1~9 (depending on the number of calibration points) to present) we will show 1 of the .png stimuli that we have listed in CALISTIMS in the selected position. The presentation of the cartoon will be accompanied by the sound passed with Sound. The cartoon will start to zoom in and out in the attempt to capture the attention of the infant.\n\n\n\n\n\nOnce we are confident that the infant is looking at the cartoon zooming in and out you can press Space to collect the first calibration sample. Once that’s been done, we can move to the following points. Once all point are done we can press Enter. This will show the result of our calibration, e.g:\n\n\n\n\n\nThe red and green lines represent the gaze data from the left and right eyes, respectively. Each cluster of lines corresponds to a calibration point, with the central cluster indicating the main fixation point. Tight convergence of the lines within each cluster signifies high calibration accuracy and precision, while any significant discrepancies or spread indicate potential calibration issues.\nAs you may have noticed, the bottom right corner shows no data. What happened?? Well probably the infant was not looking when we pressed the Spacebar. How to fix it? We can simply press the number on the keyboard related to the point that failed and then press Spacebar. This will restart the calibration for only that point. So we can focus on getting more data for this point to add to our already collected.\nIn case more points are without data –or the data is just bad– we can select multiple points or even all of them (pressing 0). If we are happy with our calibration we can just press the Spacebar without selecting any points.\n\n\n\n\n\n\nTip\n\n\n\nThe match between the infant eye and the position on the screen is made in the exact moment you press the Space button. If you wait too long to press it, the infant might look away and beat you to it! It may appear counterintuitive, but we usually prefer to be quite fast in our calibration process. Since duration doesn’t matter but timing does, you will get better results! The fast switching seems to also capture infants’ attention better, meaning that they are more likely to follow the location of the cartoon with their gaze.\nRemember, infants can easily get bored with our stimuli. Thus, being rapid and sudden in showing them may work to our advantage.\n\n\nWell done!! Calibration is done!!!!\nNow we can start with our eye-tracking study!\nHere below the entire code.\n\nimport os\nfrom psychopy import visual, sound\n\n# import Psychopy tobii infant\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\nfrom psychopy_tobii_infant import TobiiInfantController\n\n\n#%% window and stimuli\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\n\n#%% Center face - screen\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n#%% Calibration\n\n# define calibration points\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound)\nwin.flip()",
    "crumbs": [
      "Eye-tracking",
      "Calibrating eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/EyetrackingCalibration.html#tobii-pro",
    "href": "CONTENT/EyeTracking/EyetrackingCalibration.html#tobii-pro",
    "title": "Calibrating eye-tracking",
    "section": "",
    "text": "Tobii offers a nice software called Tobii Pro, which allows you to run calibrations with considerable flexibility. However, Tobii Pro requires a paid license, which may not always be available to you. If you’re fortunate enough to have a paid license for Tobii Pro, by all means, take advantage of it. But if you don’t, don’t worry! Keep reading, as we have alternative solutions to share with you.",
    "crumbs": [
      "Eye-tracking",
      "Calibrating eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/EyetrackingCalibration.html#tobii-pro-eye-tracker-manager",
    "href": "CONTENT/EyeTracking/EyetrackingCalibration.html#tobii-pro-eye-tracker-manager",
    "title": "Calibrating eye-tracking",
    "section": "",
    "text": "Another possibility is to use Tobii pro eye-tracker manager, an alternative software from Tobii. Unlike Tobii Pro, this application is free to use, though its features are more limited. If you’re looking to use Tobii software and don’t require advanced calibration options, this could be a suitable choice.",
    "crumbs": [
      "Eye-tracking",
      "Calibrating eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/EyetrackingCalibration.html#psychopy-tobii-infant",
    "href": "CONTENT/EyeTracking/EyetrackingCalibration.html#psychopy-tobii-infant",
    "title": "Calibrating eye-tracking",
    "section": "",
    "text": "Finally, if you are broke (like us!) and/or you want flexibility in running your calibration, one of the best option is to use the Tobii SDK that allows us to interact with the eyetrackers using Python. However, running a calibration using the SDK is pretty complex. We are trying to write a nice script to make it easier but in the meantime we have another solution…. We can use !! Psychopy tobii infants is a nice python code that wraps around the Tobii SDK making it easy to run a calibration, especailly an infant friendly one!! This code collection allows us to use the Tobii SDK with the Psychopy library.\nFinally, if you are broke and you want flexibility in running your calibration, one of the best options is to use the Tobii SDK. How we explained in previous tutorials (Create an eye-tracking experiment), this SDK allows the interaction with eye-trackers using Python. However, implementing a calibration using the SDK can be quite challenging……. We’re are trying to write an easy to explain and run code to perform calibrations, but in the meantime, we have an alternative solution!!\nEnter Psychopy_tobii_infant! This is a series of python functions that effectively wraps around the Tobii SDK, making it much easier to run calibrations, particularly those designed for infants. This code collection enables us to use the Tobii SDK in conjunction with the PsychoPy library, offering a more accessible approach to eye-tracking calibration!!\nIn the following paragraph we will see together how to prepare and use Psychopy tobii infant to run a nice and infant friendly calibration.\n\n\n\n\n\n\nNote\n\n\n\nUse Psychopy tobii infant to run studies\nOne question that you could ask yourself is… can I use Psychopy tobii infants to run my eye-tracking studies?? Well yes!! We have actually used it for few of our studies. Psychopy tobii infants provides an easy intarface between tobii eye-trackers and psychopy.\nOver the years we have personally move away from it but we often come back to it for some of its handy functionalities (e.g. the calibration).\n\n\n\n\nOk, let’s get started!! First thing first we need to download the codes of Psychopy tobii infant. You can find them here on GitHub: https://github.com/yh-luo/psychopy_tobii_infant.\nOn this page, you can click on &lt;&gt;Code and then on Download ZIP as shown below:\n\n\n\n\n\nPerfect!! Now that we have downloaded the code as a zip file we need to:\n\nextract the file\nidentify the folder “psychopy_tobii_infant”\ncopy this folder in the same location of your eye-tracking experiment script\n\nYou should end up like with something like this:\n\n\n\n\n\nNow we are all set and ready to go !!!!!!!!!!\n\n\n\nWe’ll now import the required libraries and custom functions. First, we’ll set the working directory to where our stimuli and the psychopy_tobii_infant folder are located. This allows Python to access these resources. Once set, we can easily import the necessary libraries and the custom functions from psychopy_tobii_infant for our eye-tracking experiment.\n\nimport os\nfrom psychopy import visual, sound\n\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\n\n# import Psychopy tobii infant\nfrom psychopy_tobii_infant import TobiiInfantController\n\nDone!! Now we can use its function to run our code.\n\n\n\nBefore running our calibration we need to prepare a few things!\nFirst, we create a nice window. As we explained before, this is the canvas where we will draw our stimuli.\n\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\nSecond we will import a few stimuli. I will explain later what we need them for, just trust me for now. If you want to follow along these are the stimuli we have used. Just download them and unzip them.\nOnce you have downloaded the stimuli we can use glob to find all the .png in the folder, read the video file and the audio file.\nThe .pngs are images of cute cartoon animals that we will use in the calibration.\n\n\n\n\n\nLet’s find them and put the in a list.\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\nPerfect we have found all the stimuli that we need.\nNow we will use the TobiiInfantController that we have imported before to connect to the eye-tracker. It is extremely simple:\n\nEyeTracker = TobiiInfantController(win)\n\nPerfect we now even have our eye-tracker!!\n\n\n\nThe first step for a proper calibration involves centering the participant’s eyes on the screen and ensuring their gaze is perpendicular to it. This positioning is crucial for accurate data collection.\nYou can see here two classic scenarios of infants in front of a eye-tracker. Again, the important thing is that the head and gaze of the child is perpendicular to the screen\n\n\n\n\n\nBut how to do it?? We can eyeball it but it’s not going to be easy. Luckily we can get some help!!\nUsing EyeTracker.show_status() we can see the eyes of the participant in relation to the screen. Therefore we can move either the eye-tracker or the infant to make sure the gaze is parallel to the screen. Sounds good doesn’t it??!!?? However, most of the times infants won’t look at the screen if nothing interesting is presented on it. Thus we need to capture their attention!!!!!\nWe can use the video we imported before. We set it to autodraw (each time the window is flipped a new frame will be drawn automatically) and the we start it by calling play().\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\nThis is what we will see:\n\n\n\n\n\nIn the center of the screen you will see the video we imported. In this case, a video of funny dancing fruits. But more importantly, at the top we can see the eyes of our participant. We need to make sure that we actually see the eyes and that they are centered in the screen. In addition is important to notice that under the eye position there is a green bar. The black line on it represents the distance of the participant head from the eye-tracker (the more on the right, the further from the screen; the more on the left, the closer to it). Ideally the black line should be in the center (overlapping the white thick line). This would indicate that the head of the participant is at an ideal distance (usually 65cm).\nOnce we are satisfied with the position of the eye-tracker and the infant we can press spacebar to proceed to the next step!! Before doing that we will stop the presentation of\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n\n\nOk, now that we are happy with the setup let’s actually run the calibration. It only takes two steps.\nFirst, we need to define the amount and the position of the points where we will present our stimuli. In infant studies we usually rely on 5 points. Four on the corners of the screen and one in the center.\nThe easiest way to define them is by using Psychopy’s normalized unit.\n\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\n\nAs we usually like to work in pixel units, we could define them in pixels or just transform our CALINORMP using the dimension of our window.\n\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nPerfect! Now we run our calibration by running\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound)\n\nOk, but what is going to happen??? After running this line we will start our calibration. Using the numbers on our keyboard (use 1~9 (depending on the number of calibration points) to present) we will show 1 of the .png stimuli that we have listed in CALISTIMS in the selected position. The presentation of the cartoon will be accompanied by the sound passed with Sound. The cartoon will start to zoom in and out in the attempt to capture the attention of the infant.\n\n\n\n\n\nOnce we are confident that the infant is looking at the cartoon zooming in and out you can press Space to collect the first calibration sample. Once that’s been done, we can move to the following points. Once all point are done we can press Enter. This will show the result of our calibration, e.g:\n\n\n\n\n\nThe red and green lines represent the gaze data from the left and right eyes, respectively. Each cluster of lines corresponds to a calibration point, with the central cluster indicating the main fixation point. Tight convergence of the lines within each cluster signifies high calibration accuracy and precision, while any significant discrepancies or spread indicate potential calibration issues.\nAs you may have noticed, the bottom right corner shows no data. What happened?? Well probably the infant was not looking when we pressed the Spacebar. How to fix it? We can simply press the number on the keyboard related to the point that failed and then press Spacebar. This will restart the calibration for only that point. So we can focus on getting more data for this point to add to our already collected.\nIn case more points are without data –or the data is just bad– we can select multiple points or even all of them (pressing 0). If we are happy with our calibration we can just press the Spacebar without selecting any points.\n\n\n\n\n\n\nTip\n\n\n\nThe match between the infant eye and the position on the screen is made in the exact moment you press the Space button. If you wait too long to press it, the infant might look away and beat you to it! It may appear counterintuitive, but we usually prefer to be quite fast in our calibration process. Since duration doesn’t matter but timing does, you will get better results! The fast switching seems to also capture infants’ attention better, meaning that they are more likely to follow the location of the cartoon with their gaze.\nRemember, infants can easily get bored with our stimuli. Thus, being rapid and sudden in showing them may work to our advantage.\n\n\nWell done!! Calibration is done!!!!\nNow we can start with our eye-tracking study!\nHere below the entire code.\n\nimport os\nfrom psychopy import visual, sound\n\n# import Psychopy tobii infant\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\nfrom psychopy_tobii_infant import TobiiInfantController\n\n\n#%% window and stimuli\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\n\n#%% Center face - screen\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n#%% Calibration\n\n# define calibration points\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound)\nwin.flip()",
    "crumbs": [
      "Eye-tracking",
      "Calibrating eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/ContentList.html",
    "href": "CONTENT/ContentList.html",
    "title": "Content listing",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nStarting with Python\n\n\n\nSetup\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStarting with PsychoPy\n\n\n\nSetup\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCreate your first paradigm\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to eye-tracking\n\n\n\nEye-tracking\n\n\nTheory\n\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCreate an eye-tracking experiment\n\n\n\nEye-tracking\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing I2MC for robust fixation extraction\n\n\n\nEye-tracking\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nJul 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nFrom fixations to measures\n\n\n\nEye-tracking\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nSep 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCalibrating eye-tracking\n\n\n\nEye-tracking\n\n\nPython\n\n\n\n\n\n\n\n\n\n\nOct 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPupil data pre-processing\n\n\n\nPupillometry\n\n\nR\n\n\nPre-processing\n\n\n\n\n\n\n\n\n\n\nDec 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPupil Data Analysis\n\n\n\nStats\n\n\nR\n\n\nModelling\n\n\nAdditive models\n\n\n\n\n\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html",
    "title": "Create an eye-tracking experiment",
    "section": "",
    "text": "This page will show you how to collect eye-tracking data in a simple Psychopy paradigm. We will use the same paradigm that we built together in the Getting started with Psychopy tutorial. If you have not done that tutorial yet, please go through it first.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#install",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#install",
    "title": "Create an eye-tracking experiment",
    "section": "Install",
    "text": "Install\nTo install the Python Tobii SDK, we can simply run this command in our conda terminal:\npip install tobii_research\nGreat! We have installed the Tobii SDK.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#connect-to-the-eye-tracker",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#connect-to-the-eye-tracker",
    "title": "Create an eye-tracking experiment",
    "section": "Connect to the eye-tracker",
    "text": "Connect to the eye-tracker\nSo how does this library work, how do we connect to the eye-tracker and collect our data? Very good questions!\nThe tobii_research documentation is quite extensive and describes in detail a lot of functions and data classes that are very useful. However, we don’t need much to start our experiment.\nFirst we need to identify all the eye trackers connected to our computer. Yes, plural, tobii_research will return a list of all the eye trackers connected to our computer. 99.99999999% of the time you will only have 1 eye tracker connected, so we can just select the first (and usually only) eye tracker found.\n\n# Import tobii_research library\nimport tobii_research as tr\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\nPerfect!! We have identified our eye-trackers, and we have selected the first one (and only).\nWe are now ready to use our eye-tracker to collect some data… but how?",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#collect-data",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#collect-data",
    "title": "Create an eye-tracking experiment",
    "section": "Collect data",
    "text": "Collect data\nTobii_research has a cool way of telling us what data we are collecting at each time point. It uses a callback function. What is a callback function, you ask? It is a function that tobii runs each time it has a new data point. Let’s say we have an eye tracker that collects data at 300Hz (300 samples per second): the function will be called every time the tobii has one of those 300 samples.\nThis callback function will give us a gaze_data object. This object contains multiple information of that collected sample and we can simply select the information we care about. In our case we want:\n\nthe system_time_stamp, our time variable\nthe left_eye.gaze_point.position_on_display_area, it contains the coordinates on the screen of the left eye (both x and y)\nthe right_eye.gaze_point.position_on_display_area, it contains the coordinates on the screen of the right eye (both x and y)\nthe left_eye.pupil.diameter, is the pupil diameter of the left eye\nthe right_eye.pupil.diameter, is the pupil diameter of the right eye\nthe left_eye.gaze_point.validity, this is a value that tells us whether we think the recognition is ok or not\n\nHere is our callback function:\n\ndef gaze_data_callback(gaze_data):\n\n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0]\n    ly = gaze_data.left_eye.gaze_point.position_on_display_area[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0]\n    ry = gaze_data.right_eye.gaze_point.position_on_display_area[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n\nHow we said, this function will be called every time tobii has a new data-point. COOL!! Now we need to tell tobii to run this function we have created. This is very simple, we can just do the following:\n\n# Start the callback function\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\nWe are telling tobii that we are interested in the EYETRACKER_GAZE_DATA and that we want it to pass it to our function gaze_data_callback.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#triggersevents",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#triggersevents",
    "title": "Create an eye-tracking experiment",
    "section": "Triggers/Events",
    "text": "Triggers/Events\nAs we have seen, our callback function can access the tobii data and tell us what it is for each sample. Just one little piece missing… We want to know what we presented and when. In most studies, we present stimuli that can be pictures, sounds or even videos. For the following analysis, it is important to know at what exact point in time we presented these stimuli.\nLuckily there is a simple way we can achieve this. We can set a text string that our callback function can access and include in our data. To make sure that our callback function can access this variable we will use the global keyword. This makes sure that we can read/modify a variable that exists outside of the function.\nIn this way, each time the callback function will be run, it will also have access to the trigger variable. We save the trigger to the ev variable and we set it back to an empty string \"\" .\nThis means that we can set the trigger to whatever string we want and when we set it it will be picked up by the callback function.\n\ndef gaze_data_callback(gaze_data):\n    global trigger\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n    \n    \ntrigger = ''\n\n# Time passes\n# when you present a stimulus you can set trigger to a string that will be captured by the callabck function\n\ntrigger = 'Presented Stimulus'\n\nPerfect! Now we have 1. a way to access the data from the eye-tracker and 2. know exactly what stimuli we are presenting the participant and when.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#correct-the-data",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#correct-the-data",
    "title": "Create an eye-tracking experiment",
    "section": "Correct the data",
    "text": "Correct the data\nTobii presents gaze data in a variety of formats. The one we’re most interested in is the Active Display Coordinate System (ADCS). This system maps all gaze data onto a 2D coordinate system that aligns with the Active Display Area. When an eye tracker is used with a monitor, the Active Display Area refers to the display area that doesn’t include the monitor frame. In the ADCS system, the point (0, 0) represents the upper left corner of the screen, and (1, 1) represents the lower right corner.\nWhile this coordinate system is perfectly acceptable, it might cause some confusion when we come to analyze and plot the data. This is because in most systems, the data’s origin is located in the lower left corner, not the upper left. It might seem a bit complicated, but the image below will make everything clear.\n\n\n\n\n\nFor this reason, we typically adjust the data to position the origin in the bottom left corner. This can be achieved by subtracting the gaze coordinates from the maximum window height size.\nIn addition to the origin point issue, the gaze coordinates are reported between 0 and 1. It’s often more convenient to handle data in pixels, so we can transform our data into pixels. This is done by multiplying the obtained coordinates by the pixel dimensions of our screen.\nLastly, we also modify the time column. Tobii provides data in microseconds, but such precision isn’t necessary for our purposes, so we convert it to milliseconds by dividing by 1000.\n\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global winsize\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n    \n    \ntrigger = ''\nwinsize = (1920, 1080)",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#save-the-data",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#save-the-data",
    "title": "Create an eye-tracking experiment",
    "section": "Save the data",
    "text": "Save the data\nYou might have noticed that we get the data in our callback function but don’t actually save it anywhere. So how to save them? There are two main approaches we can use:\n\nWe could have a saving function inside our callback that could append the new data to a .csv each time the callback is called.\nWe could append the data to a list. Once the experiment is finished we could save our data.\n\nThese two approaches have however some weaknesses. The first could slow down the callback function if our PC is not performing or if we are sampling at a very high sampling rate. The second is potentially faster, but if anything happens during the study which makes python crash (and trust me, it can happen…..) you would lose all your data.\nWhat is the solution you ask? A mixed approach!!!!!!!\nWe can store our data in a list and save it during the less important parts of the study, for example the Inter Stimulus Interval (the time between a trial and another). So let’s write a function to do exactly that.\nLets first create an empty list that we will fill with out data from the callback function. As before, we make sure that our callback function can access this list and append the new data we will use the global keyword.\n\n# Create an empty list we will append our data to\ngaze_data_buffer = []\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global gaze_data_buffer\n    global trigger\n    global winsize\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n        \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,ev))\n\nNow the gaze_data_buffer will be filled with the data we extract. Let’s save this list then.\nWe will first make a copy of the list, and then empty the original list. In this way, we have our data stored, while the original list is empty and can be filled with new data.\nAfter creating a copy, we use pandas to transform the list into a data frame and save it to a csv file. Using mode = 'a' we tell pandas to append the new data to the existing .csv. If this is the first time we are trying to save the data the .csv does not yet exist, so pandas will create a new csv instead.\n\ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names for the dataframe\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = not os.path.isfile(output_path)\n    \n    # Write the DataFrame to an HDF5 file\n    out.to_csv(output_path, mode='a', index =False, header = file_exists)",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#short-recap-of-the-paradigm",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#short-recap-of-the-paradigm",
    "title": "Create an eye-tracking experiment",
    "section": "Short recap of the paradigm",
    "text": "Short recap of the paradigm\nAs we already mentioned, we will use the experimental design that we created in Getting started with Psychopy as a base and we will add things to it to make it an eye-tracking study. If you don’t remember the paradigm please give it a rapid look as we will not go into much detail about each specific part of it.\nHere a very short summary of what the design was: After a fixation cross, two shapes can be presented: a circle or a square. The circle indicates that a reward will appear on the right of the screen while the square predicts the appearance of an empty cloud on the left.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#combine-things",
    "href": "CONTENT/EyeTracking/CreateAnEyetrackingExperiment.html#combine-things",
    "title": "Create an eye-tracking experiment",
    "section": "Combine things",
    "text": "Combine things\nLet’s try to build together the experiment then.\n\nImport and functions\nTo start, let’s import the libraries and define the two functions that we create before\n\nimport os\nimport glob\nimport pandas as pd\n\n# Import some libraries from PsychoPy\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound\n\nimport tobii_research as tr\n\n\n#%% Functions\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global gaze_data_buffer\n    global winsize\n\n    if len(trigger)==0:\n        ev = ''\n    else:\n        ev = trigger\n        trigger=[]\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,ev))\n    \n        \ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = not os.path.isfile(output_path)\n    \n    # Write the DataFrame to an HDF5 file\n    out.to_csv(output_path, mode='a', index =False, header = file_exists)\n\n\n\nLoad the stimuli\nNow we are going to set a few settings, such as the screen size, create a Psychopy window, load the stimuli and then prepare the trial definition. This is exactly the same as we did in the previous Psychopy tutorial.\n\n#%% Load and prepare stimuli\n\n# Setting the directory of our experiment\nos.chdir(r'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\TomassoGhilardi\\PersonalProj\\BCCCD')\n\n\n# Winsize\nwinsize = (1920, 1080)\n\n# create a window\nwin = visual.Window(size = winsize,fullscr=True, units=\"pix\", pos =(0,30), screen=1)\n\n# Load images\nfixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\ncircle   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\circle.png', size = (200, 200))\nsquare   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\square.png', size = (200, 200))\nwinning   = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\winning.png', size = (200, 200), pos=(560,0))\nloosing  = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\loosing.png', size = (200, 200), pos=(-560,0))\n\n# Load sound\nwinning_sound = sound.Sound('EXP\\\\Stimuli\\\\winning.wav')\nlosing_sound = sound.Sound('EXP\\\\Stimuli\\\\loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, loosing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\n\nStart recording\nNow we are ready to look for the eye-trackers connected to the computer and select the first one that we find. Once we have selected it, we will launch our callback function to start collecting data.\n\n#%% Record the data\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\n#Start recording\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\n\n\nPresent our stimuli\nThe eye-tracking is running! Let’s show our participant something!\nAs you can see below, after each time we flip our window (remember: flipping means we actually show what we drew), we set the trigger variable to a string that identifies the specific stimulus we are presenting. This will be picked up our callback function.\n\n#%% Trials\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n        \n    fixation.draw()\n    win.flip()\n    trigger = 'Fixation'\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    if trial ==0:\n        trigger = 'Circle'\n    else:\n        trigger = 'Square'\n\n    core.wait(3)  # wait for 3 seconds\n    win.flip()\n\n    ### Wait for saccadic latencty\n    core.wait(0.75)\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n    if trial ==0:\n        trigger = 'Reward'\n    else:\n        trigger = 'NoReward'\n    core.wait(2)  # wait for 1 second\n    win.flip()    # we re-flip at the end to clean the window\n    \n    ### ISI\n    clock = core.Clock()\n    while clock.getTime() &lt; 1:\n        pass\n    \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        core.quit()  # stop study\n\nAs we said before in Save the data, it is best to save the data during our study to avoid any potential data loss. And it is better to do this when there are things of minor interest, such as an ISI. If you remember, in the previous tutorial: Getting started with Psychopy, we created the ISI in a different way than just a clock.wait() and we said that this different method would come in handy later on. This is the moment!\nOur ISI starts the clock and checks when 1 second has passed after starting this clock. Why is this important? Because we can save the data after starting the clock. Since the time that it will take will be variable, we will be simply check how much time has passed after saving the data and wait (using the while clock.getTime() &lt; 1:  pass code) until 1 second has fully passed. This will ensure that we will wait for 1 second in total considering the saving of the data.\n\n### ISI\nclock = core.Clock()\nwrite_buffer_to_file(gaze_data_buffer, 'DATA\\\\RAW\\\\'+ Sub +'.csv')\nwhile clock.getTime() &lt; 1:\n    pass\n\n\n\n\n\n\n\nWarning\n\n\n\nCarefull!!!\nIf saving the data takes more than 1 second, your ISI will also be longer. However, this should not be the case with typical studies where trials are not too long. Nonetheless, it’s always a good idea to keep an eye out.\n\n\n\n\nStop recording\nWe’re almost there! We have imported our functions, started collecting data, sent the triggers, and saved the data. The last step will be stop data collection (or python will keep getting an endless amount of data from the eye tracker!). Do do that, We simply unsubscribe from the eye tracker to which we had subscribed to start of data collection:\n\nwin.close()\nEyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\nNote that we also closed the Psychopy window, so that the stimulus presentation is also officially over. Well done!!! Now go and get your data!!! We’ll see you back when it’s time to analyze it.",
    "crumbs": [
      "Eye-tracking",
      "Create an eye-tracking experiment"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html",
    "title": "From fixations to measures",
    "section": "",
    "text": "In the previous two tutorials we collected some eye-tracking data and then we used I2MC to extract the fixations from that data. Let’s load the data we recorded and pre-processed in the previous tutorial. We will import some libraries and read the raw data and the output from I2MC.\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n#%% Settings\n\n# Screen resolution\nscreensize = (1920, 1080) \n\n\n#%% Read and prepare data\n\n# The fixation data extracted from I2MC\nFixations = pd.read_csv('..\\\\..\\\\resources\\\\FromFixationToData\\\\DATA\\\\i2mc_output\\\\Adult1\\\\Adult1.csv')\n\n# The original RAW data\nRaw_data = pd.read_csv('..\\\\..\\\\resources\\\\FromFixationToData\\\\DATA\\\\RAW\\\\Adult1.csv')\nWhat can we do with just the raw data and the fixations? Not much I am afraid. But we can use these fixations to extract some more meaningful indexes.\nIn this tutorial, we will look at how to extract two variables from our paradigm:\nSo what do these two measures have in common? pregnant pause for you to answer EXACTLY!!! They are both clearly related to the position of our stimuli. For this reason, it is important to define Areas Of Interest (AOIs) on the screen (for example, the locations of the targets). Defining AOIs will allow us to check, for each single fixation, whether it happened in an area that we are interested in.",
    "crumbs": [
      "Eye-tracking",
      "From fixations to measures"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html#define-aois",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html#define-aois",
    "title": "From fixations to measures",
    "section": "Define AOIs",
    "text": "Define AOIs\nLet’s define AOIs. We will define two squares around the target locations. To do this, we can simply pass two coordinates for each AOI: the lower left corner and the upper right corner of an imaginary square.\nAn important point to understand is that tobii and Psychopy use two different coordinate systems:\n\nPsychopy has its origin (0,0) in the centre of the window/screen by default.\nTobii reports data with its origin (0,0) in the lower left corner.\n\nThis inconsistency is not a problem per se, but we need to take it into account when defining the AOIs. Let’s try to define the AOIs:\n\n# Screen resolution\nscreensize = (1920, 1080) \n\n# Define the variable realted to AOIs and target position\ndimension_of_AOI = 600/2  #the dimension of the AOIs, divided by 2\nTarget_position = 500 #the position of the targets relative to the centre (e.g., 500 pixels on the right from the centre)\n\n# Create areas of intescreensizet\nAOI1 =[[screensize[0]/2 - Target_position - dimension_of_AOI, screensize[1]/2-dimension_of_AOI], [screensize[0]/2 - Target_position + dimension_of_AOI, screensize[1]/2 + dimension_of_AOI]]\n\nAOI2 =[[screensize[0]/2 + Target_position - dimension_of_AOI, screensize[1]/2-dimension_of_AOI], [screensize[0]/2 + Target_position + dimension_of_AOI, screensize[1]/2 + dimension_of_AOI]]\n\nAOIs = [AOI1, AOI2]\n\nNice!! This step is essential. We have created two AOIs. We will use them to define whether each fixation of our participant was within either of these two AOIs. Let’s get a better idea by just plotting these two AOIs and two random points (600, 500) and (1400,1000).\n\n\nCode\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create a figure\nfig, ax = plt.subplots(1, figsize=(8,4.4))\n\n# Set the limits of the plot\nax.set_xlim(0, 1920)\nax.set_ylim(0, 1080)\n\n# Define the colors for the rectangles\ncolors = ['#46AEB9', '#C7D629']\n\n# Create a rectangle for each area of interest and add it to the plot\nfor i, (bottom_left, top_right) in enumerate(AOIs):\n    width = top_right[0] - bottom_left[0]\n    height = top_right[1] - bottom_left[1]\n    rectangle = patches.Rectangle(bottom_left, width, height, linewidth=2, edgecolor='k', facecolor=colors[i])\n    ax.add_patch(rectangle)\n\nax.plot(600,500,marker='o', markersize=8, color='green')    \nax.plot(1400,1000,marker='o', markersize=8, color='red')    \n\n# Show the plot\nplt.show()",
    "crumbs": [
      "Eye-tracking",
      "From fixations to measures"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html#points-in-aois",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html#points-in-aois",
    "title": "From fixations to measures",
    "section": "Points in AOIs",
    "text": "Points in AOIs\nAs you can see, we are plotting the two AOIs and two points. One falls into one of them and the other doesn’t. But how can we get Python to tell us if a point falls within one of our AOIs?\nWe can check whether the (x, y) coordinates of the point are within the x and y coordinates of the left bottom and top right corners of the AOI.\n\n\n\n\n\nSo imagine we have a point: point and an area: area, we can check if the point falls inside the area by:\n\n# Extract bottom left and top right points\nbottom_left, top_right = area\n\n# Extract the x and y of each point\nbottom_x, bottom_y = bottom_left\ntop_x, top_y = top_right\n\n# Extract the x and y of our point of interest\nx, y = point\n\n# Check if the point is in the area\nbottom_x &lt;= x &lt;= top_x and bottom_y &lt;= y &lt;= top_y\n\nPerfect, this will return True if the point falls inside the area and False if it falls outside. Since we have two AOIs and not just one, we want to make things a bit fancier. We will create a function that checks if a point falls within a list of areas, and tells us which area it falls in.\nWe will run the code above in a loop using enumerate. This extracts two elements to our loop: the index of the element and the element itself. In our case the index of the area and the area itself. This is very useful as we can then use both of these two pieces of information. We will use the actual area to check if our points fall into it. Then, if it does, we will return the index of that area. Conversely, if the point doesn’t fall in any area the function will return -1.\n\n# We define a function that simply takes the a point and a list of areas.\n# This function checks in which area this point is and return the index\n# of the area. If the point is in no area it returns -1\ndef find_area_for_point(point, areas):\n\n    for i, area in enumerate(areas):\n        # Extract bottom left and top right points\n        bottom_left, top_right = area\n        \n        # Extract the x and y of each point\n        bottom_x, bottom_y = bottom_left\n        top_x, top_y = top_right\n        \n        # Extract the x and y of our point of interest\n        x, y = point\n        \n        # Check if the point is in the area\n        if bottom_x &lt;= x &lt;= top_x and bottom_y &lt;= y &lt;= top_y :\n            return(i)\n    return(-1)\n\nNow we have a cool function to check whether a point falls into any of our AOIs. We can use this function to filter the fixations that are in the AOIs: These are the only ones we care about.",
    "crumbs": [
      "Eye-tracking",
      "From fixations to measures"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/FromFixationsToData.html#first-fixation",
    "href": "CONTENT/EyeTracking/FromFixationsToData.html#first-fixation",
    "title": "From fixations to measures",
    "section": "First fixation",
    "text": "First fixation\nWe have all these latency values, but we only want the first/fastest of each trial. How can we extract this information easily? We will use groupby. Groupby allows us to perform specific functions/commands on grouped sections of a data frame.\nHere we will groupby by Events and Event_trials and for each of these grouped pieces of dataframe we will extract the smallest (min()) value of latency.\n\n# We extract the first fixation of our dataframe for each event\nSaccadicLatency = Correct_Target_fixations.groupby(['Event', 'Event_trial'])['Latency'].min().reset_index()\n\nHere we have our Saccadic latency!!!\nOnce our dataset is ready, we might want to visualise the data. For example, we can plot how saccadic latency changes across trials with seaborn:\n\nimport seaborn as sns # import seaborn\nplt.figure()\n\n# Scatterplot\nax = sns.scatterplot(data=SaccadicLatency, x=\"Event_trial\", y=\"Latency\", hue='Event')\n\n# Place the legend \nsns.move_legend(ax,loc=\"upper left\")\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "Eye-tracking",
      "From fixations to measures"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/Intro_eyetracking.html",
    "href": "CONTENT/EyeTracking/Intro_eyetracking.html",
    "title": "Introduction to eye-tracking",
    "section": "",
    "text": "Eye tracking is a great tool to study cognition. It is especially suitable for developmental studies, as infants and young children might have advanced cognitive abilities, but little chances to show them (they cannot talk!).\nThis tutorial will teach you all you need to navigate the huge and often confusing eye-tracking world. First, we will introduce how an experiment can (and should) be built, explaining how to easily record eye-tracking data from Python. Then, it will focus on how to analyse the data, reducing the seemingly overwhelming amount of rows and columns in a few variables of interest (such as saccadic latency, looking time, or pupil dilation).",
    "crumbs": [
      "Eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/Intro_eyetracking.html#what-do-you-want-to-measure",
    "href": "CONTENT/EyeTracking/Intro_eyetracking.html#what-do-you-want-to-measure",
    "title": "Introduction to eye-tracking",
    "section": "What do you want to measure?",
    "text": "What do you want to measure?\n\nLooking time\nIt is much easier to start an eye-tracking project if you have a clear idea of what you want to measure. Classic paradigms on infant research rely on looking time (How long are infants attending a given stimulus?) and are often called Violation-of-Expectation tasks. They familiarize infants with a given stimulus or situation (e.g. a cat) and, after a given number of presentations (e.g., 8), they present a different stimulus (e.g., a dog). Do infants look longer at the dog, compared to the cat? If so, they were able to spot that something changed.\n\n\n\n\n\n\nImportant\n\n\n\nBeware! This does not mean that they can distinguish cats and dogs, but more simply that they can spot any difference between the two images. A careful control of the stimuli should be in place if we want to make strong conclusions from looking time.\n\n\n\n\nSaccadic Latency\nAnother very popular eye-tracking measure is saccadic latency. It measures how quickly infants can direct their gaze onto a stimulus, once it is presented on screen. This is a great measure of learning because infants will be faster at looking at a stimulus if they expect it to appear in a given position on the screen. They can even be so fast that they anticipate the correct location of the stimulus, even before the stimulus appears! This is called an anticipatory look.\n\n\n\n\n\n\nImportant\n\n\n\nBeware! Saccadic latencies are not a perfect measure of learning. Infants might be faster at looking at something just because they are more interested (pick interesting stimuli to keep them engaged!), and they might become slower due to boredom or fatigue (introduce variation in the stimuli, so that they become less boring over time!).\n\n\n\n\nPupillometry\nThe fanciest eye-tracking measure is pupil dilation. It allows us to measure arousal (How interested is the infant in the stimulus?), cognitive effort (How difficult is the task?), and - my special favourite - uncertainty (Does the infant know what will happen next?). However, its fame comes at a price: It is not only the fanciest, but also the most persnickety…\n\n\n\n\n\n\nImportant\n\n\n\nBeware! Stimuli should be carefully designed, controlling their luminance (not too high, and ALWAYS the same) to avoid that task-unrelated variations in light will affect your measurements; They have to be presented in the same location on the screen, as pupil size changes depending on screen location; Pupil dilation is very slow, so the stimulus presentation also has to be slow; Often, a fixation cross has to precede the moment in which pupil dilation is measured, so that the pupil size can return to baseline before the event you care about happens. But if you feel confident about your paradigm, go for it!!\n\n\nHere is a visual summary of what you can measure:",
    "crumbs": [
      "Eye-tracking"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html",
    "title": "Pupil data pre-processing",
    "section": "",
    "text": "Welcome to your first step into the world of pupillometry! In this tutorial, we’ll walk you through how to preprocess pupil data using a handy R package called PupillometryR. This package makes it simple to clean and even analyze your pupil data with just a few lines of R code.\nTo keep things straightforward, we’ll be working with a simulated dataset that you can download right here:\nPupilData.zip\nDownload and unzip this folder. This dataset is based on the experimental design we introduced earlier in our eye-tracking series. If you’re not familiar with it or need a quick refresher, we recommend checking out the “Introduction to eye-tracking” guide before diving in.\nThis tutorial serves as a foundation for understanding how to preprocess pupil data. Once you’ve grasped the essentials, we encourage you to explore the full range of functions and features PupillometryR has to offer.",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html#read-the-data",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html#read-the-data",
    "title": "Pupil data pre-processing",
    "section": "Read the data",
    "text": "Read the data\nLet’s begin by importing the necessary libraries and loading the downloaded dataframe.\n\nlibrary(PupillometryR)  # Library to process pupil signal\nlibrary(tidyverse)  # Library to wrangle dataframes\nlibrary(patchwork)\n\nGreat! Now, let’s locate and load all our data files. We’ll use list.files() to identify all the .csv files in our folder. Make sure to update the file path to match the location where your data is stored.\n\ncsv_files = list.files(\n  path       = \"..\\\\..\\\\resources\\\\Pupillometry\\\\RAW\",\n  pattern    = \"\\\\.csv$\",   # regex pattern to match .csv files\n  full.names = TRUE         # returns the full file paths\n)\n\ncsv_files is now a list containing all the .csv files we’ve identified. To better understand our dataset, let’s start by focusing on the first file, representing the first subject, and inspect its structure. This will give us a clear overview before we proceed further.\n\nRaw_data = read.csv(csv_files[1])\nhead(Raw_data) # database peak\n\n  X   Subject      time      L_P      R_P    Event TrialN\n1 1 Subject_1  1.000000 3.187428 3.228510 Fixation      1\n2 2 Subject_1  4.333526 3.153315 3.193957     &lt;NA&gt;     NA\n3 3 Subject_1  7.667052 3.102050 3.142032     &lt;NA&gt;     NA\n4 4 Subject_1 11.000578 3.163670 3.204446     &lt;NA&gt;     NA\n5 5 Subject_1 14.334104 3.152682 3.193316     &lt;NA&gt;     NA\n6 6 Subject_1 17.667630 3.086508 3.126289     &lt;NA&gt;     NA\n\n\nOur dataframe consists of several easily interpretable columns. time represents elapsed time in milliseconds, Subject identifies the participant, and Event indicates when and which stimuli were presented. TrialN tracks the trial number, while L_P and R_P measure pupil dilation for the left and right eyes, respectively, in millimeters.\nLet’s plot the data! Visualizing it first is always a crucial step as it provides an initial understanding of its structure and key patterns.\n\nggplot(Raw_data, aes(x = time, y = R_P)) +\n  geom_line(aes(y = R_P, color = 'Pupil Right'), lwd = 1.2) +\n  geom_line(aes(y = L_P, color = 'Pupil Left'), lwd = 1.2) +\n  geom_vline(data = Raw_data |&gt; dplyr::filter(!is.na(Event)), aes(xintercept = time, linetype = Event), lwd = 1.3) +\n  \n  theme_bw(base_size = 35) +\n  ylim(1, 6) +\n  labs(color= 'Signal', y='Pupil size')+\n  scale_color_manual(\n    values = c('Pupil Right' = '#4A6274', 'Pupil Left' = '#E2725A') )  +\n  theme(\n    legend.position = 'bottom'  ) +\n  guides(\n    color = guide_legend(override.aes = list(lwd = 20)),\n    linetype = guide_legend(override.aes = list(lwd = 1.2))\n  )",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html#prepare-the-data",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html#prepare-the-data",
    "title": "Pupil data pre-processing",
    "section": "Prepare the data",
    "text": "Prepare the data\nNice!! Now we have some sense of our data!! And….you’ve probably noticed two things:\n\nSo many events! That’s intentional — it’s better to have too many triggers than miss something important. When we recorded the data, we saved all possible events to ensure nothing was overlooked. But don’t worry, for our pupil dilation analysis, we only care about two key events: Circle and Square (check the paradigm intro if you need a refresher on why this is)\nSingle-sample events! Like in most studies, events are marked at a single time point (when the stimulus is presented). But PupilometryR needs a different structure — it expects the event value to be repeated for every row while the event is happening.\n\nSo, how do we fix this? First, let’s isolate the rows in our dataframe where the events are Circle or Square. We start by creating a list of the events we care about, then use it to filter our dataframe and keep only the rows related to those events in a new dataframe called Events\n\nEvents_to_keep = c('Circle','Square')\nEvents = filter(Raw_data, Event %in% Events_to_keep) # filter data\nhead(Events) # database peak\n\n      X   Subject       time      L_P      R_P  Event TrialN\n1   221 Subject_1   734.3757       NA       NA Circle      1\n2  2552 Subject_1  8504.8248 3.596057 3.642405 Square      2\n3  4883 Subject_1 16275.2739 3.543367       NA Circle      3\n4  7215 Subject_1 24049.0565 3.164419 3.205205 Circle      4\n5  9546 Subject_1 31819.5055 3.147494 3.188061 Square      5\n6 11877 Subject_1 39589.9546 3.343493 3.386587 Circle      6\n\n\nPerfect! Now onto the second point: we need to repeat the events we just selected for the entire duration we want to analyze. But what’s this duration? We want to cover the full cue presentation (2 seconds), plus an extra 0.1 seconds before the stimulus appears. Why? This pre-stimulus period will serve as our baseline, which we’ll use later in the analysis.\nSo, let’s define how much time to include before and after the stimulus. We’ll also set the framerate of our data (300Hz) and create a time vector that starts from the pre-stimulus period and continues in steps of 1/Hz, with a total length equal to Pre_stim + Post_stim.\n\n# Settings to cut events\nFs = 300 # framerate\nStep = 1000/Fs\n\nPre_stim = 100 # pre stimulus time (100ms)\nPost_stim = 2000 # post stimulus time (2000ms)\nPre_stim_samples = Pre_stim/Step  # pre stimulus in samples\nPost_stim_samples = Post_stim/Step  # post stimulus in samples\n\n# Time vector based on the event duration\nTime = seq(from = -Pre_stim, by=Step, length.out = (Pre_stim+Post_stim)/Step) # time vector\n\nHere’s where the magic happens. We loop through each event listed in our Events dataframe. Each row in Events corresponds to a specific event (like a “Circle” or “Square” cue) that occurred for a specific subject during a specific trial.\nFor each event, we extract 2 key details:\n\nEvent (to know if it’s a Circle or Square cue)\nTrialN (to know which trial this event is part of)\n\nNext, we identify the rows of interest in our main dataframe. First, we locate the row where the time is closest to the onset of the event. Then, we select a range of rows that fall within the Pre_stim and Post_stim window around the event.\nFinally, we use these identified rows to add the event information. The Time, Event, and TrialN values are repeated across all the rows in this window, ensuring every row in the event window is properly labeled.\n\n# Loop for each event \nfor (trial in 1:nrow(Events)){\n\n    # Extract the information\n    Event = Events[trial,]$Event\n    TrialN = Events[trial,]$TrialN\n    \n    # Event onset information\n    Onset = Events[trial,]$time\n    Onset_index = which.min(abs(Raw_data$time - Onset))\n    \n    # Find the rows to update based on pre post samples\n    rows_to_update = seq(Onset_index - Pre_stim_samples,\n                         Onset_index + Post_stim_samples-1)\n    \n    # Repeat the values of interest for all the rows\n    Raw_data[rows_to_update, 'time'] = Time\n    Raw_data[rows_to_update, 'Event'] = Event\n    Raw_data[rows_to_update, 'TrialN'] = TrialN\n}\n\nPerfect! We’ve successfully extended the event information backward and forward based on our Pre_stim and Post_stim windows. Now, it’s time to clean things up.\nSince we only care about the rows that are part of our trial of interest —and because the event information is now repeated for each row during its duration— we’ll remove all the rows that don’t belong to these event windows. This will leave us with a clean, focused dataset that only contains the data relevant to our analysis.\n\nTrial_data = Raw_data %&gt;% \n    filter(Event %in% Events_to_keep)\n\nFor all subjects\nGreat job making it this far! Fixing the data to make it usable in PupillometryR is definitely one of the trickiest parts. But… we’ve only done this for one subject so far—oops! 😅 No worries, though. Let’s automate this process by putting everything into a loop for each subject. In this loop, we’ll fix the event structure for each subject, store each subject’s processed dataframe in a list, and finally combine all these dataframes into one single dataframe for further analysis. Let’s make it happen!\n\n# Libraries and files --------------------------------------------------------------------\n\nlibrary(PupillometryR)  # Library to process pupil signal\nlibrary(tidyverse)  # Library to wrangle dataframes\nlibrary(patchwork)\n\ncsv_files = list.files(\n  path       = \"..\\\\..\\\\resources\\\\Pupillometry\\\\RAW\",\n  pattern    = \"\\\\.csv$\",   # regex pattern to match .csv files\n  full.names = TRUE         # returns the full file paths\n)\n\n\n# Event settings --------------------------------------------------------------------\n\nFs = 300 # framerate\nStep = 1000/Fs\n\nPre_stim = 100 # pre stimulus time (100ms)\nPost_stim = 2000 # post stimulus time (2000ms)\nPre_stim_samples = Pre_stim/Step  # pre stimulus in samples\nPost_stim_samples = Post_stim/Step  # post stimulus in samples\n\n# Time vector based on the event duration\nTime = seq(from = -Pre_stim, by=Step, length.out = (Pre_stim+Post_stim)/Step) # time vector\n\n\n# Event fixes --------------------------------------------------------------------\n\nList_of_subject_dataframes = list() # Empty list to be filled with dataframes\n\n# Loop for each subject\nfor (sub in 1:length(csv_files)) {\n  \n  Raw_data = read.csv(csv_files[sub]) # Raw data\n  Events = filter(Raw_data, Event %in% Events_to_keep) # Events\n  \n  \n  # Loop for each event \n  for (trial in 1:nrow(Events)){\n  \n      # Extract the information\n      Event = Events[trial,]$Event\n      TrialN = Events[trial,]$TrialN\n      \n      # Event onset information\n      Onset = Events[trial,]$time\n      Onset_index = which.min(abs(Raw_data$time - Onset))\n      \n      # Find the rows to update based on pre post samples\n      rows_to_update = seq(Onset_index - Pre_stim_samples,\n                           Onset_index + Post_stim_samples-1)\n      \n      # Repeat the values of interest for all the rows\n      Raw_data[rows_to_update, 'time'] = Time\n      Raw_data[rows_to_update, 'Event'] = Event\n      Raw_data[rows_to_update, 'TrialN'] = TrialN\n  }\n  \n  \n  # Filter only events of interest\n  Trial_data = Raw_data %&gt;% \n    filter(Event %in% Events_to_keep)\n  \n  # Add daframe to list\n  List_of_subject_dataframes[[sub]] = Trial_data\n}\n\n# Combine the list of dataframes into 1 dataframe\nTrial_data = bind_rows(List_of_subject_dataframes)\n\nNow we have our dataset all fixed and organized for each subject. Let’s take a look!\n\nCodeggplot(Trial_data, aes(x = time, y = R_P, group = TrialN)) +\n  geom_line(aes(y = R_P, color = 'Pupil Right'), lwd = 1.2) +\n  geom_line(aes(y = L_P, color = 'Pupil Left'), lwd = 1.2) +\n  geom_vline(aes(xintercept = 0), linetype = 'dashed', color = 'black', lwd = 1.2) +\n  facet_wrap(~Subject) +\n  \n  ylim(1, 6) +\n  scale_color_manual(values = c('Pupil Right' = '#4A6274', 'Pupil Left' = '#E2725A')) +\n  \n  theme_bw(base_size = 35) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nAs you can see, the data structure is now completely transformed. We’ve segmented the data into distinct time windows, with each segment starting at -0.1 seconds (-100 ms) and extending to 2 seconds (2000 ms). This new structure ensures consistency across all segments, making the data ready for further analysis.\nMake PupillometryR data\nOk, now it’s time to start working with PupillometryR! 🎉\nIn the previous steps, we changed our event structure, and you might be wondering — why all that effort? Well, it’s because PupillometryR needs the data in this specific format to do its magic. To get started, we’ll pass our dataframe to the make_pupillometryr_data() function. If you’re already thinking, “Oh no, not another weird object type that’s hard to work with!” — don’t worry! The good news is that the main object it creates is just a regular dataframe. That means we can still interact with like we’re used to. This makes the pre-processing steps much less frustrating. Let’s get started!\n\nPupilR_data = make_pupillometryr_data(data = Trial_data,\n                                 subject = Subject,\n                                 trial = TrialN,\n                                 time = time,\n                                 condition = Event)\n\nHere, we’re simply using the make_pupillometryr_data() function to pass in our data and specify which columns contain the key information. This tells PupillometryR where to find the crucial details, like subject IDs, events, and pupil measurements, so it knows how to structure and process the data properly.\n\n\n\n\n\n\nTip\n\n\n\nIf you have extra columns that you want to keep in your PupillometryR data during preprocessing, you can pass them as a list using the other = c(OtherColumn1, OtherColumn2) argument. This allows you to keep these additional columns alongside your main data throughout most of the preprocessing steps.\nBut here’s a heads-up — not all functions can keep these extra columns every time. For example, downsampling may not retain them since it reduces the number of rows, and it’s not always clear how to summarize extra columns. So, keep that in mind as you plan your analysis!\n\n\nPlot\nOne cool feature of the data created using make_pupillometryr_data() is that it comes with a simple, built-in plot function. This makes it super easy to visualize your data without needing to write tons of code. The plot function works by averaging the data over the group variable. So we can group over subject or condition. Here we use the group variable to focus on the condition and average over the subjects.\nIn this example, we’re plotting the L_P (left pupil) data, grouped by condition. The plot() function is actually just a ggplot2 wrapper, which means you can customize to a certain extent like any other ggplot. That’s why we can add elements to it, like theme_bw(), which gives the plot a cleaner, black-and-white look. Give it a go without adding anything and then learn to customize it!!\n\n\n\n\n\n\nTip\n\n\n\nPro tip! If you want more control over your plots, you can always use ggplot2. Remember, the Pupil data is just a regular dataframe, so you can plot it in any way you like!\n\n\n\nplot(PupilR_data, pupil = L_P, group = 'condition', geom = 'line') +\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn this tutorial, we’ll use two methods to plot our data. We’ll use the PupillometryR plot to visualize the average pupil response by condition, and we’ll also use ggplot to manually plot our data. Both approaches are valid and offer unique benefits.\nThe PupillometryR plot provides a quick overview by automatically averaging pupil responses across condition levels, making it ideal for high-level trend visualization. On the other hand, ggplot gives you full control to visualize specific details or customize every aspect of the plot, allowing for deeper insights and flexibility.",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html#pre-processing",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html#pre-processing",
    "title": "Pupil data pre-processing",
    "section": "Pre-processing",
    "text": "Pre-processing\nNow that we have our pupillometry data in the required format we can actually start the pre-processing!!\nRegress\nThe first step is to regress L_P against R_P (and vice versa) using a simple linear regression. This corrects small inconsistencies in pupil data caused by noise. The regression is done separately for each participant, trial, and time point, ensuring smoother and more consistent pupil dilation measurements.\n\nRegressed_data = regress_data(data = PupilR_data,\n                                pupil1 = L_P,\n                                pupil2 = R_P)\n\nError in `mutate()`:\nℹ In argument: `pupil1newkk = .predict_right(L_P, R_P)`.\nℹ In group 1: `Subject = \"Subject_1\"`, `TrialN = 1`, `Event = \"Circle\"`.\nCaused by error in `lm.fit()`:\n! 0 (non-NA) cases\n\n\nPwa pwa pwaaaaa…!!🤦‍♂️ We got an error!\nWhat’s it saying? It’s telling us that one of the trials is completely full of NAs, and since there’s no data to work with, the function fails. This happens a lot when testing infants — they don’t always do what we expect, like watching the screen. Instead, they move around or look away.\nWe’ll deal with missing data properly later, but for now, we need a quick fix. What can we do? We can simply drop any trials where both pupils (L_P and R_P) are entirely NA. This way, we avoid errors and keep the analysis moving.\nSo let’s filter our data and then redo the last two steps (make PupilR_data and then regress data)\n\n# Filter the trial data\nTrial_data = Trial_data %&gt;%\n    group_by(Subject, TrialN) %&gt;%  # group by Subject and TrialN\n    filter(!all(is.na(L_P) & is.na(R_P))) %&gt;% # filter out if both R_P and L_P are all NA\n    ungroup()  # Remove grouping\n\n# Make pupilloemtryR data\nPupilR_data = make_pupillometryr_data(data = Trial_data,\n                                 subject = Subject,\n                                 trial = TrialN,\n                                 time = time,\n                                 condition = Event)\n# Regress data\nRegressed_data = regress_data(data = PupilR_data,\n                               pupil1 = L_P,\n                                pupil2 = R_P)\n\nAnd now everything worked!! Perfect!\nMean pupil\nAs the next steps we will average the two pupil signals. This will create a new variable called mean_pupil\n\nMean_data = calculate_mean_pupil_size(data = Regressed_data, \n                                       pupil1 = L_P, \n                                       pupil2 = R_P)\n\nplot(Mean_data, pupil = mean_pupil, group = 'condition', geom = 'line')+\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nLowpass\nNow that we have a single pupil signal, we can move on to filtering it. The goal here is to remove fast noise and fluctuations that aren’t relevant to our analysis. Why? Because we know that pupil dilation is a slow physiological signal, and those rapid changes are likely just noise from blinks, eye movements, or measurement errors. By filtering out these fast fluctuations, we can focus on the meaningful changes in pupil dilation that matter for our analysis.\n\nfiltered_data = filter_data(data = Mean_data,\n                             pupil = mean_pupil,\n                             filter = 'median',\n                             degree = 11)\nplot(filtered_data, pupil = mean_pupil, group = 'condition', geom = 'line')+\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nThere are different ways to filter the data in PupillometryR we suggest you check the actual package website and make decision based on your data (filter_data). Here we use a median filter based on a 11 sample window.\nDownsample\nAs mentioned above, Pupil dilation is a slow signal, so 20Hz is enough — no need for 300Hz. Downsampling reduces file size, speeds up processing, and naturally smooths the signal by filtering out high-frequency noise, all while preserving the key information we need for analysis. To downsample to 20Hz, we’ll set the timebin size to 50 ms (since 1/20 = 0.05 seconds = 50 ms) and calculate the median for each time bin.\n\nNewHz = 20\ntimebinSize = 1/NewHz\n\nDownsampled_data = downsample_time_data(data = filtered_data,\n                              pupil = mean_pupil,\n                              timebin_size = timebinSize,\n                              option = 'median')\nplot(Downsampled_data, pupil = mean_pupil, group = 'condition', geom = 'line') +\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nTrial Rejection\nNow that our data is smaller and smoother, it’s a good time to take a look at it. It doesn’t make sense to keep trials that are mostly missing values, nor does it make sense to keep participants with very few good trials.\nWhile you might already have info on trial counts and participant performance from other sources (like video coding), PupillometryR has a super handy function to check this directly. This way, you can quickly see how many valid trials each participant has and decide which ones to keep or drop.\n\nMissing_data = calculate_missing_data(Downsampled_data,\n                                       mean_pupil)\nhead(Missing_data, n=20)\n\n# A tibble: 20 × 3\n   Subject   TrialN Missing\n   &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt;\n 1 Subject_1      2  0.0603\n 2 Subject_1      3  0.125 \n 3 Subject_1      4  0.0746\n 4 Subject_1      5  0.106 \n 5 Subject_1      6  0.0746\n 6 Subject_1      7  0.0635\n 7 Subject_1      8  0.0587\n 8 Subject_1      9  0.0619\n 9 Subject_1     10  0.130 \n10 Subject_2      1  0.143 \n11 Subject_2      2  0.121 \n12 Subject_2      3  0.0619\n13 Subject_2      4  0     \n14 Subject_2      5  0.132 \n15 Subject_2      6  0.137 \n16 Subject_2      7  0.0841\n17 Subject_2      8  0.0968\n18 Subject_2      9  0.0841\n19 Subject_2     10  0.0508\n20 Subject_3      1  0.0587\n\n\nThis gives us a new dataframe that shows the amount of missing data for each subject and each trial. While we could manually decide which trials and subjects to keep or remove, PupillometryR makes it easier with the clean_missing_data() function.\nThis function lets you set two % thresholds — one for trials and one for subjects. Here, we’ll set it to reject trials with more than 25% missing data (keep at least 75% of the data) and reject subjects with more than 25% missing data. This way, we ensure our analysis is based on clean, high-quality data.\n\nClean_data = clean_missing_data(Downsampled_data,\n                                 pupil = mean_pupil,\n                                 trial_threshold = .75,\n                                 subject_trial_threshold = .75)\n\nRemoving trials with a proportion missing &gt; 0.75 \n ...removed 3 trials \n\n\nRemoving subjects with a proportion of missing trials &gt; 0.75 \n ...removed 0 subjects \n\n\nSee?! PupillometryR shows us exactly how many trials and subjects are being excluded from our dataframe based on our thresholds. Cool!\n\n\n\n\n\n\nWarning\n\n\n\nNote that this function calculates the percentage of missing trials based only on the trials present in the dataframe. For example, if a participant only completed one trial (and watched it perfectly) before the session had to stop, the percentage would be calculated on that single trial, and the participant wouldn’t be rejected.\nIf you have more complex conditions for excluding participants (e.g., based on total expected trials or additional criteria), you’ll need to handle this manually to ensure subjects are dropped appropriately.\n\n\nFill the signal\nNow our data is clean, but… while the average signal for each condition looks smooth (as seen in our plots), the data for each individual participant is still noisy. We can still spot blinks and missing data in the signal.\nTo handle this, we’ll use interpolation to fill in the missing points. Interpolation “connects the dots” between gaps, creating a more continuous and cleaner signal. This step is crucial because large chunks of missing data can distort our analysis, and interpolation allows us to retain more usable data from each participant.\n\nggplot(Clean_data, aes(x = time, y = mean_pupil, group = TrialN, color= Event))+\n  geom_line( lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  labs(y = 'Pupil Size')+\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nSo to remove these missing values we can interpolate our data. Interpolating is easy with PupillometryR we can simply:\n\nInt_data = interpolate_data(data = Clean_data,\n                             pupil = mean_pupil,\n                             type = 'linear')\n\nggplot(Int_data, aes(x = time, y = mean_pupil, group = TrialN, color = Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  labs(y = 'Pupil Size')+\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nDone!! Well, you’ve probably noticed something strange… When there’s a blink, the pupil signal can rapidly decrease until it’s completely missing. Right now, this drop gets interpolated, and the result is a weird, unrealistic curve where the signal dips sharply and then smoothly recovers. This makes our data look horrible! 😩\nLet’s fix it!\nTo do this, we’ll use PupillometryR’s blink detection functions. There are two main ways to detect blinks:\n\nBased on size — detects pupil size.\nBased on velocity — detects rapid changes in pupil size (which happens during blinks).\n\nHere, we’ll use detection by velocity. We set a velocity threshold to detect when the pupil size changes too quickly. To ensure we capture the full blink, we use extend_forward and extend_back to expand the blink window, including the fast decrease in pupil size. The key idea is to make the entire blink period NA, not just the moment the pupil disappears. This prevents interpolation from creating unrealistic artifacts. When we interpolate, the process skips over the entire blink period, resulting in a cleaner, more natural signal.\n\nBlink_data = detect_blinks_by_velocity(\n    Clean_data,\n    mean_pupil,\n    threshold = 0.1,\n    extend_forward = 70,\n    extend_back = 70)\n\nggplot(Blink_data, aes(x = time, y = mean_pupil, group = TrialN, color=Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  labs(y = 'Pupil Size')+\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nSee !! now the rapid shrinking disappeared and we can now interpolate\n\nInt_data = interpolate_data(data = Blink_data,\n                             pupil = mean_pupil,\n                             type = 'linear')\n\nggplot(Int_data, aes(x = time, y = mean_pupil, group = TrialN, color=Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  labs(y = 'Pupil Size')+\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n\n\n\n\n\nLook how beautiful our signal is now!! 😍 Good job!!!\n\n\n\n\n\n\nCaution\n\n\n\nYou won’t always run into blink issues like this. Downsampling and filtering usually handle rapid changes during earlier preprocessing steps. Whether this happens can depend on the tracker, sampling rate, or even the population you’re testing. In this simulated data, we exaggerated the blink effects on purpose to show you how to spot and fix them! Thus, blink detection may not always be necessary. The best approach is to check your data before deciding. And how do you check it? Plotting! Plotting your signal is the best way to see if blinks are causing rapid drops or if you’re just dealing with missing data. Let the data guide your decisions.\n\n\nBaseline Correction\nGood job getting this far!! We’re now at the final step of our pre-processing: baseline correction.\nBaseline correction helps remove variability between trials and participants, like differences in baseline pupil size caused by individual differences, fatigue, or random fluctuations. By doing this, we can focus only on the variability caused by our paradigm. This step ensures that any changes we see in pupil size are truly driven by the experimental conditions, not irrelevant noise. To further adjust the data, we’ll subtract the calculated baseline, ensuring the values start at 0 instead of -100. Finally, to make the next steps of analysis easier, we’ll select only the columns of interest, dropping any irrelevant ones.\nLet’s get it done!\n\nBase_data = baseline_data(data = Int_data,\n                           pupil = mean_pupil,\n                           start = -100,\n                           stop = 0)\n\n# Remove the baseline\nFinal_data = subset_data(Base_data, start = 0) %&gt;% \n  select(Subject, Event, TrialN, mean_pupil, time)\n\nLet’s plot it to see what baseline correction and removal are actually doing!! We will plot both the average signal using the plot function (with some addition information about color and theme) and using ggplot to plot the data for each subject separately.\n\nOne = plot(Final_data, pupil = mean_pupil, group = 'condition')+\n  theme_bw(base_size = 45) +\n  theme(legend.position = 'none')\n\n\nTwo = ggplot(Final_data, aes(x = time, y = mean_pupil, group = TrialN, color = Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  labs(y = 'Pupil Size')+\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n# Using patchwork to put the plot together\nOne / Two\n\n\n\n\n\n\n\nSave and analysis\nThis tutorial will not cover the analysis of pupil dilation. We’ll stop here since, after baseline correction, the data is ready to be explored and analyzed. From this point on, we’ll shift from pre-processing to analysis, so it’s a good idea to save the data as a simple .csv file for easy access and future use.\n\nwrite.csv(Final_data, \"..\\\\..\\\\resources\\\\Pupillometry\\\\Processed\\\\Processed_PupilData.csv\")\n\nThere are multiple ways to analyze pupil data, and we’ll show you some of our favorite methods in a dedicated tutorial: Analyze Pupil Dilation.",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html#cite-pupillometryr",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html#cite-pupillometryr",
    "title": "Pupil data pre-processing",
    "section": "Cite PupillometryR",
    "text": "Cite PupillometryR\nIf you decide to use PupillometryR in your analysis, don’t forget to cite it! Proper citation acknowledges the authors’ work and supports the development of such valuable tools.\n\nCodecitation(\"PupillometryR\")\n\nTo cite PupillometryR in publications use:\n\n  Forbes, S. H. (2020). PupillometryR: An R package for preparing and\n  analysing pupillometry data. Journal of Open Source Software, 5(50),\n  2285. https://doi.org/10.21105/joss.02285\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {PupillometryR: An R package for preparing and analysing pupillometry data},\n    author = {Samuel H. Forbes},\n    journal = {Journal of Open Source Software},\n    year = {2020},\n    volume = {5},\n    number = {50},\n    pages = {2285},\n    doi = {10.21105/joss.02285},\n  }",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/EyeTracking/PupilPreprocessing.html#all-code",
    "href": "CONTENT/EyeTracking/PupilPreprocessing.html#all-code",
    "title": "Pupil data pre-processing",
    "section": "All code",
    "text": "All code\nHere below we report the whole code we went trough this tutorial as an unique script to make it easier for you to copy and explore it in it’s entirety.\n\n# Libraries and files --------------------------------------------------------------------\n\nlibrary(PupillometryR)  # Library to process pupil signal\nlibrary(tidyverse)  # Library to wrangle dataframes\nlibrary(patchwork)\n\ncsv_files = list.files(\n  path       = \"..\\\\..\\\\resources\\\\Pupillometry\\\\RAW\",\n  pattern    = \"\\\\.csv$\",   # regex pattern to match .csv files\n  full.names = TRUE         # returns the full file paths\n)\n\n\n# Prepare data --------------------------------------------------------------------\n\n## Event settings --------------------------------------------------------------------\n\n# Settings to cut events\nFs = 300 # framerate\nStep = 1000/Fs\n\nPre_stim = 100 # pre stimulus time (100ms)\nPost_stim = 2000 # post stimulus time (2000ms)\nPre_stim_samples = Pre_stim/Step  # pre stimulus in samples\nPost_stim_samples = Post_stim/Step  # post stimulus in samples\n\n# Time vector based on the event duration\nTime = seq(from = -Pre_stim, by=Step, length.out = (Pre_stim+Post_stim)/Step) # time vector\n\n\n## Event fixes --------------------------------------------------------------------\n\nList_of_subject_dataframes = list() # Empty list to be filled with dataframes\n\n# Loop for each subject\nfor (sub in 1:length(csv_files)) {\n  \n  Raw_data = read.csv(csv_files[sub]) # Raw data\n  Events = filter(Raw_data, Event %in% Events_to_keep) # Events\n\n  \n  # Loop for each event \n  for (trial in 1:nrow(Events)){\n  \n      # Extract the information\n      Event = Events[trial,]$Event\n      TrialN = Events[trial,]$TrialN\n      \n      # Event onset information\n      Onset = Events[trial,]$time\n      Onset_index = which.min(abs(Raw_data$time - Onset))\n      \n      # Find the rows to update based on pre post samples\n      rows_to_update = seq(Onset_index - Pre_stim_samples,\n                           Onset_index + Post_stim_samples-1)\n      \n      # Repeat the values of interest for all the rows\n      Raw_data[rows_to_update, 'time'] = Time\n      Raw_data[rows_to_update, 'Event'] = Event\n      Raw_data[rows_to_update, 'TrialN'] = TrialN\n  }\n  \n  \n  # Filter only events of interest\n  Trial_data = Raw_data %&gt;% \n    filter(Event %in% Events_to_keep)\n  \n  # Add daframe to list\n  List_of_subject_dataframes[[sub]] = Trial_data\n}\n\n# Combine the list of dataframes into 1 dataframe\nTrial_data = bind_rows(List_of_subject_dataframes)\n\n\n### Plot Raw Data -----------------------------------------------------------------\n\nggplot(Raw_data, aes(x = time, y = R_P)) +\n  geom_line(aes(y = R_P, color = 'Pupil Right'), lwd = 1.2) +\n  geom_line(aes(y = L_P, color = 'Pupil Left'), lwd = 1.2) +\n  geom_vline(data = Raw_data |&gt; dplyr::filter(!is.na(Event)), aes(xintercept = time, linetype = Event), lwd = 1.3) +\n  \n  theme_bw(base_size = 45) +\n  ylim(1, 6) +\n  labs(color= 'Signal', y='Pupil size')+\n  scale_color_manual(\n    values = c('Pupil Right' = '#4A6274', 'Pupil Left' = '#E2725A') )  +\n  theme(\n    legend.position = 'bottom'  ) +\n  guides(\n    color = guide_legend(override.aes = list(lwd = 20)),\n    linetype = guide_legend(override.aes = list(lwd = 1.2))\n  )\n\n\n\n# Pre-processing -----------------------------------------------------------------\n\n## Filter Out Trials with all NA -----------------------------------------------------------------\n\nTrial_data = Trial_data %&gt;%\n  group_by(Subject, TrialN) %&gt;%\n  filter(!all(is.na(L_P) & is.na(R_P))) %&gt;%\n  ungroup()\n\n\n## Make PupillometryR Data -----------------------------------------------------------------\nPupilR_data = make_pupillometryr_data(data = Trial_data,\n                                      subject = Subject,\n                                      trial = TrialN,\n                                      time = time,\n                                      condition = Event)\n\n### Plot ------------------------------------------------------------------\nplot(PupilR_data, pupil = L_P, group = 'condition', geom = 'line') +\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Regress Data -----------------------------------------------------------------\n\nRegressed_data = regress_data(data = PupilR_data,\n                               pupil1 = L_P,\n                               pupil2 = R_P)\n\n\n## Calculate Mean Pupil -----------------------------------------------------------------\n\nMean_data = calculate_mean_pupil_size(data = Regressed_data, \n                                       pupil1 = L_P, \n                                       pupil2 = R_P)\n\n### Plot --------------------------------------------------------------------\n\nplot(Mean_data, pupil = mean_pupil, group = 'condition', geom = 'line')+\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Lowpass Filter -----------------------------------------------------------------\n\nfiltered_data = filter_data(data = Mean_data,\n                             pupil = mean_pupil,\n                             filter = 'median',\n                             degree = 11)\n\n### Plot --------------------------------------------------------------------\n\nplot(filtered_data, pupil = mean_pupil, group = 'condition', geom = 'line')+\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Downsample -----------------------------------------------------------------\n\nNewHz = 20\n\ntimebinSize = 1 / NewHz\n\nDownsampled_data = downsample_time_data(data = filtered_data,\n                                         pupil = mean_pupil,\n                                         timebin_size = timebinSize,\n                                         option = 'median')\n\n# Plot --------------------------------------------------------------------\n\nplot(Downsampled_data, pupil = mean_pupil, group = 'condition', geom = 'line') +\n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Calculate Missing Data -----------------------------------------------------------------\n\nMissing_data = calculate_missing_data(Downsampled_data, mean_pupil)\n\n\n## Clean Missing Data -----------------------------------------------------------------\n\nClean_data = clean_missing_data(Downsampled_data,\n                                 pupil = mean_pupil,\n                                 trial_threshold = .75,\n                                 subject_trial_threshold = .75)\n\n### Plot --------------------------------------------------------------------\n\nggplot(Clean_data, aes(x = Time, y = mean_pupil, group = TrialN, color= Event))+\n  geom_line( lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n  \n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Detect Blinks -----------------------------------------------------------------\n\nBlink_data = detect_blinks_by_velocity(\n  Clean_data,\n  mean_pupil,\n  threshold = 0.1,\n  extend_forward = 50,\n  extend_back = 50)\n\n### Plot --------------------------------------------------------------------\n\nggplot(Blink_data, aes(x = time, y = mean_pupil, group = TrialN, color=Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n  \n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\n\n## Interpolate Data -----------------------------------------------------------------\n\nInt_data = interpolate_data(data = Clean_data,\n                             pupil = mean_pupil,\n                             type = 'linear')\n\n### Plot --------------------------------------------------------------------\n\nggplot(Int_data, aes(x = Time, y = mean_pupil, group = TrialN, color = Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n\n  facet_wrap(~Subject)+\n  ylim(1,6)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20)))\n\n\n# Baseline correction -----------------------------------------------------\n\nBase_data = baseline_data(data = Int_data,\n                           pupil = mean_pupil,\n                           start = -100,\n                           stop = 0)\n\n# Remove the baseline\nFinal_data = subset_data(Base_data, start = 0)\n\n\n### Final plot --------------------------------------------------------------\n\nOne = plot(Final_data, pupil = mean_pupil, group = 'condition')+\n  theme_bw(base_size = 45) +\n  theme(legend.position = 'none')\n\n\nTwo = ggplot(Final_data, aes(x = time, y = mean_pupil, group = TrialN, color = Event))+\n  geom_line(lwd =1.2)+\n  geom_vline(aes(xintercept = 0), linetype= 'dashed', color = 'black', lwd =1.2)+\n  \n  facet_wrap(~Subject)+\n  \n  theme_bw(base_size = 45) +\n  theme(\n    legend.position = 'bottom', \n    legend.title = element_blank()) +\n  guides(color = guide_legend(override.aes = list(lwd = 20))) \n\nOne / Two\n\n\n\n# Save data ---------------------------------------------------------------\n\nwrite.csv(Final_data, \"..\\\\..\\\\resources\\\\Pupillometry\\\\Processed\\\\Peocessed_PupilData.csv\")",
    "crumbs": [
      "Eye-tracking",
      "Pupil data pre-processing"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html",
    "title": "Starting with PsychoPy",
    "section": "",
    "text": "PsychoPy is an open-source software package written in the Python programming language primarily for use in neuroscience and experimental psychology research. It’s one of our favorite ways to create experiments and we will use it through our tutorials.\nSo, let’s start and install PsychoPy!!!",
    "crumbs": [
      "Getting started:",
      "Starting with PsychoPy"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#minicondaminiforge",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#minicondaminiforge",
    "title": "Starting with PsychoPy",
    "section": "Miniconda/Miniforge",
    "text": "Miniconda/Miniforge\nWe recommend installing PsychoPy in a dedicated virtual environment where you can create and run your studies. To ensure optimal performance and avoid compatibility issues, keep this environment as clean as possible. Ideally, install PsychoPy and only the additional libraries required for your study. For any other libraries, for example for analysis, consider creating separate environments to prevent conflicts with PsychoPy\n\nWindowsMac\n\n\nOn windows python 3.8 seems to be a safe bet. Create a new environment:\nconda create -n psychopy python=3.8\n\n\n\n\n\n\nNote\n\n\n\nRemember if you have installed minforge instead of miniconda you can use mamba and conda interchangeably for these steps.\n\n\nonce done we actually need to install psychopy in this new environment. Let’s do that, activate the environment and just type pip install psychopy :\nconda activate psychpy\npip install psychopy\n\n\nOn Mac Python 3.10 seems to work better. Thus create a new environment:\nconda create -n psychopy python=3.10\n\n\n\n\n\n\nNote\n\n\n\nRemember if you have installed minforge instead of miniconda you can use mamba and conda interchangeably for these steps.\n\n\nonce done we actually need to install psychopy in this new environment. Let’s do that, activate the environment and just type pip install psychopy :\nconda activate psychpy\npip install psychopy\n\n\n\nIt may take some time and require your confirmation during the process, but eventually, the setup will complete. Once finished, your psychopy environment will include the PsychoPy library, allowing you to import it easily using import psychopy\n\n\n\n\n\n\nCaution\n\n\n\nIf you are planning to use Spyder as your Ide to interact with PsychoPy you require an additional library to interact with Spyder\nYou can install it by:\nconda install spyder-kernels\nor\npip install spyder-kernels\nIn case you need more information please check our python installation guide",
    "crumbs": [
      "Getting started:",
      "Starting with PsychoPy"
    ]
  },
  {
    "objectID": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#psychopy-standalone",
    "href": "CONTENT/GettingStarted/GettingStartedWithPsychopy.html#psychopy-standalone",
    "title": "Starting with PsychoPy",
    "section": "Psychopy Standalone",
    "text": "Psychopy Standalone\nIf you’re unable to install PsychoPy using conda/mamba or simply prefer a reliable setup with access to the intuitive PsychoPy GUI, the standalone installer is a great option.\nHowever, it’s important to note that PsychoPy offers two main ways to create experiments:\n\nThe Builder: Ideal for those who prefer a graphical, point-and-click interface. \nThe Coder: Designed for users who prefer to program their experiments from scratch. \n\nIn our tutorials, we will focus on coding experiments directly. If you’re using the PsychoPy standalone installer, you’ll need to follow along using the Coder interface.",
    "crumbs": [
      "Getting started:",
      "Starting with PsychoPy"
    ]
  },
  {
    "objectID": "CONTENT/Stats/InterpretingModelResults.html",
    "href": "CONTENT/Stats/InterpretingModelResults.html",
    "title": "Interpreting Model Results",
    "section": "",
    "text": "In the previous tutorial we run our first model and we checked whether the model met the assumptions. You have to agree it was easy and fun! Now the real challenge begins.\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(easystats)\n\nWarning: package 'easystats' was built under R version 4.3.3\n\n\n# Attaching packages: easystats 0.7.3 (red = needs update)\n✖ bayestestR  0.14.0   ✖ correlation 0.8.5 \n✔ datawizard  0.13.0   ✔ effectsize  0.8.9 \n✔ insight     0.20.5   ✖ modelbased  0.8.8 \n✖ performance 0.12.3   ✖ parameters  0.22.2\n✔ report      0.5.9    ✔ see         0.9.0 \n\nRestart the R-Session and update packages with `easystats::easystats_update()`.\n\ndf = read.csv(\"..\\\\..\\\\resources\\\\Stats\\\\LM_SimulatedData.csv\")\nmod = lm(performance ~ time*tool, data = df)"
  },
  {
    "objectID": "CONTENT/Stats/InterpretingModelResults.html#coeffiecients",
    "href": "CONTENT/Stats/InterpretingModelResults.html#coeffiecients",
    "title": "Interpreting Model Results",
    "section": "Coeffiecients",
    "text": "Coeffiecients\nThe Coefficient section if indubitably the most important section of the summary of our model. However what are all these numbers? Let’s go together through the most challenging information:\n(Intercept)\nThe intercept often confuses who approaches for the first time linear models. What is it? The (Intercept) represent the reference levels where all our predictors (time and tool) are 0. Now you may ask…how can tool be 0? It is a categorical variable, it can’t be 0!!!!\nYou are correct. When a model encounters a categorical variable it selects the first level of such variable as reference level. If you take another look to our model summary you can see that there are information both for the hammerand spoon level of the toolvariable but nothing about the brush level. This is because the brush level has been selected by the model as the reference level and it is thus represented in the intercept value!\nwe can simply visualize it as:\n\nlibrary(ggplot2)\n\nmodel_p = parameters(mod)\n\n# Define intercept and slope for the brush\nintercept_brush &lt;- model_p[1,2]\nintercept_brush_se &lt;- model_p[1,3]\n\n# To create estimates\ntime_values &lt;- seq(0, 10, by = 0.5)\n\n\nggplot()+\n  \n  # Cartesian lines\n  geom_vline(xintercept = 0)+\n  geom_hline(yintercept = 0)+\n  \n  # Intercept\n  annotate(\"point\", x = 0, y = intercept_brush, size = pi * intercept_brush_se^2, alpha = 0.3, color = 'darkred') +\n  annotate(\"point\", x = 0, y = intercept_brush, color = 'darkred') +\n  annotate(\"text\", x = -0.5, y=intercept_brush*1.15, label='(Intercept)')+\n  \n  # Plot addition information\n  coord_cartesian(xlim = c(-1,5), ylim = c(-1,5))+\n  labs(y = 'Performance', x = 'Time')+\n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "CONTENT/Stats/InterpretingModelResults.html#tools",
    "href": "CONTENT/Stats/InterpretingModelResults.html#tools",
    "title": "Interpreting Model Results",
    "section": "Tools",
    "text": "Tools\n\n# Define intercept and slope for the brush\nintercept_brush &lt;- model_p[1,2]\nintercept_brush_se &lt;- model_p[1,3]\n\nintercept_hammer &lt;- model_p[3,2] + intercept_brush\nintercept_hammer_se &lt;- model_p[3,3]\n\nintercept_spoon &lt;- model_p[4,2]+ intercept_brush\nintercept_spoon_se &lt;- model_p[4,3]\n\n\nggplot() +\n  \n  # Intercept (brush)\n  annotate(\"point\", x = 0, y = intercept_brush, size = pi * intercept_brush_se^2, alpha = 0.3, color = 'darkred') +\n  annotate(\"point\", x = 0, y = intercept_brush, color = 'darkred', size = 4) +\n  \n  # Hammer\n  annotate(\"point\", x = 1, y = intercept_hammer, size = pi * intercept_hammer_se^2, alpha = 0.3, color = 'darkblue') +\n  annotate(\"point\", x = 1, y = intercept_hammer, color = 'darkblue', size = 4) +\n  \n  # Spoon\n  annotate(\"point\", x = 2, y = intercept_spoon, size = pi * intercept_spoon_se^2, alpha = 0.3, color = 'darkgreen') +\n  annotate(\"point\", x = 2, y = intercept_spoon, color = 'darkgreen', size = 4) +\n  \n  # Set the limits for x and y axes\n  coord_cartesian(xlim = c(-0.1, 2.2), ylim = c(0, 7)) +\n  \n  # Customize x-axis breaks\n  scale_x_continuous(breaks = c(0, 1, 2), labels = c('brush','hammer','spoon')) +\n  \n  # Labels and theme\n  labs(y = 'Performance', x = 'Tools') +\n  theme_bw(base_size = 20)\n\n\n\n\n\n\n\nTime\nThe estimates are the slopes (the inclination) of the lines of each predictor. However they should be interpreted as a difference from the intercept.\n\n# Define intercept and slope for the brush\nintercept_brush &lt;- model_p[1,2]\nintercept_brush_se &lt;- model_p[1,3]\n\nslope_brush &lt;- model_p[2,2]\nslope_brush_se &lt;- model_p[2,3]\n\n# To create estimates\ntime_values &lt;- seq(-1, 4, by = 0.25)\n\n# Create a data frame\ndf_brush &lt;- data.frame(time = time_values,\n                       est_performance = intercept_brush + slope_brush * time_values)\n\n\n\n# Function to create an arc between two angles with a custom center\ngenerate_circle_piece &lt;- function(center = c(0, 0), radius = 1, start_angle = 0, end_angle = 90) {\n  # Convert angles to radians\n  start_rad &lt;- start_angle * pi / 180\n  end_rad &lt;- end_angle * pi / 180\n  \n  # Generate points for the piece of the circle\n  theta &lt;- seq(start_rad, end_rad, length.out = 100)\n  x &lt;- center[1] + radius * cos(theta)\n  y &lt;- center[2] + radius * sin(theta)\n  \n  # Create a data frame for the circle points\n  circle_data &lt;- data.frame(x = x, y = y)\n  \n  return(circle_data)\n}\n# Create data for the arc with the angle calculated from the slope, using the new center\narc_data &lt;- generate_circle_piece(center = c(0, intercept_brush), radius = 0.5, start_angle = 0, end_angle =  atan(slope_brush) * (180 / pi))\n\n\nggplot(df_brush, aes(x = time, y = est_performance))+\n  \n  # Cartesian lines\n  geom_vline(xintercept = 0)+\n  # geom_hline(yintercept = 0)+\n  # \n  # Time\n  geom_path(data = arc_data, aes(x = x, y = y), color = 'green', size = 1.5) +\n  geom_hline(yintercept = intercept_brush, linetype = 'dashed', color = 'darkgray')+\n  geom_line(color = \"darkred\", size = 1) +\n\n  # Intercept\n  annotate(\"point\", x = 0, y = intercept_brush, size = pi * intercept_brush_se^2, alpha = 0.3, color = 'darkred') +\n  annotate(\"point\", x = 0, y = intercept_brush, color = 'darkred', size = 4) +\n\n  # Text\n  annotate(\"text\", x = -0.5, y=2.825, label='(Intercept)')+\n  annotate(\"text\", x = 0.8, y=2.82, label='time')+\n\n  \n  \n  # Plot addition information\n  # coord_cartesian(xlim = c(-1,4), ylim = c(-1,5))+\n  labs(y = 'Performance', x = 'Time')+\n  theme_bw(base_size = 20)"
  },
  {
    "objectID": "CONTENT/Stats/LMM.html",
    "href": "CONTENT/Stats/LMM.html",
    "title": "Linear mixed effect modesl",
    "section": "",
    "text": "Welcome to this introduction to Linear Mixed-effects Models (LMM)!! In this tutorial we will use R to run some simple LMM and we will try to understand together how to leverage these model for our analysis.\n        LMMs are amazing tools that have saved our asses countless times during our PhDs and Postdocs. They'll probably continue to be our trusty companions forever.\nBut why are these LMMs so amazing?\nLinear Mixed-Effects Models are basically linear models… but they allow us to account for both fixed and random effects, making them perfect for dealing with data that has multiple sources of variability, like repeated measures or hierarchical structures. Nice isn’t it? Amazing!!\nBut……what are fixed and random effects 😅?\nWell, while fixed effects represent the consistent, predictable part of your model—the data you are interested in and actively want to explore—random effects represent the variability of different sources in your data that you haven’t explicitly measured and usually aren’t interested in, but which you believe affect your data. These random effects help you model these variations, making your predictions more reliable and robust.\nIt probably still sounds complex, but everything will be clearer once we get our hands dirty with them! Let’s start!!!!"
  },
  {
    "objectID": "CONTENT/Stats/LMM.html#the-data",
    "href": "CONTENT/Stats/LMM.html#the-data",
    "title": "Linear mixed effect modesl",
    "section": "The data",
    "text": "The data\nNow let’s import the data. You can download it from here. This dataset is simulated, so some of its variables might not make much sense, but it’ll do the job for this tutorial.\n\nprint(getwd())\n\n[1] \"C:/Users/tomma/OneDrive - Birkbeck, University of London/TomassoGhilardi/PersonalProj/Website/DevStart/CONTENT/Stats\"\n\nload(\".\\\\resources\\\\dragons.RData\")"
  },
  {
    "objectID": "CONTENT/Workshops/GAP_2024.html",
    "href": "CONTENT/Workshops/GAP_2024.html",
    "title": "Bridging the Technological Gap Workshop (GAP)",
    "section": "",
    "text": "Hello hello!!! This page has been created to provide support and resources for the tutorial that will take place during the Bridging the Technological Gap Workshop."
  },
  {
    "objectID": "CONTENT/Workshops/GAP_2024.html#python",
    "href": "CONTENT/Workshops/GAP_2024.html#python",
    "title": "Bridging the Technological Gap Workshop (GAP)",
    "section": "Python",
    "text": "Python\nIn this tutorial, our primary tool will be Python!! There are lots of ways to install python. We recommend installing it via Miniconda. However, for this workshop, the suggested way to install Python is using Anaconda.\nYou might ask….Then which installation should I follow? Well, it doesn’t really matter! Miniconda is a minimal installation of Anaconda. It lacks the GUI, but has all the main features. So follow whichever one you like more!\nOnce you have it installed, we need a few more things. For the Gaze Tracking & Pupillometry Workshop (the part we will be hosting) we will need some specific libraries and files. We have tried our best to make everything as simple as possible:\n\nLibraries\nWe will be working with a conda environment (a self-contained directory that contains a specific collection of Python packages and dependencies, allowing you to manage different project requirements separately). To create this environment and install all the necessary libraries, all you need is this file:\n Psychopy.yml \nOnce you have downloaded the file, simply open the anaconda/miniconda terminal and type conda env create -f, then simply drag and drop the downloaded file onto the terminal. This will copy the filename with its absolute path. In my case it looked something like this:\n\n\n\n\n\nNow you will be asked to confirm a few things (by pressing Y) and after a while of downloading and installing you will have your new workshop environment called Psychopy!\nNow you should see a shortcut in your start menu called Spyder(psychopy), just click on it to open spyder in our newly created environment. If you don’t see it, just reopen the anaconda/miniconda terminal, activate your new environment by typing conda activate psychopy and then just type spyder.\n\n\nFiles\nWe also need some files if you want to run the examples with us. Here you can download the zip files with everything you need:\n Files \nOnce downloaded, simply extract the file by unzipping it. For our workshop we will work together in a folder that should look like this:\n\n\n\n\n\nIf you have a similar folder… you are ready to go!!!!"
  },
  {
    "objectID": "CONTENT/Workshops/GAP_2024.html#videos",
    "href": "CONTENT/Workshops/GAP_2024.html#videos",
    "title": "Bridging the Technological Gap Workshop (GAP)",
    "section": "Videos",
    "text": "Videos\nWe received several questions about working with videos and PsychoPy while doing eye-tracking. It can be quite tricky, but here are some tips:\n\nMake sure you’re using the right codec.\nIf you need to change the codec of the video, you can re-encode it using a tool like\nHandbrake (remember to set the constant framerate in the video option)\n\nBelow, you’ll find a code example that adapts our Create an eye-tracking experiment tutorial to work with a video file. The main differences are:\n\nWe’re showing a video after the fixation.\nWe’re saving triggers to our eye-tracking data and also saving the frame index at each sample (as a continuous number column).\n\n\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\n\n# Import some libraries from PsychoPy\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound\n\nimport tobii_research as tr\n\n\n#%% Functions\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global gaze_data_buffer\n    global winsize\n    global frame_indx\n    \n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n        \n    # Add gaze data to the buffer \n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,trigger, frame_indx))\n    trigger = ''\n    \ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n    \n    # Define column names\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V', \n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event', 'FrameIndex']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n    \n    # Check if the file exists\n    file_exists = not os.path.isfile(output_path)\n    \n    # Write the DataFrame to an HDF5 file\n    out.to_csv(output_path, mode='a', index =False, header = file_exists)\n    \n    \n    \n#%% Load and prepare stimuli\n\nos.chdir(r'C:\\Users\\tomma\\Desktop\\EyeTracking\\Files')\n\n# Winsize\nwinsize = (960, 540)\n\n# create a window\nwin = visual.Window(size = winsize,fullscr=False, units=\"pix\", screen=0)\n\n\n# Load images and video\nfixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\nVideo = visual.MovieStim(win, filename='EXP\\\\Stimuli\\\\Video60.mp4',  loop=False, size=[600,380],volume =0.4, autoStart=True)  \n\n\n# Define the trigger and frame index variable to pass to the gaze_data_callback\ntrigger = ''\nframe_indx = np.nan\n\n\n\n#%% Record the data\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\n#Start recording\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\n# Crate empty list to append data\ngaze_data_buffer = []\n\nTrials_number = 10\nfor trial in range(Trials_number):\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n    \n    fixation.draw()\n    win.flip()\n    trigger = 'Fixation'\n    core.wait(1)  # wait for 1 second\n\n    Video.play()\n    trigger = 'Video'\n    while not Video.isFinished:\n\n        # Draw the video frame\n        Video.draw()\n\n        # Flip the window and add index to teh frame_indx\n        win.flip()\n        \n        # add which frame was just shown to the eyetracking data\n        frame_indx = Video.frameIndex\n        \n    Video.stop()\n    win.flip()\n\n\n    ### ISI\n    win.flip()    # we re-flip at the end to clean the window\n    clock = core.Clock()\n    write_buffer_to_file(gaze_data_buffer, 'DATA\\\\RAW\\\\Test.csv')\n    while clock.getTime() &lt; 1:\n        pass\n    \n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        Eyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\n        core.quit()  # stop study\n      \nwin.close() # close window\nEyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\ncore.quit() # stop study"
  },
  {
    "objectID": "CONTENT/Workshops/GAP_2024.html#calibration",
    "href": "CONTENT/Workshops/GAP_2024.html#calibration",
    "title": "Bridging the Technological Gap Workshop (GAP)",
    "section": "Calibration",
    "text": "Calibration\nWe received a question about the calibration. How to change the focus time that the eye-tracking uses to record samples for each calibration point. Luckily, the function from the Psychopy_tobii_infant repository allows for an additional argument that specifies how long we want the focus time (default = 0.5s). Thus, you can simply change it by running it with a different value.\nHere below we changed the example of Calibrating eye-tracking by increasing the focus_time to 2s. You can increase or decrease it based on your needs!!\n\nimport os\nfrom psychopy import visual, sound\n\n# import Psychopy tobii infant\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\nfrom psychopy_tobii_infant import TobiiInfantController\n\n\n#%% window and stimuli\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')  \n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\n\n#%% Center face - screen\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n#%% Calibration\n\n# define calibration points\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound, focus_time=2)\nwin.flip()"
  }
]