[
  {
    "objectID": "Registration.html",
    "href": "Registration.html",
    "title": "DevStart_Workshop",
    "section": "",
    "text": "Interested in joining the workshop? Simply click the button below to access the application form and secure your spot.\n\n\nThe workshop has a cost of £35, which includes:\n\nFull participation in the workshop.\nAccess to all materials, uploaded and shared online.\nCoffee breaks throughout the sessions.\nEntry to the poster session.\n\n\n\n  \n    Register here\n  \n\n\n  \n\n\nFor any questions, feel free to reach out to us at: t.ghilardi@bbk.ac.uk."
  },
  {
    "objectID": "Registration.html#apply",
    "href": "Registration.html#apply",
    "title": "DevStart_Workshop",
    "section": "",
    "text": "Interested in joining the workshop? Simply click the button below to access the application form and secure your spot.\n\n\nThe workshop has a cost of £35, which includes:\n\nFull participation in the workshop.\nAccess to all materials, uploaded and shared online.\nCoffee breaks throughout the sessions.\nEntry to the poster session.\n\n\n\n  \n    Register here\n  \n\n\n  \n\n\nFor any questions, feel free to reach out to us at: t.ghilardi@bbk.ac.uk."
  },
  {
    "objectID": "Information.html",
    "href": "Information.html",
    "title": "DevStart_Workshop",
    "section": "",
    "text": "We are three postdoctoral researchers who became friends while overcoming the challenges of our PhDs. After years of struggle and growth, we realized the value of sharing the hard-earned knowledge we gained along the way. This led us to create DevStart—a platform where we share what we’ve learned through simple, accessible tutorials. Having presented at various workshops, we now want to take the next step and host our own, where we can have full control over what we teach and how we teach it.\n\n  \n    \n      \n      Tommaso Ghilardi\n      Postdoctoral researcherCentre for Brain and Cognitive Development\n    \n  \n  \n  \n    \n      \n      Giulia Serino\n      Postdoctoral researcherCentre for Brain and Cognitive Development\n    \n  \n  \n  \n    \n      \n      Francesco Poli\n      Postdoctoral researcherCambridge University"
  },
  {
    "objectID": "Information.html#speakers",
    "href": "Information.html#speakers",
    "title": "DevStart_Workshop",
    "section": "",
    "text": "We are three postdoctoral researchers who became friends while overcoming the challenges of our PhDs. After years of struggle and growth, we realized the value of sharing the hard-earned knowledge we gained along the way. This led us to create DevStart—a platform where we share what we’ve learned through simple, accessible tutorials. Having presented at various workshops, we now want to take the next step and host our own, where we can have full control over what we teach and how we teach it.\n\n  \n    \n      \n      Tommaso Ghilardi\n      Postdoctoral researcherCentre for Brain and Cognitive Development\n    \n  \n  \n  \n    \n      \n      Giulia Serino\n      Postdoctoral researcherCentre for Brain and Cognitive Development\n    \n  \n  \n  \n    \n      \n      Francesco Poli\n      Postdoctoral researcherCambridge University"
  },
  {
    "objectID": "Information.html#program-overview",
    "href": "Information.html#program-overview",
    "title": "DevStart_Workshop",
    "section": "Program Overview",
    "text": "Program Overview\n\nDay 1\n9-10: Help-desk setting up Python and R\nEye-Tracking Experiment Design and Data Collection\n\n10:00 - 10:45 Theory of Eye-Tracking in Developmental Research\nIntroduction to eye-tracking technology and its applications in developmental science.\nCoffee break\n11 - 11:45 Building Your First Eye-Tracking Experiment\nHands-on session to create a basic experiment in Python using PsychoPy.\nLunch break\n13 - 13:30 Coding Eye-Tracking Connection\nLearn how to integrate eye-tracking hardware with your experimental code for real-time data capture.\n13:30 - 14:15 Calibration with Tobii Infant Eye Tracker\nUnderstand the steps required to calibrate eye-tracking equipment for use with infant participants.\n14:30 - 14:45 Data Collection\nLearn best practices for collecting reliable and accurate eye-tracking data.\nCoffee break\n15 - 16 Using I2MC for Robust Fixation Extraction\nProcess raw eye-tracking data and extract fixations using the I2MC algorithm.\n16 - 17:15 Measures of Interest\n\nDefine and use AOIs (Areas of Interest) in your experiment.\nFocus on relevant time windows in eye-tracking data.\nCombine spatial and temporal parameters to extract meaningful metrics (e.g., saccadic latency, first fixation, looking time).\n\n17:15 - 17:30 Conclusion and Q&A\nDiscuss the day’s topics, ask questions\n\n\n\nDay 2\nData Analysis in R\n\n9 - 9:30 Introduction to Linear Models in R\nLearn the basics of linear models and their applications in analyzing experimental data.\n9:30 - 10 Linear Models with Continuous Variables, Categorical Variables, and Interaction Effects\nExplore how to include different types of predictors in your models and interpret their effects.\nCoffee break\n10:15 - 11:00 Estimating Effects, Means, and Contrasts\nUnderstand how to extract meaningful insights from your models, including effects and group comparisons.\n11 - 12 Plotting the Results\nUse R’s visualization tools to create clear, informative plots for presenting your results.\nLunch break\n13 - 13:45 Checking Model Assumptions\nLearn diagnostic techniques to ensure your models meet key assumptions.\n13:45 - 14:45 Generalized Linear Models\nExtend linear models to handle binary or count data, commonly used in developmental research.\n14:45 - 16 Mixed Effects Models (Random Effects)\n\nUnderstand the theory behind mixed-effects models.\nLearn how to include random intercepts and slopes in your analyses.\n\n16 - 16:30 Wrap-Up and Discussion\nSummarize key concepts and discuss how to apply these techniques in your research.\n18 - … Poster presentation"
  },
  {
    "objectID": "Information.html#poster-session",
    "href": "Information.html#poster-session",
    "title": "DevStart_Workshop",
    "section": "Poster Session",
    "text": "Poster Session\nWe are excited to host a poster session where participants can present their research, exchange ideas, and network with other attendees. The session will take place at the end of Day 2 and offers a fantastic opportunity to showcase your work, get feedback from peers, and spark potential collaborations. Whether you’re presenting or just attending, this session is a valuable addition to the workshop experience!\nMore details about the poster dimensions and guidelines will be shared soon, so stay tuned!"
  },
  {
    "objectID": "Information.html#material",
    "href": "Information.html#material",
    "title": "DevStart_Workshop",
    "section": "Material",
    "text": "Material\nSlides and presentation materials from the workshop will be shared on this website after the sessions, ensuring participants have access to all the key resources. However, most of the core material covered during the workshop is already part of the DevStart website, where it is available as detailed tutorials and documentation.\nThe DevStart website serves as the foundational resource for this workshop, offering step-by-step guides, experiment scripts, data processing pipelines, and statistical analysis tutorials. By building on these resources, the workshop aims to provide a practical, hands-on experience while promoting long-term learning and accessibility.   \n\n\nFor any questions, feel free to reach out to us at: t.ghilardi@bbk.ac.uk."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DevStart_Workshop",
    "section": "",
    "text": "Eye-Tracking Workshop\n      forDevelopmental Scientists\n    \n    The first workshop brought to you by DevStart.\n  \n  \n    \n      \n    \n  \n\n\n\n  \n    3 April - 4 April, 2025\n    Bloomsbury, London\n  \n\nEye-tracking is a powerful tool for studying cognition, particularly in developmental science where participants like infants and young children cannot verbally communicate their thoughts. This comprehensive two-day workshop is designed to equip developmental scientists with the skills needed to effectively design, implement, and analyze eye-tracking experiments. To promote open science without financial barriers, we will use open-source tools throughout the workshop, ensuring the techniques and methods are accessible and widely applicable.\n\n\n\n\n\n\n\nWhile this workshop is primarily tailored to meet the needs of developmental scientists, its foundational principles and practical skills are universally applicable.\nResearchers and students outside the field of developmental science are warmly encouraged to participate and will find the content equally valuable for expanding their expertise in eye-tracking methodologies.\n\n\n\nDay 1 covers the basics of eye-tracking studies. Participants will learn to build experiments using Python and PsychoPy, connect and calibrate Tobii eye-tracking hardware, and collect gaze and pupil data. The day also includes preprocessing with the I2MC algorithm and defining Areas and Times of Interest to extract measures like saccadic latency and looking time.\nDay 2 focuses on data analysis using R, including linear, generalized linear, and mixed effects models. Participants will learn to handle different variables, interpret interactions, estimate effects, and validate models, applying these techniques to eye-tracking data through hands-on sessions. The day will conclude with a poster session, providing attendees the opportunity to present their research projects, share their current efforts and future plans, and engage in collaborative discussions.\nBy the end of the workshop, participants will have a comprehensive understanding of the entire eye-tracking research process—from experiment design and data collection to advanced data analysis.\n​The workshop will be held on 3-4 April 2025 in Bloomsbury, London.\n\n\n\n\n\n\nAlthough this workshop will focus on leveraging Tobii eye-trackers for data collection, the concepts, preprocessing, and analysis techniques covered are universal and can be applied to eye-tracking data from any system.\n\n\n\n\n\n  \n    Register here"
  },
  {
    "objectID": "PreviousWokshops/BTG2024.html",
    "href": "PreviousWokshops/BTG2024.html",
    "title": "BTG 2024",
    "section": "",
    "text": "Hello hello!!! This page has been created to provide support and resources for the tutorial that will take place during the Bridging the Technological Gap Workshop."
  },
  {
    "objectID": "PreviousWokshops/BTG2024.html#python",
    "href": "PreviousWokshops/BTG2024.html#python",
    "title": "BTG 2024",
    "section": "Python",
    "text": "Python\nIn this tutorial, our primary tool will be Python!! There are lots of ways to install python. We recommend installing it via Miniconda. However, for this workshop, the suggested way to install Python is using Anaconda.\nYou might ask….Then which installation should I follow? Well, it doesn’t really matter! Miniconda is a minimal installation of Anaconda. It lacks the GUI, but has all the main features. So follow whichever one you like more!\nOnce you have it installed, we need a few more things. For the Gaze Tracking & Pupillometry Workshop (the part we will be hosting) we will need some specific libraries and files. We have tried our best to make everything as simple as possible:\n\nLibraries\nWe will be working with a conda environment (a self-contained directory that contains a specific collection of Python packages and dependencies, allowing you to manage different project requirements separately). To create this environment and install all the necessary libraries, all you need is this file:\n Psychopy.yml \nOnce you have downloaded the file, simply open the anaconda/miniconda terminal and type conda env create -f, then simply drag and drop the downloaded file onto the terminal. This will copy the filename with its absolute path. In my case it looked something like this:\n\n\n\n\n\nNow you will be asked to confirm a few things (by pressing Y) and after a while of downloading and installing you will have your new workshop environment called Psychopy!\nNow you should see a shortcut in your start menu called Spyder(psychopy), just click on it to open spyder in our newly created environment. If you don’t see it, just reopen the anaconda/miniconda terminal, activate your new environment by typing conda activate psychopy and then just type spyder.\n\n\nFiles\nWe also need some files if you want to run the examples with us. Here you can download the zip files with everything you need:\n Files \nOnce downloaded, simply extract the file by unzipping it. For our workshop we will work together in a folder that should look like this:\n\n\n\n\n\nIf you have a similar folder… you are ready to go!!!!"
  },
  {
    "objectID": "PreviousWokshops/BTG2024.html#videos",
    "href": "PreviousWokshops/BTG2024.html#videos",
    "title": "BTG 2024",
    "section": "Videos",
    "text": "Videos\nWe received several questions about working with videos and PsychoPy while doing eye-tracking. It can be quite tricky, but here are some tips:\n\nMake sure you’re using the right codec.\nIf you need to change the codec of the video, you can re-encode it using a tool like\nHandbrake (remember to set the constant framerate in the video option)\n\nBelow, you’ll find a code example that adapts our Create an eye-tracking experiment tutorial to work with a video file. The main differences are:\n\nWe’re showing a video after the fixation.\nWe’re saving triggers to our eye-tracking data and also saving the frame index at each sample (as a continuous number column).\n\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\n\n# Import some libraries from PsychoPy\nfrom psychopy import core, event, visual, prefs\nprefs.hardware['audioLib'] = ['PTB']\nfrom psychopy import sound\n\nimport tobii_research as tr\n\n\n#%% Functions\n\n# This will be called every time there is new gaze data\ndef gaze_data_callback(gaze_data):\n    global trigger\n    global gaze_data_buffer\n    global winsize\n    global frame_indx\n\n    # Extract the data we are interested in\n    t  = gaze_data.system_time_stamp / 1000.0\n    lx = gaze_data.left_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ly = winsize[1] - gaze_data.left_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    lp = gaze_data.left_eye.pupil.diameter\n    lv = gaze_data.left_eye.gaze_point.validity\n    rx = gaze_data.right_eye.gaze_point.position_on_display_area[0] * winsize[0]\n    ry = winsize[1] - gaze_data.right_eye.gaze_point.position_on_display_area[1] * winsize[1]\n    rp = gaze_data.right_eye.pupil.diameter\n    rv = gaze_data.right_eye.gaze_point.validity\n\n    # Add gaze data to the buffer\n    gaze_data_buffer.append((t,lx,ly,lp,lv,rx,ry,rp,rv,trigger, frame_indx))\n    trigger = ''\n\ndef write_buffer_to_file(buffer, output_path):\n\n    # Make a copy of the buffer and clear it\n    buffer_copy = buffer[:]\n    buffer.clear()\n\n    # Define column names\n    columns = ['time', 'L_X', 'L_Y', 'L_P', 'L_V',\n               'R_X', 'R_Y', 'R_P', 'R_V', 'Event', 'FrameIndex']\n\n    # Convert buffer to DataFrame\n    out = pd.DataFrame(buffer_copy, columns=columns)\n\n    # Check if the file exists\n    file_exists = not os.path.isfile(output_path)\n\n    # Write the DataFrame to an HDF5 file\n    out.to_csv(output_path, mode='a', index =False, header = file_exists)\n\n\n\n#%% Load and prepare stimuli\n\nos.chdir(r'C:\\Users\\tomma\\Desktop\\EyeTracking\\Files')\n\n# Winsize\nwinsize = (960, 540)\n\n# create a window\nwin = visual.Window(size = winsize,fullscr=False, units=\"pix\", screen=0)\n\n\n# Load images and video\nfixation = visual.ImageStim(win, image='EXP\\\\Stimuli\\\\fixation.png', size = (200, 200))\nVideo = visual.MovieStim(win, filename='EXP\\\\Stimuli\\\\Video60.mp4',  loop=False, size=[600,380],volume =0.4, autoStart=True)\n\n\n# Define the trigger and frame index variable to pass to the gaze_data_callback\ntrigger = ''\nframe_indx = np.nan\n\n\n\n#%% Record the data\n\n# Find all connected eye trackers\nfound_eyetrackers = tr.find_all_eyetrackers()\n\n# We will just use the first one\nEyetracker = found_eyetrackers[0]\n\n#Start recording\nEyetracker.subscribe_to(tr.EYETRACKER_GAZE_DATA, gaze_data_callback)\n\n# Crate empty list to append data\ngaze_data_buffer = []\n\nTrials_number = 10\nfor trial in range(Trials_number):\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n\n    fixation.draw()\n    win.flip()\n    trigger = 'Fixation'\n    core.wait(1)  # wait for 1 second\n\n    Video.play()\n    trigger = 'Video'\n    while not Video.isFinished:\n\n        # Draw the video frame\n        Video.draw()\n\n        # Flip the window and add index to teh frame_indx\n        win.flip()\n\n        # add which frame was just shown to the eyetracking data\n        frame_indx = Video.frameIndex\n\n    Video.stop()\n    win.flip()\n\n\n    ### ISI\n    win.flip()    # we re-flip at the end to clean the window\n    clock = core.Clock()\n    write_buffer_to_file(gaze_data_buffer, 'DATA\\\\RAW\\\\Test.csv')\n    while clock.getTime() &lt; 1:\n        pass\n\n    ### Check for closing experiment\n    keys = event.getKeys() # collect list of pressed keys\n    if 'escape' in keys:\n        win.close()  # close window\n        Eyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\n        core.quit()  # stop study\n\nwin.close() # close window\nEyetracker.unsubscribe_from(tr.EYETRACKER_GAZE_DATA, gaze_data_callback) # unsubscribe eyetracking\ncore.quit() # stop study"
  },
  {
    "objectID": "PreviousWokshops/BTG2024.html#calibration",
    "href": "PreviousWokshops/BTG2024.html#calibration",
    "title": "BTG 2024",
    "section": "Calibration",
    "text": "Calibration\nWe received a question about the calibration. How to change the focus time that the eye-tracking uses to record samples for each calibration point. Luckily, the function from the Psychopy_tobii_infant repository allows for an additional argument that specifies how long we want the focus time (default = 0.5s). Thus, you can simply change it by running it with a different value.\nHere below we changed the example of Calibrating eye-tracking by increasing the focus_time to 2s. You can increase or decrease it based on your needs!!\nimport os\nfrom psychopy import visual, sound\n\n# import Psychopy tobii infant\nos.chdir(r\"C:\\Users\\tomma\\Desktop\\EyeTracking\\Files\\Calibration\")\nfrom psychopy_tobii_infant import TobiiInfantController\n\n\n#%% window and stimuli\nwinsize = [1920, 1080]\nwin = visual.Window(winsize, fullscr=True, allowGUI=False,screen = 1, color = \"#a6a6a6\", unit='pix')\n\n# visual stimuli\nCALISTIMS = glob.glob(\"CalibrationStim\\\\*.png\")\n\n# video\nVideoGrabber = visual.MovieStim(win, \"CalibrationStim\\\\Attentiongrabber.mp4\", loop=True, size=[800,450],volume =0.4, unit = 'pix')\n\n# sound\nSound = sound.Sound(directory + \"CalibrationStim\\\\audio.wav\")\n\n\n#%% Center face - screen\n\n# set video playing\nVideoGrabber.setAutoDraw(True)\nVideoGrabber.play()\n\n# show the relative position of the subject to the eyetracker\nEyeTracker.show_status()\n\n# stop the attention grabber\nVideoGrabber.setAutoDraw(False)\nVideoGrabber.stop()\n\n\n#%% Calibration\n\n# define calibration points\nCALINORMP = [(-0.4, 0.4), (-0.4, -0.4), (0.0, 0.0), (0.4, 0.4), (0.4, -0.4)]\nCALIPOINTS = [(x * winsize[0], y * winsize[1]) for x, y in CALINORMP]\n\nsuccess = controller.run_calibration(CALIPOINTS, CALISTIMS, audio = Sound, focus_time=2)\nwin.flip()"
  }
]