[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "This is a helpline for all the new PhD students in developmental psychology and developmental cognitive neuroscience, and everyone who is approaching this world for the first time. \nAssuming no apriori knowledge, this website will guide you through your first steps as a developmental researcher. You’ll find many examples and guidelines on how to grasp the basic principles of developmental psychology research, design and set up a study with different research methods, analyse data, and start programming.\n\n\nThere are many resources on the web to learn programming, build experiments, and analyse data. However, they often assume basic skills in programming, and they don’t tackle the challenges that we encounter when testing babies and young children. This knowledge is often handed down from Professors and Postdocs to PhD students, or it is acquired through hours and hours of despair. \nWe tried to summarise all the struggles that we encountered during our PhDs and the solutions we came up with. With this website, we aim to offer you a solution to overcome these problems and we invite you to do better than us and contribute to this open science framework! \n\n\n\nIf you’re an expert on a specific topic and you want to change something or add something, if you don’t agree with our pipeline, or you spot an error in our code, please get in touch! \nYou can find us at:\n\n t.ghilardi@bbk.ac.uk\n\n francesco.poli@donders.ru.nl\n\n g.serino@bbk.ac.uk\n\n\n\n\n\nThis is a story of bullying and love started in 2018 at the Donders Institute in the Netherlands. Tommaso and Francesco were living their best PhD life when Giulia bumped into it, looking for data to write her master’s thesis.\nUnited by a strong passion for research and a creepy attraction to complex and noisy datasets, they immediately got along well. We haven’t stopped collaborating since then.\n\n\n\n\n\n\nTommaso Ghilardi is the programmer behind the scenes. He excels in Python and is known for his impeccably clean code. If you explore our website, you're likely to come across his contributions. Tommaso's skills aren't limited to programming—he's capable of setting up a lab in just an hour. He's equally comfortable with research methods and a variety of programming languages. However, be cautious about imperfect code; Tommaso has a short fuse for anything less than perfect. He plays a significant role in shaping this website, making it what it is today. Thank you, Tommaso!\n\n\n\nPostdoc at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://tommasoghilardi.github.io\n t.ghilardi@bbk.ac.uk\n @TommasoGhi\n\n\n\n\n\n\n\n\nFrancesco Poli is the deep thinker among us. He's not just any ordinary thinker; he can build computational models to predict behavior and decipher your thoughts in the blink of an eye. Francesco's abilities extend to creating intricate mathematical theories to explain behavior patterns. He's adept at juggling multiple projects, though he might occasionally forget to reply to emails amidst his busy schedule. With Francesco, expect the extraordinary.\n\n\n\nPostDoc at the Donders Centre for Cognition.\n https://www.ru.nl/en/people/poli-f\n francesco.poli@donders.ru.nl\n @FrancescPoli\n\n\n\n\n\n\n\n\nGiulia Serino brings imagination to the table. Introduced to programming by Francesco and Tommaso, she has surpassed them to become a skilled programmer herself. Her experiments are a blend of art and science, often resembling graphic masterpieces. However, Giulia tends to get lost in the minutiae of her work. She has a reputation for disappearing, yet her curiosity leads her to explore every corner of Google and various repositories, piecing together her unique theories on attention development.\n\n\n\nPhD student in Developmental Cognitive Neuroscience at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://cbcd.bbk.ac.uk/people/students/giulia-serino\n g.serino@bbk.ac.uk\n @GiSerino\n\n\n\n\nWe tried our best to offer the best and most accurate solutions. However, do always check the code and if the outputs make sense. Please get in touch if you spot any errors.\nWe apologize in advance for our poor coding skills. Our scripts are not perfect, and they don’t mean to be. But, as Francesco always says, they work! And we hope they will support you during your PhD."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#heading",
    "href": "index.html#heading",
    "title": "DevStart",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#what-is-this",
    "href": "index.html#what-is-this",
    "title": "DevStart",
    "section": "",
    "text": "This is a helpline for all the new PhD students in developmental\npsychology and developmental cognitive neuroscience, and everyone who is\napproaching this world for the first time. \nAssuming no apriori knowledge, this website will guide you through your\nfirst steps as a developmental researcher. You’ll find many examples and\nguidelines on how to grasp the basic principles of developmental\npsychology research, design and set up a study with different research\nmethods, analyse data, and start programming."
  },
  {
    "objectID": "index.html#why-we-created-it",
    "href": "index.html#why-we-created-it",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "There are many resources on the web to learn programming, build experiments, and analyse data. However, they often assume basic skills in programming, and they don’t tackle the challenges that we encounter when testing babies and young children. This knowledge is often handed down from Professors and Postdocs to PhD students, or it is acquired through hours and hours of despair. \nWe tried to summarise all the struggles that we encountered during our PhDs and the solutions we came up with. With this website, we aim to offer you a solution to overcome these problems and we invite you to do better than us and contribute to this open science framework!"
  },
  {
    "objectID": "index.html#how-to-contribute",
    "href": "index.html#how-to-contribute",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "If you’re an expert on a specific topic and you want to change something or add something, if you don’t agree with our pipeline, or you spot an error in our code, please get in touch! \nYou can find us at:\n\n t.ghilardi@bbk.ac.uk\n\n francesco.poli@donders.ru.nl\n\n g.serino@bbk.ac.uk"
  },
  {
    "objectID": "index.html#who-we-are",
    "href": "index.html#who-we-are",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "This is a story of bullying and love started in 2018 at the Donders Institute in the Netherlands. Tommaso and Francesco were living their best PhD life when Giulia bumped into it, looking for data to write her master’s thesis.\nUnited by a strong passion for research and a creepy attraction to complex and noisy datasets, they immediately got along well. We haven’t stopped collaborating since then.\n\n\n\n\n\n\nTommaso Ghilardi is the programmer behind the scenes. He excels in Python and is known for his impeccably clean code. If you explore our website, you're likely to come across his contributions. Tommaso's skills aren't limited to programming—he's capable of setting up a lab in just an hour. He's equally comfortable with research methods and a variety of programming languages. However, be cautious about imperfect code; Tommaso has a short fuse for anything less than perfect. He plays a significant role in shaping this website, making it what it is today. Thank you, Tommaso!\n\n\n\nPostdoc at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://tommasoghilardi.github.io\n t.ghilardi@bbk.ac.uk\n @TommasoGhi\n\n\n\n\n\n\n\n\nFrancesco Poli is the deep thinker among us. He's not just any ordinary thinker; he can build computational models to predict behavior and decipher your thoughts in the blink of an eye. Francesco's abilities extend to creating intricate mathematical theories to explain behavior patterns. He's adept at juggling multiple projects, though he might occasionally forget to reply to emails amidst his busy schedule. With Francesco, expect the extraordinary.\n\n\n\nPostDoc at the Donders Centre for Cognition.\n https://www.ru.nl/en/people/poli-f\n francesco.poli@donders.ru.nl\n @FrancescPoli\n\n\n\n\n\n\n\n\nGiulia Serino brings imagination to the table. Introduced to programming by Francesco and Tommaso, she has surpassed them to become a skilled programmer herself. Her experiments are a blend of art and science, often resembling graphic masterpieces. However, Giulia tends to get lost in the minutiae of her work. She has a reputation for disappearing, yet her curiosity leads her to explore every corner of Google and various repositories, piecing together her unique theories on attention development.\n\n\n\nPhD student in Developmental Cognitive Neuroscience at the Centre for Brain and Cognitive Development, Birkbeck, University of London\n https://cbcd.bbk.ac.uk/people/students/giulia-serino\n g.serino@bbk.ac.uk\n @GiSerino"
  },
  {
    "objectID": "index.html#warnings",
    "href": "index.html#warnings",
    "title": "Welcome to DevStart",
    "section": "",
    "text": "We tried our best to offer the best and most accurate solutions. However, do always check the code and if the outputs make sense. Please get in touch if you spot any errors.\nWe apologize in advance for our poor coding skills. Our scripts are not perfect, and they don’t mean to be. But, as Francesco always says, they work! And we hope they will support you during your PhD."
  },
  {
    "objectID": "GettingStartedWithPython.html",
    "href": "GettingStartedWithPython.html",
    "title": "Starting with Python",
    "section": "",
    "text": "Python is one of the most popular programming languages in general. In data science, it competes with Matlab and R for first place on the podium.\nIn our everyday we often use python to pre-process and analyze the data. In this tutorial we will explain our preferred way of installing python and managing its libraries. There are several ways to install python\nthis is the one we recommend for its simplicity and flexibility"
  },
  {
    "objectID": "GettingStartedWithPython.html#anaconda",
    "href": "GettingStartedWithPython.html#anaconda",
    "title": "Starting with Python",
    "section": "Anaconda",
    "text": "Anaconda\nAnaconda is an open-source distribution for python. It is used for data science, machine learning, deep learning, etc. It makes simple to download and organize your python environment.\nTo install Anaconda simply follow this link. Choose a version suitable for you and click on download. Once you complete the download, open the setup.\n\n\n\n\n\nFollow the instructions in the setup. Don’t forget to click on add anaconda to my path environment variable. As shown below:\n\n\n\n\n\nAfter the installation is complete, launch the Anaconda navigator. The Anaconda Navigator is a desktop GUI that comes with the anaconda distribution. Navigator allows you to launch common Python programs and easily manage conda packages, environments, and channels without using command-line commands. Navigator can search for packages on Anaconda Cloud or in a local Anaconda Repository."
  },
  {
    "objectID": "GettingStartedWithPython.html#miniconda",
    "href": "GettingStartedWithPython.html#miniconda",
    "title": "Starting with Python",
    "section": "Miniconda",
    "text": "Miniconda\nWhile Anaconda is one of the most simple way to install python it is quite heavy. It install multiple programs and libraries that most of the time are unnecessary. For this reason we prefer to use Miniconda.\nMiniconda is a minimal installer for conda. It is a small, bootstrap version of Anaconda that includes only conda, Python, the packages they depend on, and a small number of other useful packages. This means that fewer packages are installed and that we have more control on what to have on our PC. To use Miniconda download the installer from Miniconda\nThe installation is exactly the same as the one for Anaconda. After it has finished the installation you will find the Anaconda Prompt between your programs. We will use the Anaconda Prompt to install the packages and the software that we need."
  },
  {
    "objectID": "GettingStartedWithPython.html#creating-an-environment-from-an-environment.yml-file",
    "href": "GettingStartedWithPython.html#creating-an-environment-from-an-environment.yml-file",
    "title": "Starting with Python",
    "section": "Creating an environment from an environment.yml file",
    "text": "Creating an environment from an environment.yml file\nAnother interesting feature of conda is that we can save an environment file from an environment we have created. We can later use this file to recreate the same environment with all its packages. This is very convenient when we want to have several people and several PCs with the same environment. We will provide some environment files in our tutorials. These environments have been tested for their specific use and will provide a standard environment for all the users.\nTo create an environment from a environment file just type:\nconda env create -f environment.yml\nThis again will take some time but it will create the new environment with all the packages specified in it.\nconda env create -f environment.yml"
  },
  {
    "objectID": "GettingStartedWithPsychopy.html",
    "href": "GettingStartedWithPsychopy.html",
    "title": "Starting with PsychoPy",
    "section": "",
    "text": "PsychoPy is an open source software package written in the Python programming language primarily for use in neuroscience and experimental psychology research. It’s one of our favorite ways to create experiments and we will use it through our tutorials.\nSo, let’s start and install PsychoPy!!!"
  },
  {
    "objectID": "GettingStartedWithPsychopy.html#install-psychopy",
    "href": "GettingStartedWithPsychopy.html#install-psychopy",
    "title": "Starting with PsychoPy",
    "section": "Install PsychoPy",
    "text": "Install PsychoPy\nAs reported on the PsychoPy website, there are multiple ways to install PsychoPy. Our favorite way to install it is using conda (refer to the Getting started with python).\nPsychoPy offers a nice .yml file that will install everything that we need. Download the file from their website and store it somewhere.\nNow open the Anaconda prompt and type:\nconda env create -f  your_dowloaded_file.yml\nThis will take some time and will ask for confirmation but in the end you will have a nice virtual environment containing everything you need to run your experiment on PsychoPy.\nThe virtual environment will be called “psychopy” and you can activate it just by typing:\nconda activate psychopy\nWe are done!! You now should have a nice conda environment called “psychoPy” with PsychoPy in it. To launch the PsychoPy Gui (also referred to as Builder) you can just type psychopy in your Anaconda prompt and PsychoPy will launch. You are definitely free to try the Builder or the Coder that PsychoPy offers, however we usually prefer to write and launch our scripts from Spyder, let’s be honest, if you want to code your experiment the Gui of Spyder is way better!!\nThus we suggest to install Spyder in your newly created “psychopy” environment. You can do that just by simply typing in:\nconda install –c anaconda spyder\nYou can also refer to our guide Installing Spyder ide.\nNow that you should have both PsychoPy and Spyder installed let’s see how to create a simple experiment with them."
  },
  {
    "objectID": "GettingStartedWithPsychopy.html#lets-create-our-first-experiment-using-psychopy.",
    "href": "GettingStartedWithPsychopy.html#lets-create-our-first-experiment-using-psychopy.",
    "title": "Starting with PsychoPy",
    "section": "Let’s create our first experiment using PsychoPy.",
    "text": "Let’s create our first experiment using PsychoPy.\nWe will create a very simple and basic experiment that will be the stepping stone for some of the future tutorials. In the future tutorials we will show you how to extend and make this tutorial in a real experiment.\n\n\n\n\n\n\nStimuli!\n\n\n\nYou can download from here the stimuli that we will use in this example. They are very simple and basic stimuli:\n\na fixation cross\ntwo cues (a circle and a square)\na reward (a cute medal)\na non-reward (a cartoon of an exploding empty cloud)\na sound of winning at an arcade game\na sound of losing at an arcade game\n\n\n\nIn this tutorial we will create an experiment in which, after the fixation cross, one of the two cues is presented. The cues will indicate whether we will receive a reward or not and where this will appear. After the circle is presented as cue the medal will be presented on the right. After the circle the empty cloud will be presented on the left. Thus, if you follow the cued indication you will be able to predict the location of the next stimuli and whether or not it will be rewarding.\n\nPreparation:\nFirst thing first let’s import the relevant libraries and define the path where our stimuli are in. PsychoPy has a lot of different modules that allow us to interface with different type of stimuli and systems. For this tutorial\n\n# Import some libraries from PsychoPy\nfrom psychopy import core\nfrom psychopy import visual  \nfrom psychopy import sound\n\n# Let's define the path we will work with.\n# in my case the stimuli that we will use are in here but you will have to adapt\n# this variable to where you have downloaded the stimuli.\nPath = 'C:\\\\Users\\\\tomma\\\\surfdrive\\\\Documentation\\\\psychopy\\\\'\n\n\n\nStimuli:\nThe next step is to create the window The window is what we will show the stimuli in; it is the canvas in which to draw objects. For now we will create a small window of 800*600 pixels. In this way we will able to see the stimuli and still interact with the rest of our pc interface. In a real experiment we would probably set the window dimension to the entirety of the the display (Fullscreen).\n\n#create a window\nwin = visual.Window([800,600], units=\"pix\")\n\nNow let’s import the stimuli that we will present in this tutorial. We have 5 stimuli:\n\na fixation cross that we will use to catch the attention of our participants\na circle that will be our cue that signal a rewarding trial\na square that will be our cue that signal a non-rewarding trial\na cartoon of a medal that will be our reward\na cartoon of an empty cloud that will be our non-reward\n\nOn top of these visual stimuli we will also import two sounds that will help us signal the type of trials. So:\n\na tada! winning sound\na papapaaa! losing sound\n\n\n\n\n\n\n\nTip\n\n\n\nWhen importing a visual stimulus we need to pass to the importing function in which window it will be displayed. In our case we will pass all of them the “win” window that we just created.\n\n\n\n# Load images\nfixation = visual.ImageStim(win, image=Path + 'fixation.png', size = (100, 100))\ncircle   = visual.ImageStim(win, image=Path + 'circle.png', size = (100, 100))\nsquare   = visual.ImageStim(win, image=Path + 'square.png', size = (100, 100))\nwinning   = visual.ImageStim(win, image=Path + 'winning.png', size = (100, 100), pos=(250,0))\nloosing  = visual.ImageStim(win, image=Path + 'loosing.png', size = (100, 100), pos=(-250,0))\n\n# Load sound\nwinning_sound = sound.Sound(Path + 'winning.wav')\nlosing_sound = sound.Sound(Path + 'loosing.wav')\n\nNote that in this simple experiment we will present the reward always on the right and the non-rewards always on the left that’s why when we import the two rewards we set their pos to (250,0) and (-250,0). The first value indicates the number of pixels on the x-axis and the second the number of pixels on the y-axis.\n\n\nShow a visual stimulus:\nNo we want to show a stimuli in the center of our window. To do so we will have to use the function “draw”. As the name suggests this function draws the stimulus that we want on the window.\nLet’s start with displaying the fixation cross in the center.\n\n# Draw the fixation\nfixation.draw()\n\nDo you see the fixation cross?????? Probably not!! This is because we have drawn the fixation cross but we have not refreshed the window. Psychopy allows you to draw as many stimuli as you want on a window but the changes are only shown when you “refresh” the window. To do so we need to use the “flip” function.\n\nwin.flip()\n\nPerfect!!!! The fixation cross is there. Before each flip we need to draw our objects. Otherwise we will only see the basic window with nothing in it. Let’s try!!! flip the window now.\n\n# Flipping the window (refreshing)\nwin.flip()\n\nThe fixation is gone again! Exactly as predicted. Flipping the window allows us to draw and show something new each frame. This means that the speed limit of our presentation is the actual frame rate of our display. If we have a 60Hz display we can present an image 60 times in a second.\nSo if we want to present our fixation for an entire second we would have to draw and flip it 60 times (our display has a refresh rate of 60Hz)! Let’s try:\n\nfor _ in range(60):\n    fixation.draw()\n    win.flip()\nwin.flip() # we re-flip at the end to clean the window\n\nNow we have shown the fixation for 1 second and then it disappeared. Nice!! However you probably have already figured out that what we have done was unnecessary. If we want to present a static stimulus for 1s we could have just drawn it, flip the window and then wait for 1s. But now you have an idea on how to show animated stimuli or even videos!!! AMAZING!!!.\nNow let’s try to show the fixation for 1s by just waiting.\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n\n\nPlay a sound:\nWe have seen how to show a stimulus let’s now play the sounds that we have imported. This is extremely simple, we can just play() them:\n\nwinning_sound.play()\ncore.wait(2)\nlosing_sound.play()\n\nGreat now we have played our two sounds!!\n\n\n\n\n\n\nWarning\n\n\n\nWhen playing a sound the script will continue and will not wait for the sound to have finished playing. So if you play two sounds one after without waiting the two sounds will play overlapping. That’s why we have used core.wait(2), this tells PsychoPy to wait 2 seconds after starting to play the sound.\n\n\n\n\nCreate a trial:\nNow let’s try to put everything we have learned in one place and present one rewarding and one non-rewarding trial: - we present the fixation for 1s - we present one of the two cues for 3s - we present the reward or the non-reward depending on the cue for 2s.\nIn the end we also close the window.\n\n###### 1st Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the winning cue\ncircle.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nwinning.draw()\nwin.flip()\nwinning_sound.play()\ncore.wait(2)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n###### 2nd Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the non-rewarding cue\nsquare.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nlosing.draw()\nwin.flip()\nlosing_sound.play()\ncore.wait(2)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n\nwin.close()  # let's close the window at the end of the trial\n\n\n\nCreate an entire experiment:\nIn an experiment, we want more than 1 trial. Let’s then create an experiment with 10 trials. We just need to repeat what we have done above multiple times. However, we need to randomize the type of trials, otherwise, it would be too easy to learn. To do so, we will create a list of 0 and 1. where 0 would identify a rewarding trial and 1 would index a non-rewarding trial.\nTo properly utilize this list of 0 and 1, we will need to create other lists of our stimuli. This will make it easier to call the right stimuli depending on the trial. We can do so by:\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, loosing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\nPerfect!! Now we can put all the pieces together and run our experiment.\n\n\n\n\n\n\nNote\n\n\n\nIn this final script, we will change the dimension of the window we will use. Since in most of the experiments, we will want to use the entire screen to our disposal, we will set fullscr = True when defining the window. In addition, we will also change the position of the rewarding and non-rewarding stimulus since now the window is bigger.\n\n\n\n# Import some libraries from PsychoPy\nfrom psychopy import visual\nfrom psychopy import core\nfrom psychopy import sound\n\n# Let's define the path we will work with.\n# in my case the stimuli that we will use are in here but you will have to adapt\n# this variable to where you have downloaded the stimuli.\nPath = r'C:\\\\Users\\\\tomma\\\\surfdrive - Ghilardi, T. (Tommaso)@surfdrive.surf.nl\\\\Documentation\\\\Working\\\\GettingStartedWithPsychopy\\\\'\n\n# create a window\nwin = visual.Window(fullscr = True, units=\"pix\")\n\n# Load images\nfixation = visual.ImageStim(win, image=Path + 'fixation.png', size = (100, 100))\ncircle   = visual.ImageStim(win, image=Path + 'circle.png', size = (100, 100))\nsquare   = visual.ImageStim(win, image=Path + 'square.png', size = (100, 100))\nwinning   = visual.ImageStim(win, image=Path + 'winning.png', size = (100, 100), pos=(600,0))\nloosing  = visual.ImageStim(win, image=Path + 'loosing.png', size = (100, 100), pos=(-600,0))\n\n# Load sound\nwinning_sound = sound.Sound(Path + 'winning.wav')\nlosing_sound = sound.Sound(Path + 'loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, loosing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n    fixation.draw()\n    win.flip()\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    core.wait(3)  # wait for 3 seconds\n\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n    sounds[trial].play()\n    core.wait(2)  # wait for 1 second\n    win.flip()    # we re-flip at the end to clean the window\n\n    ### ISI\n    core.wait(0.3) # add Inter Stimulus Interval to make exp more understandable\n\nwin.close()\n\n\n\nEND\nWe have our basic experiment and if you have followed up to here you should be able to get along with the basic concepts of PsychoPy!! Well done!!!."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Starting with PsychoPy",
    "section": "",
    "text": "PsychoPy is an open source software package written in the Python programming language primarily for use in neuroscience and experimental psychology research. It’s one of our favorite ways to create experiments and we will use it through our tutorials.\nSo, let’s start and install PsychoPy!!!"
  },
  {
    "objectID": "test.html#install-psychopy",
    "href": "test.html#install-psychopy",
    "title": "Starting with PsychoPy",
    "section": "Install PsychoPy",
    "text": "Install PsychoPy\nAs reported on the PsychoPy website, there are multiple ways to install PsychoPy. Our favorite way to install it is using conda (refer to the Getting started with python).\nPsychoPy offers a nice .yml file that will install everything that we need. Download the file from their website and store it somewhere.\nNow open the Anaconda prompt and type:\nconda env create -f  your_dowloaded_file.yml\nThis will take some time and will ask for confirmation but in the end you will have a nice virtual environment containing everything you need to run your experiment on PsychoPy.\nThe virtual environment will be called “psychopy” and you can activate it just by typing:\nconda activate psychopy\nWe are done!! You now should have a nice conda environment called “psychoPy” with PsychoPy in it. To launch the PsychoPy Gui (also referred to as Builder) you can just type psychopy in your Anaconda prompt and PsychoPy will launch. You are definitely free to try the Builder or the Coder that PsychoPy offers, however we usually prefer to write and launch our scripts from Spyder, let’s be honest, if you want to code your experiment the Gui of Spyder is way better!!\nThus we suggest to install Spyder in your newly created “psychopy” environment. You can do that just by simply typing in:\nconda install –c anaconda spyder\nYou can also refer to our guide Installing Spyder ide.\nNow that you should have both PsychoPy and Spyder installed let’s see how to create a simple experiment with them."
  },
  {
    "objectID": "test.html#lets-create-our-first-experiment-using-psychopy.",
    "href": "test.html#lets-create-our-first-experiment-using-psychopy.",
    "title": "Starting with PsychoPy",
    "section": "Let’s create our first experiment using PsychoPy.",
    "text": "Let’s create our first experiment using PsychoPy.\nWe will create a very simple and basic experiment that will be the stepping stone for some of the future tutorials. In the future tutorials we will show you how to extend and make this tutorial in a real experiment.\n\n\n\n\n\n\nStimuli!\n\n\n\nYou can download from here the stimuli that we will use in this example. They are very simple and basic stimuli:\n\na fixation cross\ntwo cues (a circle and a square)\na reward (a cute medal)\na non-reward (a cartoon of an exploding empty cloud)\na sound of winning at an arcade game\na sound of losing at an arcade game\n\n\n\nIn this tutorial we will create an experiment in which, after the fixation cross, one of the two cues is presented. The cues will indicate whether we will receive a reward or not and where this will appear. After the circle is presented as cue the medal will be presented on the right. After the circle the empty cloud will be presented on the left. Thus, if you follow the cued indication you will be able to predict the location of the next stimuli and whether or not it will be rewarding.\n\nPreparation:\nFirst thing first let’s import the relevant libraries and define the path where our stimuli are in. PsychoPy has a lot of different modules that allow us to interface with different type of stimuli and systems. For this tutorial\n\n# Import some libraries from PsychoPy\nfrom psychopy import core\nfrom psychopy import visual  \nfrom psychopy import sound\n\n# Let's define the path we will work with.\n# in my case the stimuli that we will use are in here but you will have to adapt\n# this variable to where you have downloaded the stimuli.\nPath = 'C:\\\\Users\\\\tomma\\\\surfdrive\\\\Documentation\\\\psychopy\\\\'\n\n\n\nStimuli:\nThe next step is to create the window The window is what we will show the stimuli in; it is the canvas in which to draw objects. For now we will create a small window of 800*600 pixels. In this way we will able to see the stimuli and still interact with the rest of our pc interface. In a real experiment we would probably set the window dimension to the entirety of the the display (Fullscreen).\n\n#create a window\nwin = visual.Window([800,600], units=\"pix\")\n\nNow let’s import the stimuli that we will present in this tutorial. We have 5 stimuli:\n\na fixation cross that we will use to catch the attention of our participants\na circle that will be our cue that signal a rewarding trial\na square that will be our cue that signal a non-rewarding trial\na cartoon of a medal that will be our reward\na cartoon of an empty cloud that will be our non-reward\n\nOn top of these visual stimuli we will also import two sounds that will help us signal the type of trials. So:\n\na tada! winning sound\na papapaaa! losing sound\n\n\n\n\n\n\n\nTip\n\n\n\nWhen importing a visual stimulus we need to pass to the importing function in which window it will be displayed. In our case we will pass all of them the “win” window that we just created.\n\n\n\n# Load images\nfixation = visual.ImageStim(win, image=Path + 'fixation.png', size = (100, 100))\ncircle   = visual.ImageStim(win, image=Path + 'circle.png', size = (100, 100))\nsquare   = visual.ImageStim(win, image=Path + 'square.png', size = (100, 100))\nwinning   = visual.ImageStim(win, image=Path + 'winning.png', size = (100, 100), pos=(250,0))\nloosing  = visual.ImageStim(win, image=Path + 'loosing.png', size = (100, 100), pos=(-250,0))\n\n# Load sound\nwinning_sound = sound.Sound(Path + 'winning.wav')\nlosing_sound = sound.Sound(Path + 'loosing.wav')\n\nNote that in this simple experiment we will present the reward always on the right and the non-rewards always on the left that’s why when we import the two rewards we set their pos to (250,0) and (-250,0). The first value indicates the number of pixels on the x-axis and the second the number of pixels on the y-axis.\n\n\nShow a visual stimulus:\nNo we want to show a stimuli in the center of our window. To do so we will have to use the function “draw”. As the name suggests this function draws the stimulus that we want on the window.\nLet’s start with displaying the fixation cross in the center.\n\n# Draw the fixation\nfixation.draw()\n\nDo you see the fixation cross?????? Probably not!! This is because we have drawn the fixation cross but we have not refreshed the window. Psychopy allows you to draw as many stimuli as you want on a window but the changes are only shown when you “refresh” the window. To do so we need to use the “flip” function.\n\nwin.flip()\n\nPerfect!!!! The fixation cross is there. Before each flip we need to draw our objects. Otherwise we will only see the basic window with nothing in it. Let’s try!!! flip the window now.\n\n# Flipping the window (refreshing)\nwin.flip()\n\nThe fixation is gone again! Exactly as predicted. Flipping the window allows us to draw and show something new each frame. This means that the speed limit of our presentation is the actual frame rate of our display. If we have a 60Hz display we can present an image 60 times in a second.\nSo if we want to present our fixation for an entire second we would have to draw and flip it 60 times (our display has a refresh rate of 60Hz)! Let’s try:\n\nfor _ in range(60):\n    fixation.draw()\n    win.flip()\nwin.flip() # we re-flip at the end to clean the window\n\nNow we have shown the fixation for 1 second and then it disappeared. Nice!! However you probably have already figured out that what we have done was unnecessary. If we want to present a static stimulus for 1s we could have just drawn it, flip the window and then wait for 1s. But now you have an idea on how to show animated stimuli or even videos!!! AMAZING!!!.\nNow let’s try to show the fixation for 1s by just waiting.\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n\n\nPlay a sound:\nWe have seen how to show a stimulus let’s now play the sounds that we have imported. This is extremely simple, we can just play() them:\n\nwinning_sound.play()\ncore.wait(2)\nlosing_sound.play()\n\nGreat now we have played our two sounds!!\n\n\n\n\n\n\nWarning\n\n\n\nWhen playing a sound the script will continue and will not wait for the sound to have finished playing. So if you play two sounds one after without waiting the two sounds will play overlapping. That’s why we have used core.wait(2), this tells PsychoPy to wait 2 seconds after starting to play the sound.\n\n\n\n\nCreate a trial:\nNow let’s try to put everything we have learned in one place and present one rewarding and one non-rewarding trial: - we present the fixation for 1s - we present one of the two cues for 3s - we present the reward or the non-reward depending on the cue for 2s.\nIn the end we also close the window.\n\n###### 1st Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the winning cue\ncircle.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nwinning.draw()\nwin.flip()\nwinning_sound.play()\ncore.wait(2)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n###### 2nd Trial ######\n\n### Present the fixation\nwin.flip() # we flip to clean the window\n\nfixation.draw()\nwin.flip()\ncore.wait(1)  # wait for 1 second\n\n### Present the non-rewarding cue\nsquare.draw()\nwin.flip()\ncore.wait(3)  # wait for 3 seconds\n\n### Present the reward \nlosing.draw()\nwin.flip()\nlosing_sound.play()\ncore.wait(2)  # wait for 1 second\nwin.flip()    # we re-flip at the end to clean the window\n\n\nwin.close()  # let's close the window at the end of the trial\n\n\n\nCreate an entire experiment:\nIn an experiment, we want more than 1 trial. Let’s then create an experiment with 10 trials. We just need to repeat what we have done above multiple times. However, we need to randomize the type of trials, otherwise, it would be too easy to learn. To do so, we will create a list of 0 and 1. where 0 would identify a rewarding trial and 1 would index a non-rewarding trial.\nTo properly utilize this list of 0 and 1, we will need to create other lists of our stimuli. This will make it easier to call the right stimuli depending on the trial. We can do so by:\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, loosing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\nPerfect!! Now we can put all the pieces together and run our experiment.\n\n\n\n\n\n\nNote\n\n\n\nIn this final script, we will change the dimension of the window we will use. Since in most of the experiments, we will want to use the entire screen to our disposal, we will set fullscr = True when defining the window. In addition, we will also change the position of the rewarding and non-rewarding stimulus since now the window is bigger.\n\n\n\n# Import some libraries from PsychoPy\nfrom psychopy import visual\nfrom psychopy import core\nfrom psychopy import sound\n\n# Let's define the path we will work with.\n# in my case the stimuli that we will use are in here but you will have to adapt\n# this variable to where you have downloaded the stimuli.\nPath = r'C:\\\\Users\\\\tomma\\\\surfdrive - Ghilardi, T. (Tommaso)@surfdrive.surf.nl\\\\Documentation\\\\Working\\\\GettingStartedWithPsychopy\\\\'\n\n# create a window\nwin = visual.Window(fullscr = True, units=\"pix\")\n\n# Load images\nfixation = visual.ImageStim(win, image=Path + 'fixation.png', size = (100, 100))\ncircle   = visual.ImageStim(win, image=Path + 'circle.png', size = (100, 100))\nsquare   = visual.ImageStim(win, image=Path + 'square.png', size = (100, 100))\nwinning   = visual.ImageStim(win, image=Path + 'winning.png', size = (100, 100), pos=(600,0))\nloosing  = visual.ImageStim(win, image=Path + 'loosing.png', size = (100, 100), pos=(-600,0))\n\n# Load sound\nwinning_sound = sound.Sound(Path + 'winning.wav')\nlosing_sound = sound.Sound(Path + 'loosing.wav')\n\n# List of stimuli\ncues = [circle, square] # put both cues in a list\nrewards = [winning, loosing] # put both rewards in a list\nsounds = [winning_sound,losing_sound] # put both sounds in a list\n\n\n# Create list of trials in which 0 means winning and 1 means losing\nTrials = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1 ]\n\n\nfor trial in Trials:\n\n    ### Present the fixation\n    win.flip() # we flip to clean the window\n\n    fixation.draw()\n    win.flip()\n    core.wait(1)  # wait for 1 second\n\n\n    ### Present the cue\n    cues[trial].draw()\n    win.flip()\n    core.wait(3)  # wait for 3 seconds\n\n\n    ### Present the reward\n    rewards[trial].draw()\n    win.flip()\n    sounds[trial].play()\n    core.wait(2)  # wait for 1 second\n    win.flip()    # we re-flip at the end to clean the window\n\n    ### ISI\n    core.wait(0.3) # add Inter Stimulus Interval to make exp more understandable\n\nwin.close()\n\n\n\nEND\nWe have our basic experiment and if you have followed up to here you should be able to get along with the basic concepts of PsychoPy!! Well done!!!."
  },
  {
    "objectID": "test.html#Psychopy",
    "href": "test.html#Psychopy",
    "title": "test",
    "section": "1 Install PsychoPy",
    "text": "1 Install PsychoPy\nAs reported on the PsychoPy website, there are multiple ways to install PsychoPy. Our favorite way to install it is using conda (refer to the Getting started with python).\nPsychoPy offers a nice .yml file that will install everything that we need. Download the file from their website and store it somewhere.\nNow open the Anaconda prompt and type:\nconda env create -f  your_dowloaded_file.yml\nThis will take some time and will ask for confirmation but in the end you will have a nice virtual environment containing everything you need to run your experiment on PsychoPy.\nThe virtual environment will be called “psychopy” and you can activate it just by typing:\nconda activate psychopy\nWe are done!! You now should have a nice conda environment called “psychoPy” with PsychoPy in it. To launch the PsychoPy Gui (also referred to as Builder) you can just type psychopy in your Anaconda prompt and PsychoPy will launch. You are definitely free to try the Builder or the Coder that PsychoPy offers, however we usually prefer to write and launch our scripts from Spyder, let’s be honest, if you want to code your experiment the Gui of Spyder is way better!!\nThus we suggest to install Spyder in your newly created “psychopy” environment. You can do that just by simply typing in:\nconda install –c anaconda spyder\nYou can also refer to our guide\naaaaaaaaaaa\nGetting started with python.\nNow that you should have both PsychoPy and Spyder installed let’s see how to create a simple experiment with them."
  },
  {
    "objectID": "test.html#psy",
    "href": "test.html#psy",
    "title": "test",
    "section": "1 Install PsychoPy",
    "text": "1 Install PsychoPy\nAs reported on the PsychoPy website, there are multiple ways to install PsychoPy. Our favorite way to install it is using conda (refer to the Getting started with python).\nPsychoPy offers a nice .yml file that will install everything that we need. Download the file from their website and store it somewhere.\nNow open the Anaconda prompt and type:\nconda env create -f  your_dowloaded_file.yml\nThis will take some time and will ask for confirmation but in the end you will have a nice virtual environment containing everything you need to run your experiment on PsychoPy.\nThe virtual environment will be called “psychopy” and you can activate it just by typing:\nconda activate psychopy\nWe are done!! You now should have a nice conda environment called “psychoPy” with PsychoPy in it. To launch the PsychoPy Gui (also referred to as Builder) you can just type psychopy in your Anaconda prompt and PsychoPy will launch. You are definitely free to try the Builder or the Coder that PsychoPy offers, however we usually prefer to write and launch our scripts from Spyder, let’s be honest, if you want to code your experiment the Gui of Spyder is way better!!\nThus we suggest to install Spyder in your newly created “psychopy” environment. You can do that just by simply typing in:\nconda install –c anaconda spyder\nYou can also refer to our guide\naaaaaaaaaaa\nGetting started with python.\nNow that you should have both PsychoPy and Spyder installed let’s see how to create a simple experiment with them."
  },
  {
    "objectID": "test.html#sec-introduction",
    "href": "test.html#sec-introduction",
    "title": "test",
    "section": "Introduction",
    "text": "Introduction\nSee Section 3 for additional context.\nsee Section 1\nlink normal\nefdf Section 1"
  },
  {
    "objectID": "test.html#sec-psyco",
    "href": "test.html#sec-psyco",
    "title": "test",
    "section": "Install PsychoPy",
    "text": "Install PsychoPy\nAs reported on the PsychoPy website, there are multiple ways to install PsychoPy. Our favorite way to install it is using conda (refer to the Getting started with python).\nPsychoPy offers a nice .yml file that will install everything that we need. Download the file from their website and store it somewhere.\nNow open the Anaconda prompt and type:\nconda env create -f  your_dowloaded_file.yml\nThis will take some time and will ask for confirmation but in the end you will have a nice virtual environment containing everything you need to run your experiment on PsychoPy.\nThe virtual environment will be called “psychopy” and you can activate it just by typing:\nconda activate psychopy\nWe are done!! You now should have a nice conda environment called “psychoPy” with PsychoPy in it. To launch the PsychoPy Gui (also referred to as Builder) you can just type psychopy in your Anaconda prompt and PsychoPy will launch. You are definitely free to try the Builder or the Coder that PsychoPy offers, however we usually prefer to write and launch our scripts from Spyder, let’s be honest, if you want to code your experiment the Gui of Spyder is way better!!\nThus we suggest to install Spyder in your newly created “psychopy” environment. You can do that just by simply typing in:\nconda install –c anaconda spyder\nYou can also refer to our guide Installing Spyder ide.\nNow that you should have both PsychoPy and Spyder installed let’s see how to create a simple experiment with them."
  },
  {
    "objectID": "Intro_eyetracking.html",
    "href": "Intro_eyetracking.html",
    "title": "Introduction to eye-tracking",
    "section": "",
    "text": "Eye-tracking is a great tool to study cognition. It is especially suitable for developmental studies, as infants and young children might have advanced cognitive abilities, but little chances to show them (they cannot talk!).\nThis tutorial will teach you all you need to navigate the huge and often confusing eye-tracking world. First, we will introduce how an experiment can (and should) be built, explaining how to easily record eye-tracking data from Python. Then, it will focus on how to analyse the data, reducing the seemingly overwhelming amount of rows and columns in a few variables of interest (such as saccadic latency, looking time or pupil dilation)."
  },
  {
    "objectID": "Intro_eyetracking.html#what-do-you-want-to-measure",
    "href": "Intro_eyetracking.html#what-do-you-want-to-measure",
    "title": "Introduction to eye-tracking",
    "section": "What do you want to measure?",
    "text": "What do you want to measure?\nIt is much easier to start an eye-tracking project if you have a clear idea of what you want to measure. Classic paradigms on infant research rely on looking time (How long are infants attending a given stimulus?) and are often called Violation-of-Expectation tasks. They familiarize infants with a given stimulus or situation (e.g. a cat) and, after a given number of presentations (e.g., 8), they present a different stimulus (e.g., a dog). Do infants look longer at the dog, compared to the cat? If so, they were able to spot that something changed.\n\n\n\n\n\n\nImportant\n\n\n\nBeware! This does not mean that they can distinguish cats and dogs, but more simply that they could spot any difference between the two images. A careful control of the stimuli should be in place if we want to make strong conclusions from looking time.\n\n\nAnother very popular eye-tracking measure is saccadic latency. It measures how quickly infants can direct their gaze onto a stimulus, once it is presented on screen. This is a great measure of learning because infants will be faster at looking at a stimulus if they expect it to appear in a given position on the screen. They can even be so fast that they anticipate the correct location of the stimulus, even before the stimulus appears! This is called an anticipatory look.\n\n\n\n\n\n\nImportant\n\n\n\nBeware! Saccadic latencies are not a perfect measure of learning. Infants might be faster at looking at something just because they are more interested (pick interesting stimuli to keep them engaged!), and they might become slower due to boredom or fatigue (introduce variation in the stimuli, so that they become less boring over time!).\n\n\nThe fanciest eye-tracking measure is pupil dilation. It allows us to measure arousal (How interested is the infant in the stimulus?), cognitive effort (How difficult is the task?), and - my special favourite - uncertainty (Does the infant know what will happen next?). However, its fame comes at a price: It is not only the fanciest, but also the most persnickety…\n\n\n\n\n\n\nImportant\n\n\n\nBeware! Stimuli should be carefully designed, controlling their luminance (not too high, and ALWAYS the same) to avoid that task-unrelated variations in light will affect your measurements; They have to be presented in the center of the screen, as pupil dilation measures are inaccurate elsewhere; Pupil dilation is very slow, so the stimulus presentation also has to be slow; A fixation cross has to precede the moment in which pupil dilation is measured so that its signal returns to baseline before you observe the change you care about. But if you feel confident about your paradigm, go for it!!\n\n\nHere is a visual summary of what you can measure:"
  },
  {
    "objectID": "I2MC_tutorial.html",
    "href": "I2MC_tutorial.html",
    "title": "Using I2MC for robust fixation extraction",
    "section": "",
    "text": "When it comes to eye-tracking data, one of the most important things we want to figure out is fixations. Fixations are specific points in an eye-tracking dataset where a person’s gaze remains relatively still and focused on a particular area or object for a period of time. These moments represent critical instances when a person’s visual attention is focused on a particular point of interest.\nTypically, eye-tracking programs come with their own fixation detection algorithms that give us a rough idea of what the person was looking at. But here’s the problem: these tools aren’t always very good when it comes to data from infants and children. Why? Because infants and children can be all over the place! They move their heads, put their hands (or even feet) in front of their faces, close their eyes, or just look away. All of this makes the data a big mess that’s hard to make sense of with regular fixation detection algorithms. Because the data is so messy, it is difficult to tell which data points are part of the same fixation or different fixations.\nBut don’t worry! We’ve got a solution: I2MC.\nI2MC stands for “Identification by Two-Means Clustering”, and it was designed specifically for this kind of problem. It’s designed to deal with all kinds of noise, and even periods of data loss. In this tutorial, we’ll show you how to use I2MC to find fixations. We won’t get into the nerdy stuff about how it works - this is all about the practical side. If you’re curious about the science, you can read the article.\nNow that we’ve introduced I2MC, let’s get our hands dirty and see how to use it!"
  },
  {
    "objectID": "I2MC_tutorial.html#import-data",
    "href": "I2MC_tutorial.html#import-data",
    "title": "Using I2MC for robust fixation extraction",
    "section": "Import data",
    "text": "Import data\nNow we will write a simple function to import our data. This step unfortunately will have to be adapted depending on the system you used to collect the data and the data structure you will have in the end. For this tutorial, you can use your data-set (probably you will have to adapt the importing function) or use our data that you can download from here LINK.\nLet’s create step by step our function to import the data\n\n# Load data\n    raw_df = pd.read_csv(PATH_TO_DATA, delimiter=',')\n\nAfter reading the data we will create a new data-frame that we will fill with the information needed from our raw_df. this is the point that would change depending on you eye-tracked and data format. you will probably have to change the columns names to be sure to have the 5 relevant ones.\n\n# Create empty dataframe\ndf = pd.DataFrame()\n    \n# Extract required data\ndf['time'] = raw_df['Time']\ndf['L_X'] = raw_df['LeftX']\ndf['L_Y'] = raw_df['LeftY']\ndf['R_X'] = raw_df['RightX']\ndf['R_Y'] = raw_df['RightY']\n\nAfter selecting the relevant data we will perform some very basic processing. - Sometimes there could be weird peaks where one sample is (very) far outside the monitor. Here, we will count as missing any data that is more than one monitor distance outside the monitor. Tobii gives us the validity index of the measured eye, here when the validity is too low (&gt;1) we will consider the sample as missing\n\n# Sometimes we have weird peaks where one sample is (very) far outside the monitor. Here, count as missing any data that is more than one monitor distance outside the monitor.\n\n# Left eye\nlMiss1 = (df['L_X'] &lt; -res[0]) | (df['L_X']&gt;2*res[0])\nlMiss2 = (df['L_Y'] &lt; -res[1]) | (df['L_Y']&gt;2*res[1])\nlMiss  = lMiss1 | lMiss2 | (raw_df['ValidityLeft'] &gt; 1)\ndf.loc[lMiss,'L_X'] = np.NAN\ndf.loc[lMiss,'L_Y'] = np.NAN\n\n# Right eye\nrMiss1 = (df['R_X'] &lt; -res[0]) | (df['R_X']&gt;2*res[0])\nrMiss2 = (df['R_Y'] &lt; -res[1]) | (df['R_Y']&gt;2*res[1])\nrMiss  = rMiss1 | rMiss2 | (raw_df['ValidityRight'] &gt; 1)\ndf.loc[rMiss,'R_X'] = np.NAN\ndf.loc[rMiss,'R_Y'] = np.NAN\n\nPerfect!!!\n\nEverything into a function\nWe have read the data, extracted the relevant information and done some extremely basic processing rejecting data that had to be considered non valid. Now we will wrap this code in a function to make it easier to use with I2MC:\n\n# ===================================================================\n# Import data from Tobii TX300\n# ===================================================================\n\ndef tobii_TX300(fname, res=[1920,1080]):\n    '''\n    Imports data from Tobii TX300\n    \n    Parameters\n    ----------\n    fname : string\n        The file (filepath)\n    res : tuple\n        The (X,Y) resolution of the screen\n    \n    Returns\n    -------\n    df : pandas.DataFrame\n         Gaze data, with columns:\n         t : The sample times from the dataset\n         L_X : X positions from the left eye\n         L_Y : Y positions from the left eye\n         R_X : X positions from the right eye\n         R_Y : Y positions from the right eye\n    '''\n\n    # Load all data\n    raw_df = pd.read_csv(fname, delimiter=',')\n    df = pd.DataFrame()\n    \n    # Extract required data\n    df['time'] = raw_df['Time']\n    df['L_X'] = raw_df['LeftX']\n    df['L_Y'] = raw_df['LeftY']\n    df['R_X'] = raw_df['RightX']\n    df['R_Y'] = raw_df['RightY']\n    \n    ###\n    # Sometimes we have weird peaks where one sample is (very) far outside the\n    # monitor. Here, count as missing any data that is more than one monitor\n    # distance outside the monitor.\n    \n    # Left eye\n    lMiss1 = (df['L_X'] &lt; -res[0]) | (df['L_X']&gt;2*res[0])\n    lMiss2 = (df['L_Y'] &lt; -res[1]) | (df['L_Y']&gt;2*res[1])\n    lMiss  = lMiss1 | lMiss2 | (raw_df['ValidityLeft'] &gt; 1)\n    df.loc[lMiss,'L_X'] = np.NAN\n    df.loc[lMiss,'L_Y'] = np.NAN\n    \n    # Right eye\n    rMiss1 = (df['R_X'] &lt; -res[0]) | (df['R_X']&gt;2*res[0])\n    rMiss2 = (df['R_Y'] &lt; -res[1]) | (df['R_Y']&gt;2*res[1])\n    rMiss  = rMiss1 | rMiss2 | (raw_df['ValidityRight'] &gt; 1)\n    df.loc[rMiss,'R_X'] = np.NAN\n    df.loc[rMiss,'R_Y'] = np.NAN\n    \n    return(df)\n\n\n\nFind our data\nNice!! we have our import function that we will use to read our data. Now, let’s find our data! To do this, we will use the glob library, which is a handy tool for finding files in Python. In the code above, we are telling Python to look for files with a .csv extension in a specific folder on our computer. Let’s import glob and then let’s find the files:\n\nimport glob\n    data_files = glob.glob('C:\\\\Users\\\\tomma\\\\surfdrive - Ghilardi, T. (Tommaso)@surfdrive.surf.nl\\\\Documentation\\Working\\\\Eyetracking\\\\data\\\\**\\\\*.csv', recursive = True)\n\n\n/home/geeks/Desktop/gfg/: This is the path where we want to start our search.\n\\*\\*: This special symbol tells Python to search in all the subfolders (folders within folders) under our starting path.\n/*.csv: We’re asking Python to look for files with names ending in “.csv”.\nrecursive=True: This option means that Python should search for files not just in the immediate subfolders but in all the subfolders within subfolders, and so on."
  },
  {
    "objectID": "CreateAnEyetrackingExperiment.html",
    "href": "CreateAnEyetrackingExperiment.html",
    "title": "Create an eye-tracking experiment",
    "section": "",
    "text": "This page will show you how to collect eye-tracking data in a simple psychopy paradigm. We will use the same paradigm that we built together in the Getting started with Psychopy tutorial. If you have not done that tutorial yet, please go through it first."
  },
  {
    "objectID": "CreateAnEyetrackingExperiment.html#how-to-connect-to-the-eye-tracker",
    "href": "CreateAnEyetrackingExperiment.html#how-to-connect-to-the-eye-tracker",
    "title": "Create an eye-tracking experiment",
    "section": "How to connect to the eye-tracker",
    "text": "How to connect to the eye-tracker\nWe have different options to use our Tobii eye tracker. One option is to use the software that Tobii provides. However, this software is very expensive and sometimes not flexible enough for all the studies. We prefer to use the SDK that Tobii provides.\nAn SDK is a collection of tools and programs for developing applications for a specific platform or device. We will use the Python Tobii SDK that lets us easily find and get data from our Tobii eye tracker.\nTo install the Python Tobii SDK, we can run this command:\npip install tobii_research\nGreat! We have installed the Tobii SDK. However, using the SDK is not very easy. But don’t worry! We have a solution for you. We can use Psychopy_tobii_infant, a wrapper around the Tobii SDK that is specially designed for running infant-friendly studies. This code collection allows us to use the Tobii SDK with the Psychopy interface.\nTo use Psychopy_tobii_infant, you need to go to this GitHub page: https://github.com/yh-luo/psychopy_tobii_infant. On this page, you can click on &lt;&gt;Code and then on Download ZIP as shown below:\n\n\n\n\n\nPerfect!! Now that we have downloaded the code as a zip file we need to:\n\nextract the file\nidentify the folder ‘psychopy_tobii_infant’\ncopy this folder in the same location of your eye-tracking experiment script\n\nYou should end up like with something like this:\n\nNow we are all set and ready to go !!!!!!!!!!"
  },
  {
    "objectID": "CreateAnEyetrackingExperiment.html#short-recap-of-the-paradigm",
    "href": "CreateAnEyetrackingExperiment.html#short-recap-of-the-paradigm",
    "title": "Create an eye-tracking experiment",
    "section": "Short recap of the paradigm",
    "text": "Short recap of the paradigm"
  }
]