clusterEvalQ(cl, {
library(tidyverse)
library(zoo)
})
clusterExport(cl, c("calculate_running_correlation", "average_fisher_z_transform"))
#--- Run synchrony adn ttest ---#
Children = db %>%
filter(Group == 'Children') %>%
arrange(Stimulus) %>%
pivot_wider(names_from = Subject, values_from = Pupil,
id_cols = c( "Seconds", "Stimulus"))
TTest_res = list()
start.time <- Sys.time()
for (x in 1:10000){
print(x)
Sub = sample(3:20, 9)
ByStimulus = Children %>% select(all_of(c(1,2,Sub))) %>% split(.$Stimulus)
ByStimulus = lapply(ByStimulus, function(df) df %>% select(-1:-2))
Res = parLapply(cl, ByStimulus , calculate_running_correlation, window_sizeP = window_sizeP)
Chil =  unlist(Res,use.names=FALSE)
TTest_res[[x]] = parameters(t.test(Chil, Adults$PupilSynch))
}
stopCluster(cl)
time.taken = round(Sys.time() - start.time,2)
time.taken
#--- Bind and save ---#
TTest_res = bind_rows(TTest_res)
write.csv(TTest_res, paste(output_dir , 'TTest.csv', sep=''), row.names = F)
TTest_res
install.packages("meta")
TTest_res$CI
library(meta)
pcombine(TTest_res$p)
estimate_density(TTest_res$p)
point_estimate(TTest_res$p, centrality = "all", dispersion = TRUE)
# Calculate the combined p-value
combined_p_value <- point_estimate(TTest_res$p, centrality = "all", dispersion = TRUE)
View(combined_p_value)
# Calculate the combined p-value
combined_p_value <- point_estimate(TTest_res$p, centrality = "all", dispersion = TRUE)
# Calculate the combined t-score
combined_t_score <- point_estimate(TTest_res$t, centrality = "all", dispersion = TRUE)
# Print the results
print(combined_p_value)
print(combined_t_score)
point_estimate(TTest_res$p)
estimate_density(TTest_res$p)
a= estimate_density(TTest_res$p)
plot(a)
ggplot(TTest_res, aes(x = p))+
geom_density()
ggplot(TTest_res, aes(y = p))+
geom_density()
TTest_res$p
View(TTest_res)
mean(TTest_res$p)
ggplot(TTest_res, aes(x = p))+
geom_density()
library(plotly)
ggplotly(ggplot(TTest_res, aes(x = p))+
geom_density())
mean(TTest_res$p)
median(TTest_res$p)
Chil
View(Res)
Children %>% select(all_of(c(1,2,Sub)))
sample(3:20, 9)
sample(3:20, 9)
sample(3:20, 9)
sample(3:20, 9)
sample(3:20, 9)
sample(3:20, 9)
sample(3:20, 9)
sample(3:20, 9)
sample(3:20, 9)
sample(3:20, 9)
sample(3:20, 9)
sample(3:20, 9)
ggplotly(ggplot(TTest_res, aes(x = t))+
geom_density())
View(TTest_res)
ggplotly(ggplot())+
geom_density(aes(x = Chil)))
ggplotly(ggplot()+
geom_density(aes(x = Chil)))
ggplotly(ggplot()+
geom_density(aes(x = Adults$PupilSynch)))
parameters(wilcox.test(Chil, Adults$PupilSynch))
library(tidyverse)
library(zoo)
library(easystats)
library(parallel)
# Functions ------------------------------------------------------------
#### Running correlation
calculate_running_correlation <- function(Matrix, window_sizeP) {
# Define a function to calculate correlation for each window
calc_correlation <- function(window_rows) {
if (sum(is.na(window_rows)) > 0.75*prod(dim(window_rows))){
return(NA)
} else {
# Calculate the correlation matrix for the rows in the current window
correlation_matrix <- cor(window_rows, use = "pairwise.complete.obs")
correlation = average_fisher_z_transform(correlation_matrix)
return(correlation)
}
}
# Use rollapply to apply the function to each rolling window in the Matrix
correlation_matrices <- rollapply(Matrix, width = window_sizeP, FUN = calc_correlation,
by.column = FALSE, align = "center", partial = TRUE)
# Return the list of correlation matrices
return(correlation_matrices)
}
#### Extract Average normalized person correlation
average_fisher_z_transform <- function(corr_matrix, threshold = 0.75) {
# Check if more than a given threshold of the columns have a NA
if(sum(colSums(is.na(corr_matrix)) >= ncol(corr_matrix)-1) > threshold*ncol(corr_matrix)) {
return(NA)
}
# Take the lower triangle, excluding the diagonal
lower_tri <- corr_matrix[lower.tri(corr_matrix)]
# Apply Fisher's z transformation using atanh
z_scores <- atanh(lower_tri - 1e-10)
# Calculate the mean of z_scores
average_z_score <- mean(z_scores, na.rm = TRUE)
# Transform back to the correlation scale using tanh
average_correlation <- tanh(average_z_score)
return(average_correlation)
}
# Read Data and settings ------------------------------------------------------------
Hz = 20
Tresh =  0.75
window_sizeP = Hz*2 # Window to calculate the rolling correlation over
db = read.csv('C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\PupilDilationSync_2023\\Data\\ProcessedData\\FinalData.csv', sep = ',')
output_dir = 'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\PupilDilationSync_2023\\Data\\ProcessedData\\'
# Extract synchrony in Pupil for adults ------------------------------------------------------------
#### Adults
Adults = db %>%
filter(Group == 'Adults') %>%
arrange(Stimulus) %>%
pivot_wider(names_from = Subject, values_from = Pupil,
id_cols = c( "Seconds", "Stimulus"))
ByStimulus = Adults %>%
split(.$Stimulus)
ByStimulus = lapply(ByStimulus, function(df) df %>% select(-1:-2))
Res = lapply(ByStimulus , calculate_running_correlation, window_sizeP = window_sizeP)
Adults$PupilSynch = unlist(Res,use.names=FALSE)
Adults = Adults %>%
mutate(PupilSynch = unlist(Res,use.names=FALSE),
Group = 'Children')%>%
select(Seconds, Group, Stimulus, PupilSynch )
# Bootstrap ttest ------------------------------------------------------------
#--- Initialize parallelizion ---#
# Prepare cluster
cl = makeCluster(8)
# Load libraries on each node
clusterEvalQ(cl, {
library(tidyverse)
library(zoo)
})
clusterExport(cl, c("calculate_running_correlation", "average_fisher_z_transform"))
#--- Run synchrony adn ttest ---#
Children = db %>%
filter(Group == 'Children') %>%
arrange(Stimulus) %>%
pivot_wider(names_from = Subject, values_from = Pupil,
id_cols = c( "Seconds", "Stimulus"))
TTest_res = list()
start.time <- Sys.time()
for (x in 1:10000){
print(x)
Sub = sample(3:20, 9)
ByStimulus = Children %>% select(all_of(c(1,2,Sub))) %>% split(.$Stimulus)
ByStimulus = lapply(ByStimulus, function(df) df %>% select(-1:-2))
Res = parLapply(cl, ByStimulus , calculate_running_correlation, window_sizeP = window_sizeP)
Chil =  unlist(Res,use.names=FALSE)
TTest_res[[x]] = parameters(wilcox.test(Chil, Adults$PupilSynch))
}
stopCluster(cl)
library(tidyverse)
library(zoo)
library(easystats)
library(parallel)
# Functions ------------------------------------------------------------
#### Running correlation
calculate_running_correlation <- function(Matrix, window_sizeP) {
# Define a function to calculate correlation for each window
calc_correlation <- function(window_rows) {
if (sum(is.na(window_rows)) > 0.75*prod(dim(window_rows))){
return(NA)
} else {
# Calculate the correlation matrix for the rows in the current window
correlation_matrix <- cor(window_rows, use = "pairwise.complete.obs")
correlation = average_fisher_z_transform(correlation_matrix)
return(correlation)
}
}
# Use rollapply to apply the function to each rolling window in the Matrix
correlation_matrices <- rollapply(Matrix, width = window_sizeP, FUN = calc_correlation,
by.column = FALSE, align = "center", partial = TRUE)
# Return the list of correlation matrices
return(correlation_matrices)
}
#### Extract Average normalized person correlation
average_fisher_z_transform <- function(corr_matrix, threshold = 0.75) {
# Check if more than a given threshold of the columns have a NA
if(sum(colSums(is.na(corr_matrix)) >= ncol(corr_matrix)-1) > threshold*ncol(corr_matrix)) {
return(NA)
}
# Take the lower triangle, excluding the diagonal
lower_tri <- corr_matrix[lower.tri(corr_matrix)]
# Apply Fisher's z transformation using atanh
z_scores <- atanh(lower_tri - 1e-10)
# Calculate the mean of z_scores
average_z_score <- mean(z_scores, na.rm = TRUE)
# Transform back to the correlation scale using tanh
average_correlation <- tanh(average_z_score)
return(average_correlation)
}
# Read Data and settings ------------------------------------------------------------
Hz = 20
Tresh =  0.75
window_sizeP = Hz*2 # Window to calculate the rolling correlation over
db = read.csv('C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\PupilDilationSync_2023\\Data\\ProcessedData\\FinalData.csv', sep = ',')
output_dir = 'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\PupilDilationSync_2023\\Data\\ProcessedData\\'
# Extract synchrony in Pupil for adults ------------------------------------------------------------
#### Adults
Adults = db %>%
filter(Group == 'Adults') %>%
arrange(Stimulus) %>%
pivot_wider(names_from = Subject, values_from = Pupil,
id_cols = c( "Seconds", "Stimulus"))
ByStimulus = Adults %>%
split(.$Stimulus)
ByStimulus = lapply(ByStimulus, function(df) df %>% select(-1:-2))
Res = lapply(ByStimulus , calculate_running_correlation, window_sizeP = window_sizeP)
Adults$PupilSynch = unlist(Res,use.names=FALSE)
Adults = Adults %>%
mutate(PupilSynch = unlist(Res,use.names=FALSE),
Group = 'Children')%>%
select(Seconds, Group, Stimulus, PupilSynch )
# Bootstrap ttest ------------------------------------------------------------
#--- Initialize parallelizion ---#
# Prepare cluster
cl = makeCluster(8)
# Load libraries on each node
clusterEvalQ(cl, {
library(tidyverse)
library(zoo)
})
clusterExport(cl, c("calculate_running_correlation", "average_fisher_z_transform"))
#--- Run synchrony adn ttest ---#
Children = db %>%
filter(Group == 'Children') %>%
arrange(Stimulus) %>%
pivot_wider(names_from = Subject, values_from = Pupil,
id_cols = c( "Seconds", "Stimulus"))
TTest_res = list()
start.time <- Sys.time()
for (x in 1:10000){
print(x)
Sub = sample(3:20, 9)
ByStimulus = Children %>% select(all_of(c(1,2,Sub))) %>% split(.$Stimulus)
ByStimulus = lapply(ByStimulus, function(df) df %>% select(-1:-2))
Res = parLapply(cl, ByStimulus , calculate_running_correlation, window_sizeP = window_sizeP)
Chil =  unlist(Res,use.names=FALSE)
TTest_res[[x]] = parameters(t.test(mean(Chil), mean(Adults$PupilSynch)))
}
Adults
#### Adults
Adults = db %>%
filter(Group == 'Adults') %>%
arrange(Stimulus) %>%
pivot_wider(names_from = Subject, values_from = Pupil,
id_cols = c( "Seconds", "Stimulus", "Video"))
ByStimulus = Adults %>%
split(.$Stimulus)
ByStimulus = lapply(ByStimulus, function(df) df %>% select(-1:-3))
Res = lapply(ByStimulus , calculate_running_correlation, window_sizeP = window_sizeP)
Adults$PupilSynch = unlist(Res,use.names=FALSE)
Adults = Adults %>%
mutate(PupilSynch = unlist(Res,use.names=FALSE),
Group = 'Adults')%>%
select(Seconds, Group,Video,Stimulus, PupilSynch ) %>%
group_by(Stimulus)%>%
summarise(PupilSynch = mean(PupilSynch))
Adults
library(tidyverse)
library(zoo)
library(easystats)
library(parallel)
# Functions ------------------------------------------------------------
#### Running correlation
calculate_running_correlation <- function(Matrix, window_sizeP) {
# Define a function to calculate correlation for each window
calc_correlation <- function(window_rows) {
if (sum(is.na(window_rows)) > 0.75*prod(dim(window_rows))){
return(NA)
} else {
# Calculate the correlation matrix for the rows in the current window
correlation_matrix <- cor(window_rows, use = "pairwise.complete.obs")
correlation = average_fisher_z_transform(correlation_matrix)
return(correlation)
}
}
# Use rollapply to apply the function to each rolling window in the Matrix
correlation_matrices <- rollapply(Matrix, width = window_sizeP, FUN = calc_correlation,
by.column = FALSE, align = "center", partial = TRUE)
# Return the list of correlation matrices
return(correlation_matrices)
}
#### Extract Average normalized person correlation
average_fisher_z_transform <- function(corr_matrix, threshold = 0.75) {
# Check if more than a given threshold of the columns have a NA
if(sum(colSums(is.na(corr_matrix)) >= ncol(corr_matrix)-1) > threshold*ncol(corr_matrix)) {
return(NA)
}
# Take the lower triangle, excluding the diagonal
lower_tri <- corr_matrix[lower.tri(corr_matrix)]
# Apply Fisher's z transformation using atanh
z_scores <- atanh(lower_tri - 1e-10)
# Calculate the mean of z_scores
average_z_score <- mean(z_scores, na.rm = TRUE)
# Transform back to the correlation scale using tanh
average_correlation <- tanh(average_z_score)
return(average_correlation)
}
# Read Data and settings ------------------------------------------------------------
Hz = 20
Tresh =  0.75
window_sizeP = Hz*2 # Window to calculate the rolling correlation over
db = read.csv('C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\PupilDilationSync_2023\\Data\\ProcessedData\\FinalData.csv', sep = ',')
output_dir = 'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\PupilDilationSync_2023\\Data\\ProcessedData\\'
# Extract synchrony in Pupil for adults ------------------------------------------------------------
#### Adults
Adults = db %>%
filter(Group == 'Adults') %>%
arrange(Stimulus) %>%
pivot_wider(names_from = Subject, values_from = Pupil,
id_cols = c( "Seconds", "Stimulus"))
ByStimulus = Adults %>%
split(.$Stimulus)
ByStimulus = lapply(ByStimulus, function(df) df %>% select(-1:-2))
Res = lapply(ByStimulus , calculate_running_correlation, window_sizeP = window_sizeP)
Adults$PupilSynch = unlist(Res,use.names=FALSE)
Adults = Adults %>%
mutate(PupilSynch = unlist(Res,use.names=FALSE),
Group = 'Children')%>%
select(Seconds, Group, Stimulus, PupilSynch )%>%
group_by(Stimulus)%>%
summarise(PupilSynch = mean(PupilSynch))
Adults
Chil = Children
Chil = Chil%>%
mutate(PupilSynch = unlist(Res,use.names=FALSE),
Group = 'Children')%>%
select(Seconds, Group, Stimulus, PupilSynch )%>%
group_by(Stimulus)%>%
summarise(PupilSynch = mean(PupilSynch))
Chil
library(tidyverse)
library(zoo)
library(easystats)
library(parallel)
# Functions ------------------------------------------------------------
#### Running correlation
calculate_running_correlation <- function(Matrix, window_sizeP) {
# Define a function to calculate correlation for each window
calc_correlation <- function(window_rows) {
if (sum(is.na(window_rows)) > 0.75*prod(dim(window_rows))){
return(NA)
} else {
# Calculate the correlation matrix for the rows in the current window
correlation_matrix <- cor(window_rows, use = "pairwise.complete.obs")
correlation = average_fisher_z_transform(correlation_matrix)
return(correlation)
}
}
# Use rollapply to apply the function to each rolling window in the Matrix
correlation_matrices <- rollapply(Matrix, width = window_sizeP, FUN = calc_correlation,
by.column = FALSE, align = "center", partial = TRUE)
# Return the list of correlation matrices
return(correlation_matrices)
}
#### Extract Average normalized person correlation
average_fisher_z_transform <- function(corr_matrix, threshold = 0.75) {
# Check if more than a given threshold of the columns have a NA
if(sum(colSums(is.na(corr_matrix)) >= ncol(corr_matrix)-1) > threshold*ncol(corr_matrix)) {
return(NA)
}
# Take the lower triangle, excluding the diagonal
lower_tri <- corr_matrix[lower.tri(corr_matrix)]
# Apply Fisher's z transformation using atanh
z_scores <- atanh(lower_tri - 1e-10)
# Calculate the mean of z_scores
average_z_score <- mean(z_scores, na.rm = TRUE)
# Transform back to the correlation scale using tanh
average_correlation <- tanh(average_z_score)
return(average_correlation)
}
# Read Data and settings ------------------------------------------------------------
Hz = 20
Tresh =  0.75
window_sizeP = Hz*2 # Window to calculate the rolling correlation over
db = read.csv('C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\PupilDilationSync_2023\\Data\\ProcessedData\\FinalData.csv', sep = ',')
output_dir = 'C:\\Users\\tomma\\OneDrive - Birkbeck, University of London\\PupilDilationSync_2023\\Data\\ProcessedData\\'
# Extract synchrony in Pupil for adults ------------------------------------------------------------
#### Adults
Adults = db %>%
filter(Group == 'Adults') %>%
arrange(Stimulus) %>%
pivot_wider(names_from = Subject, values_from = Pupil,
id_cols = c( "Seconds", "Stimulus"))
ByStimulus = Adults %>%
split(.$Stimulus)
ByStimulus = lapply(ByStimulus, function(df) df %>% select(-1:-2))
Res = lapply(ByStimulus , calculate_running_correlation, window_sizeP = window_sizeP)
Adults$PupilSynch = unlist(Res,use.names=FALSE)
Adults = Adults %>%
mutate(PupilSynch = unlist(Res,use.names=FALSE),
Group = 'Children')%>%
select(Seconds, Group, Stimulus, PupilSynch )%>%
group_by(Stimulus)%>%
summarise(PupilSynch = mean(PupilSynch))
# Bootstrap ttest ------------------------------------------------------------
#--- Initialize parallelizion ---#
# Prepare cluster
cl = makeCluster(8)
# Load libraries on each node
clusterEvalQ(cl, {
library(tidyverse)
library(zoo)
})
clusterExport(cl, c("calculate_running_correlation", "average_fisher_z_transform"))
#--- Run synchrony adn ttest ---#
Children = db %>%
filter(Group == 'Children') %>%
arrange(Stimulus) %>%
pivot_wider(names_from = Subject, values_from = Pupil,
id_cols = c( "Seconds", "Stimulus"))
TTest_res = list()
start.time <- Sys.time()
for (x in 1:10000){
print(x)
Sub = sample(3:20, 9)
ByStimulus = Children %>% select(all_of(c(1,2,Sub))) %>% split(.$Stimulus)
ByStimulus = lapply(ByStimulus, function(df) df %>% select(-1:-2))
Res = parLapply(cl, ByStimulus , calculate_running_correlation, window_sizeP = window_sizeP)
Chil = Children
Chil = Chil%>%
mutate(PupilSynch = unlist(Res,use.names=FALSE),
Group = 'Children')%>%
select(Seconds, Group, Stimulus, PupilSynch )%>%
group_by(Stimulus)%>%
summarise(PupilSynch = mean(PupilSynch))
TTest_res[[x]] = parameters(t.test(Chil$PupilSynch, Adults$PupilSynch))
}
stopCluster(cl)
time.taken = round(Sys.time() - start.time,2)
time.taken
#--- Bind and save ---#
TTest_res = bind_rows(TTest_res)
write.csv(TTest_res, paste(output_dir , 'TTest.csv', sep=''), row.names = F)
# Calculate the combined p-value
combined_p_value <- point_estimate(TTest_res$p, centrality = "all", dispersion = TRUE)
# Calculate the combined t-score
combined_t_score <- point_estimate(TTest_res$t, centrality = "all", dispersion = TRUE)
# Print the results
print(combined_p_value)
print(combined_t_score)
library(plotly)
ggplotly(ggplot()+
geom_density(aes(x = Adults$PupilSynch)))
parameters(wilcox.test(Chil, Adults$PupilSynch))
point_estimate(TTest_res$p, centrality = "all", dispersion = TRUE)
View(TTest_res)
ggplotly(ggplot()+
geom_density(aes(x = TTest_res$p)))
ggplotly(ggplot()+
geom_density(aes(x = Chil$PupilSynch)))
ggplotly(ggplot()+
geom_density(aes(x = Adults$PupilSynch)))
####### Libraries ---------------------------------------------------------------
library(tidyverse)
library(easystats)
library(tictoc)
####### Function ---------------------------------------------------------------
outliers_removal = function(d) {
for(outl in 3:4){
m = mean(d[,outl])
s = sd(d[,outl])
d[d[,outl] < m-2*s | d[,outl] > m+2*s, outl] = NaN
}
return(d)
}
Interpolation =  function(Data) {
step =0.01
Data_out = data.frame(approx(Data$Specificity ,Data$Sensitivity, seq(0+step, 1, by=step), method="linear", ties = 'ordered'))
Data_out <- rbind(Data_out,c(0,0))
Data_out = rename(Data_out, Specificity = x, Sensitivity=y )
Data_out['Model'] =  Data$Model[1]
return(Data_out)
}
####### Settings ---------------------------------------------------------------
setwd("C:\\Users\\tomma\\Desktop\\BabyBrain\\Projects\\EMG")
input_dir    = "./Data\\Processing\\Pipelines"
output_dir   = "./Results\\"
subsets_path = "./SubsamplePipelines\\Subsamples.Rda"
Iterations   = 500
####### Set and read data ---------------------------------------------------------------
load(subsets_path)
setwd("C:\\Users\\tomma\\Desktop\\BabyBrain\\Projects\\EMG")
source("~/.active-rstudio-document", echo=TRUE)
